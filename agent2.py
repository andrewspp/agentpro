#!/usr/bin/env python3
"""
Agent 2: Analyse √âconom√©trique

Ce script g√©n√®re et ex√©cute du code d'analyse √©conom√©trique √† partir d'un fichier CSV
et des suggestions fournies par l'agent 1. Il g√®re la correction automatique des erreurs,
capture les visualisations et interpr√®te les r√©sultats.

Usage:
    python agent2.py chemin_csv prompt_utilisateur chemin_sortie_agent1 [--model modele] [--backend backend] [--auto-confirm]

Arguments:
    chemin_csv: Chemin vers le fichier CSV √† analyser
    prompt_utilisateur: Prompt initial de l'utilisateur
    chemin_sortie_agent1: Chemin vers le fichier de sortie de l'agent 1
    --model: Mod√®le LLM √† utiliser (d√©faut: gemma3:27b)
    --backend: Backend LLM ('ollama' ou 'gemini', d√©faut: 'ollama')
    --auto-confirm: Ignorer la pause manuelle pour correction
"""

import argparse
import json
import logging
import os
import re
import sys
import subprocess
import tempfile
from datetime import datetime
import base64
import matplotlib.pyplot as plt
from io import StringIO
import pandas as pd
import threading # <--- AJOUTER CECI
import time      # <--- AJOUTER CECI
# Important : S'assurer que matplotlib et pandas sont import√©s quelque part avant
try:
    import matplotlib.pyplot as plt
    import pandas as pd
except ImportError:
    print("ERREUR: Les modules matplotlib ou pandas sont manquants. Veuillez les installer.", file=sys.stderr)
    sys.exit(1)
from io import StringIO

# Importation du module llm_utils
from llm_utils import call_llm


# --- Constantes pour la d√©tection et le filtrage (AJUST√âES) ---
INTEREST_KEYWORDS = [
    'intercept', 'reforme', 'post', 'interaction_did', 'treat', 'did',
    'policy', 'intervention'
    # Ajoutez d'autres variables cl√©s attendues ici (ex: variables d'interaction sp√©cifiques)
]
CONTROL_KEYWORDS = [
    'log_budget', 'ratio_eleves_enseignant', 'taux_pauvrete',
    'niveau_urbanisation', 'budget_education', 'nb_eleves' # Ajout des versions non-log aussi
    # Ajoutez d'autres contr√¥les principaux attendus ici
]
# Patterns Regex PLUS SP√âCIFIQUES bas√©s sur vos images
FIXED_EFFECT_PATTERNS = [
    re.compile(r'^C\(region_id\)\[T\.\d+\]$', re.IGNORECASE),
    re.compile(r'^C\(trimestre\)\[T\.\d+\]$', re.IGNORECASE),
    # Gardez les patterns g√©n√©riques comme fallback si besoin, mais ceux ci-dessus sont prioritaires
    re.compile(r'^C\(\w+\)\[T\..*?\]$', re.IGNORECASE),
    re.compile(r'^\w+_FE$', re.IGNORECASE),
    re.compile(r'^factor\(\w+\)\d+$', re.IGNORECASE),
]
MIN_COEFS_FOR_FILTERING = 5      # Abaiss√© l√©g√®rement pour √™tre s√ªr
MIN_FE_RATIO_FOR_FILTERING = 0  # Abaiss√© l√©g√®rement
# --- Fin des constantes ---

# ======================================================
# Configuration du logging
# ======================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("agent2.log"),
        logging.StreamHandler(sys.stderr)
    ]
)
logger = logging.getLogger("agent2")

# Exemple minimal si non d√©j√† fait plus haut dans le script
if not logging.getLogger().hasHandlers():
     logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("agent2.capture") # Utiliser un logger sp√©cifique

def save_prompt_to_file(prompt_content: str, log_file_path: str, prompt_type: str):
    """
    Ajoute un prompt format√© au fichier journal sp√©cifi√©.
    Cr√©e le r√©pertoire si n√©cessaire.
    
    Args:
        prompt_content: Contenu du prompt √† sauvegarder
        log_file_path: Chemin du fichier journal
        prompt_type: Type de prompt (pour identification)
    """
    try:
        log_dir = os.path.dirname(log_file_path)
        os.makedirs(log_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        separator = "=" * 80

        with open(log_file_path, "a", encoding="utf-8") as f:
            f.write(f"{separator}\n")
            f.write(f"Timestamp: {timestamp}\n")
            f.write(f"Prompt Type: {prompt_type}\n")
            f.write(f"{separator}\n\n")
            f.write(prompt_content)
            f.write(f"\n\n{separator}\n\n")

        logger.info(f"Prompt '{prompt_type}' ajout√© √† {log_file_path}")

    except Exception as e:
        logger.error(f"Impossible d'√©crire le prompt '{prompt_type}' dans {log_file_path}: {e}")

def interpret_single_visualization_thread(vis, agent1_data, model, backend, prompts_log_path, results_list):
    """
    Fonction ex√©cut√©e dans un thread pour interpr√©ter une seule visualisation.
    Ajoute le r√©sultat (ou une erreur) √† la liste partag√©e 'results_list'.
    """
    # Utiliser le filename ou l'id comme identifiant unique
    if 'filename' in vis:
        vis_id = os.path.splitext(vis.get("filename", "unknown.png"))[0]
    else:
        vis_id = vis.get("id", f"unknown_{time.time()}") # Fallback ID unique

    try:
        logger.info(f"Thread d√©marr√© pour l'interpr√©tation de: {vis_id}")
        interpretation = interpret_visualization_with_gemini(
            vis,
            agent1_data,
            model,
            backend,
            prompts_log_path
        )
        # Ajouter le r√©sultat avec l'identifiant
        results_list.append({'id': vis_id, 'interpretation': interpretation})
        logger.info(f"Interpr√©tation re√ßue pour {vis_id} dans le thread.")
    except Exception as e:
        logger.error(f"Erreur dans le thread pour {vis_id}: {e}")
        # Ajouter l'erreur avec l'identifiant
        results_list.append({'id': vis_id, 'interpretation': f"Erreur d'interpr√©tation (thread): {e}"})
# --- FIN NOUVELLE FONCTION ---

def extract_code(llm_output: str) -> str:
    """
    Extrait uniquement le code Python des blocs d√©limit√©s par triple backticks.
    
    Args:
        llm_output: Texte complet de la sortie du LLM
        
    Returns:
        str: Code Python extrait ou texte complet si aucun bloc n'est trouv√©
    """
    code_blocks = re.findall(r"```(?:python)?\s*(.*?)```", llm_output, re.DOTALL)
    if code_blocks:
        return "\n\n".join(code_blocks)
    else:
        return llm_output

def remove_shell_commands(code: str) -> str:
    """
    Filtre le code en supprimant les lignes ressemblant √† des commandes shell.
    
    Args:
        code: Code Python potentiellement contenant des commandes shell
        
    Returns:
        str: Code nettoy√©
    """
    filtered_lines = []
    for line in code.splitlines():
        stripped = line.strip()
        if (stripped.startswith("pip install") or 
            stripped.startswith("bash") or 
            stripped.startswith("$") or 
            (stripped.startswith("python") and not stripped.startswith("python3"))):
            continue
        filtered_lines.append(line)
    return "\n".join(filtered_lines)

def sanitize_code(code: str, csv_path: str, valid_columns: list[str]) -> str:
    """
    Force le bon chemin CSV, corrige les noms de colonnes, corrige les probl√®mes d'indentation,
    et ins√®re un bloc pour √©viter les erreurs de corr√©lation.
    
    Args:
        code: Code Python √† nettoyer
        csv_path: Chemin absolu du fichier CSV
        valid_columns: Liste des noms de colonnes valides
        
    Returns:
        str: Code Python nettoy√©
    """
    import ast

    # Forcer le bon chemin CSV
    code = re.sub(
        r"pd\.read_csv\((.*?)\)",
        f"pd.read_csv('{csv_path}')",
        code
    )

    # Identifier les colonnes utilis√©es
    used_columns = set(re.findall(r"df\[['\"](.*?)['\"]\]", code)) | \
                   set(re.findall(r"df\.([a-zA-Z_][a-zA-Z0-9_]*)", code))

    # Mapper les noms de colonnes utilis√©s aux noms r√©els
    col_map = {}
    for used in used_columns:
        for real in valid_columns:
            if used.lower() == real.lower():
                col_map[used] = real
                break

    # Corriger les noms de colonnes
    for used, correct in col_map.items():
        if used != correct:
            code = re.sub(rf"df\[['\"]{used}['\"]\]", f"df['{correct}']", code)
            code = re.sub(rf"df\.{used}\b", f"df['{correct}']", code)

    # Ajouter un bloc pour √©viter les erreurs de corr√©lation
    if "df.corr()" in code or re.search(r"df\.corr\s*\(", code):
        init_numeric_block = "\n# üîç S√©lection des colonnes num√©riques pour √©viter les erreurs sur df.corr()\ndf_numeric = df.select_dtypes(include='number')\n"
        match = re.search(r"(df\s*=\s*pd\.read_csv\(.*?\))", code)
        if match:
            insertion_point = match.end()
            code = code[:insertion_point] + init_numeric_block + code[insertion_point:]
        else:
            code = init_numeric_block + code

        code = re.sub(r"df\.corr(\s*\(.*?\))", r"df_numeric.corr\1", code)
    
    # NOUVEAU: Corriger les probl√®mes d'indentation avec plt.show() dans les blocs try/except/else
    # Cette solution utilise une approche ligne par ligne pour une correction pr√©cise
    lines = code.split('\n')
    i = 0
    while i < len(lines) - 1:
        current_line = lines[i].rstrip()
        next_line = lines[i + 1].rstrip()
        
        # Chercher plt.show() suivi par except, else ou elif √† une indentation diff√©rente
        if current_line.strip() == 'plt.show()' and next_line.lstrip().startswith(('except', 'else', 'elif')):
            # Calculer l'indentation du bloc suivant
            next_indent = len(next_line) - len(next_line.lstrip())
            if next_indent > 0:
                # R√©indenter plt.show() pour correspondre au bloc
                lines[i] = ' ' * next_indent + 'plt.show()'
        i += 1
    
    # Reconstruire le code avec les lignes corrig√©es
    code = '\n'.join(lines)
    
    # MODIFICATION: Ne pas d√©sactiver plt.show() compl√®tement, mais le remplacer par une sauvegarde suivie d'un show()
    code = re.sub(r"plt\.show\(\)", "plt.savefig('temp_figure.png', dpi=100, bbox_inches='tight')\nplt.show()", code)
    
    # Remplacer les styles Seaborn obsol√®tes
    code = code.replace("seaborn-v0_8-whitegrid", "whitegrid")
    code = code.replace("seaborn-whitegrid", "whitegrid")
    code = code.replace("seaborn-v0_8-white", "white")
    code = code.replace("seaborn-white", "white")
    code = code.replace("seaborn-v0_8-darkgrid", "darkgrid")
    code = code.replace("seaborn-darkgrid", "darkgrid")
    code = code.replace("seaborn-v0_8-dark", "dark")
    code = code.replace("seaborn-dark", "dark")
    code = code.replace("seaborn-v0_8-paper", "ticks")
    code = code.replace("seaborn-paper", "ticks")
    code = code.replace("seaborn-v0_8-talk", "ticks")
    code = code.replace("seaborn-talk", "ticks")
    code = code.replace("seaborn-v0_8-poster", "ticks")
    code = code.replace("seaborn-poster", "ticks")
    
    return code

def _build_filtered_table_text(full_table_text: str, final_coefficients: list, r_squared: str) -> str:
    """
    Reconstruit une version textuelle simplifi√©e de la table de r√©gression.
    LOGGING AM√âLIOR√â POUR D√âBOGAGE.
    """
    logger.debug("[BUILD DEBUG] Appel de _build_filtered_table_text")
    lines = full_table_text.splitlines()
    header_lines = []
    coef_header_line = ""
    footer_lines = []
    in_coef_section = False
    coef_section_found = False
    # Patterns ajust√©s pour √™tre plus pr√©cis
    coef_start_pattern = re.compile(r"^={2,}$") # Ligne de ===
    # Cherche explicitement "coef" ou "std err" etc. pour l'en-t√™te
    coef_header_pattern = re.compile(r"coef|std err|t|p>\|t\||\[0\.025", re.IGNORECASE)

    # --- Parsing am√©lior√© pour s√©parer header/coef/footer ---
    current_section = "header" # header, coef, footer
    for i, line in enumerate(lines):
        stripped = line.strip()
        is_separator_line = coef_start_pattern.match(stripped)

        if current_section == "header":
            # Si on trouve une ligne qui ressemble √† un en-t√™te de coeffs *apr√®s* un s√©parateur potentiel
            if coef_header_pattern.search(line) and (i > 0 and coef_start_pattern.match(lines[i-1].strip())):
                logger.debug(f"[BUILD DEBUG] D√©tection potentielle header coeffs Ligne {i+1}: {line}")
                # V√©rification suppl√©mentaire : la ligne suivante a-t-elle des chiffres ?
                if i + 1 < len(lines) and re.search(r'\d', lines[i+1]):
                     logger.debug(f"[BUILD DEBUG] Confirmation header coeffs Ligne {i+1}.")
                     coef_header_line = line
                     current_section = "coef"
                     coef_section_found = True
                     # La ligne de s√©paration juste avant fait partie du header
                     if coef_start_pattern.match(lines[i-1].strip()):
                         header_lines.append(lines[i-1])
                     else: # Si pas de separateur avant, ajouter celui par defaut
                         header_lines.append("=" * 80)

                else: # Fausse alerte, on reste dans le header
                     header_lines.append(line)
            else:
                 header_lines.append(line)
        elif current_section == "coef":
            # Si on rencontre une nouvelle ligne de s√©paration, c'est la fin des coeffs
            if is_separator_line:
                logger.debug(f"[BUILD DEBUG] Fin de section coeffs d√©tect√©e Ligne {i+1}: {line}")
                current_section = "footer"
                footer_lines.append(line) # Garder ce s√©parateur
            # Sinon, on ignore la ligne (ce sont les coeffs originaux)
        elif current_section == "footer":
            footer_lines.append(line)
    # --- Fin Parsing ---

    if not coef_section_found:
        logger.error("[BUILD DEBUG] En-t√™te de la section Coefficients non trouv√©! Impossible de reconstruire la table filtr√©e. Retour texte complet.")
        return full_table_text # Fallback critique

    logger.debug(f"[BUILD DEBUG] Header Lines ({len(header_lines)}): \n" + "\n".join(header_lines[-5:])) # 5 derni√®res lignes
    logger.debug(f"[BUILD DEBUG] Coef Header Line: {coef_header_line}")
    logger.debug(f"[BUILD DEBUG] Footer Lines ({len(footer_lines)}): \n" + "\n".join(footer_lines[:5])) # 5 premi√®res lignes

    # --- Reconstruction ---
    output_lines = header_lines
    output_lines.append(coef_header_line)

    # Formater les coefficients filtr√©s
    if final_coefficients:
        max_var_len = max(len(c['variable']) for c in final_coefficients) if final_coefficients else 20
        max_var_len = max(max_var_len, 15) + 2 # Assurer une largeur min + padding

        # Largeurs fixes (simplifi√©) - ajuster si n√©cessaire
        col_widths = {'coef': 12, 'std_err': 11, 'p_value': 9} # Laisser de la place

        for coef_data in final_coefficients:
            var_name = coef_data['variable']
            coef_s = coef_data.get('coef', 'N/A')
            std_s = coef_data.get('std_err', 'N/A')
            pval_s = coef_data.get('p_value', 'N/A')

            # Juste alignement basique
            line = f"{var_name:<{max_var_len}}" \
                   f"{coef_s:>{col_widths['coef']}}" \
                   f"{std_s:>{col_widths['std_err']}}" \
                   f"{pval_s:>{col_widths['p_value']}}"
            # Ajouter d'autres colonnes si elles existent dans l'en-t√™te et les donn√©es
            output_lines.append(line)
        logger.debug(f"[BUILD DEBUG] {len(final_coefficients)} coefficients format√©s pour la sortie.")
    else:
         logger.warning("[BUILD DEBUG] La liste final_coefficients est vide.")

    # Ajouter note et footer
    output_lines.append("=" * 80) # S√©parateur
    output_lines.append("[Note: Effets fixes et autres coefficients non essentiels omis pour la lisibilit√©.]")
    output_lines.append(f"[R-squared original: {r_squared}]")
    output_lines.extend(footer_lines)

    filtered_text = "\n".join(output_lines)
    logger.debug(f"[BUILD DEBUG] Texte filtr√© construit (longueur: {len(filtered_text)}). Premi√®res lignes:\n{filtered_text[:300]}...")
    return filtered_text

def capture_regression_outputs(output_text, output_dir, executed_code=""):
    """
    Capture OLS tables. Saves full raw text.
    Saves image (full OR filtered if many FEs).
    Filters coeffs in structured data/CSV if many FEs. Enhanced logging.
    """

        # --- D√©finitions des Patterns Regex ---
    # !!! LIGNE MANQUANTE AJOUT√âE ICI !!!
    regression_pattern = re.compile(r"={10,}\s*OLS Regression Results\s*={10,}(.*?)(?:\n={10,}|\Z)", re.DOTALL)
    # --- Fin de l'ajout ---
    coef_section_pattern = re.compile(r"={2,}\s*\n\s*(coef.*?)(?:\n\s*={2,}|\Z)", re.DOTALL | re.IGNORECASE)
    r_squared_pattern = re.compile(r"R-squared:\s*([\d\.]+)", re.IGNORECASE)
    interaction_pattern = re.compile(r'\*|:')
    # --- Fin D√©finitions ---
    # ... (d√©but de la fonction, patterns, etc. comme avant) ...
    os.makedirs(output_dir, exist_ok=True)
    matches = regression_pattern.finditer(output_text)
    match_found = False
    regression_outputs = [] # Initialiser ici

    for i, match in enumerate(matches):
        match_found = True
        full_table_text = match.group(0)
        table_content = match.group(1).strip()
        table_id = f"regression_{i+1}"
        coefficients_filtered = False
        logger.info(f"[CAPTURE] Traitement table r√©gression: {table_id}")

        # --- 1. Sauvegarde Texte Complet ---
        text_file_path = os.path.join(output_dir, f"{table_id}.txt")
        # ... (try/except pour sauvegarder full_table_text) ...
        try:
            with open(text_file_path, "w", encoding="utf-8") as f: f.write(full_table_text)
            logger.debug(f"[CAPTURE] Table brute sauvegard√©e: {text_file_path}")
        except Exception as e:
            logger.error(f"[CAPTURE] Erreur sauvegarde texte {table_id}: {e}"); continue

        # --- 2. Parsing & D√©cision Filtrage ---
        r_squared_match = r_squared_pattern.search(table_content)
        r_squared = r_squared_match.group(1) if r_squared_match else "N/A"
        all_coefficients_raw = []
        num_fixed_effects_found = 0
        final_coefficients = [] # Doit √™tre d√©fini avant le bloc try/except image

        coef_section_match = coef_section_pattern.search(table_content)
        if not coef_section_match:
            logger.warning(f"[CAPTURE] Section coeffs non trouv√©e {table_id}")
        else:
            # ... (logique de parsing des coefficients comme avant, remplissant all_coefficients_raw) ...
            # --- D√©but Parsing Coeffs ---
            coef_lines = coef_section_match.group(1).strip().split('\n')
            if coef_lines and ("coef" in coef_lines[0].lower() or "std err" in coef_lines[0].lower()):
                coef_lines = coef_lines[1:]
            for line_num, line in enumerate(coef_lines):
                stripped_line = line.strip();
                if not stripped_line or stripped_line.startswith('---'): continue
                parts = re.split(r'\s{2,}', stripped_line);
                if len(parts) < 2: continue
                variable_name = parts[0]; values = parts[1:]
                def safe_get(arr, index, default="N/A"):
                    try: return arr[index]
                    except IndexError: return default
                coef_val = safe_get(values, 0); std_err_val = safe_get(values, 1)
                p_value_val = safe_get(values, 2) # Heuristique simple
                try:
                    if not (0 <= float(p_value_val) <= 1):
                         p_value_candidate = safe_get(values, 3)
                         if 0 <= float(p_value_candidate) <= 1: p_value_val = p_value_candidate
                         else: p_value_val = "N/A"
                except: p_value_val = "N/A"
                all_coefficients_raw.append({"variable": variable_name, "coef": coef_val, "std_err": std_err_val, "p_value": p_value_val})
                if any(pattern.match(variable_name) for pattern in FIXED_EFFECT_PATTERNS): num_fixed_effects_found += 1
            # --- Fin Parsing Coeffs ---

            total_coeffs = len(all_coefficients_raw)
            logger.info(f"[CAPTURE] {table_id} - Coeffs pars√©s: {total_coeffs}, FE d√©tect√©s: {num_fixed_effects_found}")

            # D√©cision de filtrer
            trigger_filtering = False
            if total_coeffs >= MIN_COEFS_FOR_FILTERING:
                fixed_effect_ratio = num_fixed_effects_found / total_coeffs if total_coeffs > 0 else 0
                logger.info(f"[CAPTURE] {table_id} - Ratio FE: {fixed_effect_ratio:.2f}")
                if fixed_effect_ratio >= MIN_FE_RATIO_FOR_FILTERING:
                    trigger_filtering = True
                    logger.info(f"[CAPTURE] {table_id} - FILTRAGE ACTIV√â.")

            # Application du filtrage
            if trigger_filtering:
                coefficients_filtered = True
                # ... (logique de filtrage comme avant, remplissant final_coefficients) ...
                # --- D√©but Filtrage ---
                logger.debug(f"[CAPTURE] {table_id} - Application filtre:")
                for coef_data in all_coefficients_raw:
                    var_name = coef_data["variable"]; var_name_lower = var_name.lower()
                    is_fe = any(pattern.match(var_name) for pattern in FIXED_EFFECT_PATTERNS)
                    is_interest = any(keyword == var_name_lower for keyword in INTEREST_KEYWORDS) or interaction_pattern.search(var_name)
                    is_control = any(keyword == var_name_lower for keyword in CONTROL_KEYWORDS)
                    keep = is_interest or is_control or (not is_fe)
                    if keep: final_coefficients.append(coef_data)
                    #logger.debug(f"  -> {'CONSERVER' if keep else 'EXCLURE'} '{var_name}' (Int:{is_interest}, Ctrl:{is_control}, FE:{is_fe})") # Log tr√®s verbeux
                # --- Fin Filtrage ---
                logger.info(f"[CAPTURE] {table_id} - Coeffs apr√®s filtrage: {len(final_coefficients)}")
                if len(final_coefficients) == 0 and len(all_coefficients_raw) > 0:
                     logger.warning(f"[CAPTURE] {table_id} - Filtrage annul√© (0 coeff conserv√©).")
                     final_coefficients = all_coefficients_raw; coefficients_filtered = False
            else:
                 logger.info(f"[CAPTURE] {table_id} - Filtrage NON activ√©.")
                 final_coefficients = all_coefficients_raw # Garder tout

        # --- 3. Pr√©paration Texte & Cr√©ation Image ---
        img_file_path = os.path.join(output_dir, f"{table_id}.png")
        img_data = None
        text_for_image = full_table_text # D√©faut

        if coefficients_filtered:
             logger.info(f"[CAPTURE] {table_id} - Tentative construction texte filtr√© pour image...")
             try:
                 text_for_image = _build_filtered_table_text(full_table_text, final_coefficients, r_squared)
                 logger.info(f"[CAPTURE] {table_id} - Texte filtr√© pour image OK.")
             except Exception as e_build:
                 logger.error(f"[CAPTURE] {table_id} - Erreur construction texte filtr√©: {e_build}. Utilisation texte complet.")
                 text_for_image = full_table_text # Fallback

        # LOG CRUCIAL : Quel texte est utilis√© pour l'image ?
        logger.info(f"[CAPTURE] G√©n√©ration image {table_id} avec texte {'FILTR√â' if coefficients_filtered and text_for_image != full_table_text else 'COMPLET'}.")
        logger.debug(f"[CAPTURE] Texte pour image {table_id} (d√©but):\n{text_for_image[:400]}...")
        logger.debug(f"[CAPTURE] Texte pour image {table_id} (fin):\n...{text_for_image[-400:]}")

        try:
            # ... (logique de g√©n√©ration d'image plt.text(..., text_for_image, ...)) ...
            num_lines_img = len(text_for_image.split('\n'))
            fig_height = max(6, num_lines_img * 0.18); fig_width = 12
            plt.figure(figsize=(fig_width, fig_height))
            fontsize = 7 if num_lines_img > 60 else 8
            plt.text(0.01, 0.99, text_for_image, family='monospace', fontsize=fontsize, verticalalignment='top')
            plt.axis('off'); plt.tight_layout(pad=0.3)
            plt.savefig(img_file_path, dpi=120, bbox_inches='tight'); plt.close()
            logger.info(f"[CAPTURE] Image sauvegard√©e: {img_file_path}")
            with open(img_file_path, 'rb') as img_file: img_data = base64.b64encode(img_file.read()).decode('utf-8')
        except Exception as e:
            logger.error(f"[CAPTURE] Erreur cr√©ation/encodage image {table_id}: {e}")
            img_file_path = None

        # --- 4. Pr√©paration Donn√©es Structur√©es & CSV ---
        regression_data = {
            "r_squared": r_squared,
            "coefficients": final_coefficients, # Utilise la liste (filtr√©e ou non)
            "coefficients_filtered": coefficients_filtered,
            "csv_data": None, "regression_code": ""
        }

        # ... (logique g√©n√©ration CSV comme avant, utilisant final_coefficients) ...
        if regression_data["coefficients"]:
            try:
                coef_df = pd.DataFrame(regression_data["coefficients"])
                # Add significance column
                try:
                    coef_df['p_value_float'] = pd.to_numeric(coef_df['p_value'], errors='coerce')
                    coef_df['significatif'] = coef_df['p_value_float'] < 0.05
                    coef_df = coef_df.drop(columns=['p_value_float'])
                except Exception: coef_df['significatif'] = 'N/A'
                csv_header = f"# R√©sultats R√©gression OLS - Table {i+1}\n# R¬≤: {r_squared}\n"
                if coefficients_filtered: csv_header += f"# NOTE: Effets fixes omis.\n"
                csv_header += f"# Colonnes: variable, coef, std_err, p_value, significatif (<0.05)\n"
                regression_data["csv_data"] = csv_header + coef_df.to_csv(index=False, line_terminator='\n')
                csv_path = os.path.join(output_dir, f"{table_id}_coefficients.csv")
                with open(csv_path, 'w', encoding='utf-8', newline='\n') as f: f.write(regression_data["csv_data"])
                logger.info(f"[CAPTURE] CSV {'filtr√©' if coefficients_filtered else 'complet'} sauvegard√©: {csv_path}")
            except Exception as e: logger.error(f"[CAPTURE] Erreur CSV {table_id}: {e}"); regression_data["csv_data"] = "# Erreur CSV\n"
        else: logger.warning(f"[CAPTURE] {table_id} - Pas de coeffs pour CSV.")


        # --- 5. Extraction Code Source ---
        if executed_code:
             try: regression_data["regression_code"] = extract_regression_code(executed_code, full_table_text)
             except Exception as e: logger.error(f"[CAPTURE] Erreur extraction code {table_id}: {e}")

        # --- 6. Stockage R√©sultat Final ---
        regression_outputs.append({
            'type': 'regression_table', 'id': table_id,
            'text_path': text_file_path, 'image_path': img_file_path,
            'base64': img_data, # Base64 de l'image (filtr√©e ou non)
            'title': f"R√©sultats R√©gression: {table_id.replace('_', ' ').capitalize()}",
            'metadata': {'r_squared': r_squared, 'variables': [c['variable'] for c in regression_data['coefficients']]},
            'data': regression_data,
            'csv_data': regression_data.get("csv_data", ""),
            'regression_code': regression_data["regression_code"],
            'coefficients_filtered': coefficients_filtered
        })

    if not match_found: logger.warning("[CAPTURE] Aucune table OLS trouv√©e.")
    return regression_outputs

def interpret_visualization_with_gemini(vis, agent1_data, model, backend, prompts_log_path, timeout):
    """
    Interpr√®te une visualisation en envoyant directement l'image √† Gemini via la fonction call_llm.
    
    Args:
        vis: M√©tadonn√©es de la visualisation avec base64 de l'image
        agent1_data: Donn√©es de l'agent1
        model: Mod√®le LLM √† utiliser
        backend: Backend pour les appels LLM
        prompts_log_path: Chemin pour sauvegarder les prompts
        
    Returns:
        str: Interpr√©tation de la visualisation
    """
    # Importer call_llm depuis llm_utils
    from llm_utils import call_llm
    
    # Valider que l'image est disponible
    if 'base64' not in vis or not vis['base64']:
        logger.error(f"Pas de donn√©es base64 disponibles pour la visualisation {vis.get('id', 'non identifi√©e')}")
        return "Erreur: Impossible d'analyser cette visualisation (image non disponible)"
    
    # Extraire le titre et le type
    if 'filename' in vis:
        filename = vis.get("filename", "figure.png")
        vis_id = os.path.splitext(filename)[0]
    else:
        vis_id = vis.get("id", "visualisation")
        filename = vis_id + '.png'
    
    # D√©terminer le type de visualisation
    vis_type = "Unknown visualization"
    if 'regression' in vis_id.lower():
        vis_type = "Table de R√©gression OLS"
    elif 'correlation' in vis_id.lower() or 'corr' in vis_id.lower():
        vis_type = "Matrice de Corr√©lation"
    elif 'distribution' in vis_id.lower() or 'hist' in vis_id.lower():
        vis_type = "Graphique de Distribution"
    elif 'scatter' in vis_id.lower() or 'relation' in vis_id.lower():
        vis_type = "Nuage de Points"
    elif 'box' in vis_id.lower():
        vis_type = "Bo√Æte √† Moustaches"
    
    # R√©cup√©rer le titre
    title = vis.get("title", vis_type)
    
    # Extraire des informations suppl√©mentaires pour le contexte
    metadata_str = json.dumps(agent1_data["metadata"], indent=2, ensure_ascii=False)
    extra_context = ""
    if 'metadata' in vis:
        if 'r_squared' in vis['metadata']:
            extra_context += f"\nR-squared: {vis['metadata']['r_squared']}"
        if 'variables' in vis['metadata']:
            extra_context += f"\nVariables principales: {', '.join(vis['metadata']['variables'])}"
    
    # Cr√©er le prompt pour Gemini
    prompt = f"""## INTERPR√âTATION DE VISUALISATION √âCONOMIQUE

### Type de visualisation
{vis_type}

### Titre
{title}

### Identifiant
{vis_id}

### M√©tadonn√©es sp√©cifiques
{extra_context}

### Contexte des donn√©es
Le dataset contient les variables suivantes: {', '.join(agent1_data["metadata"].get("noms_colonnes", []))}

### M√©tadonn√©es de l'ensemble de donn√©es
```json
{metadata_str[:500]}...
```

### Question de recherche initiale
{agent1_data.get("user_prompt", "Non disponible")}

---

Analyse cette visualisation √©conomique. Tu re√ßois directement l'image, donc base ton analyse sur ce que tu observes visuellement. Ton interpr√©tation doit:

1. D√©crire pr√©cis√©ment ce que montre la visualisation (tendances, relations, valeurs aberrantes)
2. Expliquer les relations entre les variables visibles
3. Mentionner les valeurs num√©riques sp√©cifiques (minimums, maximums, moyennes) que tu peux d√©duire visuellement
4. Relier cette visualisation √† la question de recherche

Ton interpr√©tation doit √™tre factuelle, pr√©cise et bas√©e uniquement sur ce que tu peux observer dans l'image. Reste √©conomique dans ton analyse en te concentrant sur les informations les plus importantes.
"""

    # Sauvegarder le prompt dans le fichier journal
    save_prompt_to_file(prompt, prompts_log_path, f"Gemini Image Interpretation - {vis_id}")
    
    try:
        # Utiliser call_llm de llm_utils qui g√®re maintenant les images
        logger.info(f"Appel √† Gemini avec image pour visualisation {vis_id}")
        interpretation = call_llm(
            prompt=prompt, 
            model_name=model, 
            backend=backend, 
            image_base64=vis['base64']
        )
        
        logger.info(f"Interpr√©tation g√©n√©r√©e par Gemini pour: {vis_id}")
        return interpretation
    except Exception as e:
        logger.error(f"Erreur lors de l'interpr√©tation de l'image pour {vis_id}: {e}")
        return f"Erreur d'interpr√©tation: {e}"

# --- REMPLACEMENT DE LA FONCTION ---
def generate_visualization_interpretations(visualizations, regression_outputs, agent1_data, model, backend, prompts_log_path, timeout=120):
    """
    G√©n√®re des interpr√©tations pour les visualisations S√âQUENTIELLEMENT (SANS THREADING).
    Utilise le LLM Gemini directement √† partir des images.

    Args:
        visualizations (list): Liste des m√©tadonn√©es des visualisations.
        regression_outputs (list): Liste des sorties des tables de r√©gression.
        agent1_data (dict): Donn√©es de l'agent1.
        model (str): Mod√®le LLM √† utiliser.
        backend (str): Backend pour les appels LLM.
        prompts_log_path (str): Chemin pour sauvegarder les prompts.
        timeout (int): Timeout en secondes pour chaque appel LLM d'interpr√©tation. D√©faut: 120.

    Returns:
        Liste mise √† jour des visualisations avec interpr√©tations.
    """
    # Combiner visualisations et r√©gressions
    all_visuals = visualizations + regression_outputs
    if not all_visuals:
        logger.info("Aucune visualisation ou table de r√©gression √† interpr√©ter.")
        return []

    logger.info(f"Lancement des interpr√©tations S√âQUENTIELLES pour {len(all_visuals)} √©l√©ments (timeout/item: {timeout}s)")

    updated_visuals = [] # Nouvelle liste pour stocker les r√©sultats mis √† jour

    for i, vis in enumerate(all_visuals):
        # G√©n√©rer un ID unique si n√©cessaire pour le mapping des r√©sultats
        if 'id' not in vis: # Assigner un ID si manquant
             vis_prefix = os.path.splitext(vis.get("filename", "unknown"))[0] if 'filename' in vis else vis.get("type", "item")
             vis['id'] = f"{vis_prefix}_{i}_{int(time.time()*1000)}"

        vis_id = vis['id'] # Utiliser l'ID assign√©

        # V√©rifier que l'image est disponible en base64
        if 'base64' not in vis or not vis['base64']:
            logger.warning(f"({i+1}/{len(all_visuals)}) Image manquante pour {vis_id}. Interpr√©tation impossible.")
            vis['interpretation'] = "Interpr√©tation impossible: image non disponible"
            updated_visuals.append(vis) # Ajouter l'√©l√©ment avec l'erreur
            continue # Passe √† la visualisation suivante

        logger.info(f"({i+1}/{len(all_visuals)}) Interpr√©tation de : {vis_id}")
        try:
            # Appel DIRECT √† la fonction d'interpr√©tation
            interpretation = interpret_visualization_with_gemini(
                vis,
                agent1_data,
                model,
                backend,
                prompts_log_path,
                timeout # Passer le timeout
            )
            vis['interpretation'] = interpretation
            logger.info(f"({i+1}/{len(all_visuals)}) Interpr√©tation re√ßue pour {vis_id}.")

        except Exception as e:
            logger.error(f"({i+1}/{len(all_visuals)}) Erreur lors de l'interpr√©tation directe pour {vis_id}: {e}", exc_info=True)
            vis['interpretation'] = f"Erreur d'interpr√©tation (directe): {e}"

        updated_visuals.append(vis) # Ajouter l'√©l√©ment trait√© (avec succ√®s ou erreur)

    logger.info(f"Toutes les {len(all_visuals)} interpr√©tations s√©quentielles sont termin√©es.")

    return updated_visuals

# --- FIN REMPLACEMENT FONCTION ---
def attempt_execution_loop(code: str, csv_file: str, agent1_data: dict, model: str, prompts_log_path: str, backend: str) -> dict:
    """
    Tente d'ex√©cuter le code et, en cas d'erreur, demande une correction au LLM en utilisant le backend choisi.
    
    Args:
        code: Code Python √† ex√©cuter
        csv_file: Chemin absolu du fichier CSV
        agent1_data: Donn√©es de l'agent1
        model: Mod√®le LLM √† utiliser
        prompts_log_path: Chemin pour sauvegarder les prompts
        backend: Backend pour les appels LLM ('ollama' ou 'gemini')
        
    Returns:
        Dictionnaire contenant les r√©sultats de l'ex√©cution
    """
    import base64
    from collections import deque
    import time
    import shutil
    import os
    import tempfile
    import json
    import subprocess
    import logging
    from datetime import datetime

    logger = logging.getLogger("agent2")

    # Initialiser le mod√®le courant (sera mis √† jour si on bascule vers le mod√®le puissant)
    current_model = model

    # V√©rifier si le code utilise le bon chemin CSV
    if csv_file not in code:
        logger.warning(f"Le code initial ne semble pas utiliser le chemin absolu '{csv_file}'.")

    # Cr√©er les r√©pertoires temporaires pour l'ex√©cution
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    temp_dir = tempfile.mkdtemp(prefix=f"analysis_{timestamp}_")
    
    # Ajouter un r√©pertoire pour les tables de r√©gression
    tables_dir = os.path.join(temp_dir, "tables")
    os.makedirs(tables_dir, exist_ok=True)
    
    # Ajouter un r√©pertoire pour les visualisations
    vis_dir = os.path.join(temp_dir, "visualisations")
    os.makedirs(vis_dir, exist_ok=True)

    # Cr√©er un r√©pertoire pour sauvegarder les versions du code
    code_versions_dir = os.path.join("outputs", f"code_versions_{timestamp}")
    os.makedirs(code_versions_dir, exist_ok=True)
    logger.info(f"Les versions de code seront sauvegard√©es dans {code_versions_dir}")

    import textwrap

# Code pour capturer les visualisations - MODIFI√â AVEC SUPPORT CSV AM√âLIOR√â
    vis_code = textwrap.dedent(f"""
# Ajout pour capturer les visualisations
import os
import logging
import io
import re
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
import json
import numpy as np

# Configuration pour un meilleur rendu - Utilisation d'un style compatible avec les versions r√©centes de matplotlib
try:
    plt.style.use('whitegrid')  # Pour seaborn r√©cent
except:
    try:
        plt.style.use('seaborn')  # Fallback vers le style seaborn de base
    except:
        pass  # Utiliser le style par d√©faut si seaborn n'est pas disponible

# D√©finition des r√©pertoires de sortie
VIS_DIR = r"{vis_dir}"
TABLES_DIR = r"{tables_dir}"
os.makedirs(VIS_DIR, exist_ok=True)
os.makedirs(TABLES_DIR, exist_ok=True)

# Configuration du logger
vis_logger = logging.getLogger("visualisation_capture")

# Redirection de l'affichage pour capturer les sorties
from io import StringIO
import sys

# Classe pour capturer stdout
class CaptureOutput:
    def __init__(self):
        self.old_stdout = sys.stdout
        self.buffer = StringIO()

    def __enter__(self):
        sys.stdout = self.buffer
        return self.buffer

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout = self.old_stdout

# Classe pour convertir les donn√©es numpy en JSON
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        return super(NumpyEncoder, self).default(obj)

# Remplacer la fonction plt.show() pour enregistrer les figures
_original_show = plt.show
_fig_counter = 0

def _custom_show(*args, **kwargs):
    global _fig_counter
    try:
        fig = plt.gcf()
        if fig.get_axes():
            _fig_counter += 1
            filepath = os.path.join(VIS_DIR, f"figure_{{_fig_counter}}.png")

            # Capturer les donn√©es du graphique actuel
            fig_data = {{}}

            # MODIFICATION: Capturer les informations sur les axes et le titre
            fig_title = fig.get_label() or f"Figure {{_fig_counter}}"
            fig_data["title"] = fig_title

            # MODIFICATION: Essayer de capturer les donn√©es utilis√©es pour la visualisation
            for ax_idx, ax in enumerate(fig.get_axes()):
                ax_data = {{}}
                
                # Capturer les titres et labels d'axes
                x_label = ax.get_xlabel() or "Axe X"
                y_label = ax.get_ylabel() or "Axe Y"
                ax_title = ax.get_title() or fig_title
                
                ax_data["x_label"] = x_label
                ax_data["y_label"] = y_label
                ax_data["title"] = ax_title
                
                # Capturer les donn√©es des lignes
                for line_idx, line in enumerate(ax.get_lines()):
                    line_id = f"ax{{ax_idx}}_line{{line_idx}}"
                    x_data = line.get_xdata()
                    y_data = line.get_ydata()
                    
                    # Cr√©er un petit dataframe et le convertir en CSV avec contexte
                    if len(x_data) > 0 and len(y_data) > 0:
                        try:
                            import pandas as pd
                            # Utiliser les labels des axes comme noms de colonnes
                            line_df = pd.DataFrame({{
                                x_label: x_data, 
                                y_label: y_data
                            }})
                            
                            # CORRECTION: R√©cup√©rer le titre de l'axe depuis ax_data ou utiliser une valeur par d√©faut
                            current_ax_title = ax_data.get("title", f"Figure {{_fig_counter}}")
                            
                            # Ajouter des m√©tadonn√©es en commentaire CSV
                            csv_header = f"# {{current_ax_title}} - Relation entre {{x_label}} et {{y_label}}\\n"  # Ajout du caract√®re \\n √† la fin
                            csv_header += f"# Source: Figure {{_fig_counter}}, Ligne {{line_idx+1}}\\n"
                            
                            # Ajouter des statistiques en commentaire CSV
                            if hasattr(x_data, 'min') and hasattr(y_data, 'min'):
                                stats = f"# Statistiques {{x_label}}: min={{x_data.min():.2f}}, max={{x_data.max():.2f}}, moyenne={{x_data.mean():.2f}}\\n"
                                stats += f"# Statistiques {{y_label}}: min={{y_data.min():.2f}}, max={{y_data.max():.2f}}, moyenne={{y_data.mean():.2f}}\\n"
                                csv_header += stats
                            
                            csv_data = csv_header + line_df.to_csv(index=False)
                            
                            ax_data[line_id] = {{
                                "type": "line",
                                "x": x_data.tolist() if hasattr(x_data, 'tolist') else list(x_data),
                                "y": y_data.tolist() if hasattr(y_data, 'tolist') else list(y_data),
                                "label": line.get_label() if line.get_label() != '_nolegend_' else f"Line {{line_idx+1}}",
                                "x_label": x_label,  # Ajout du label de l'axe x
                                "y_label": y_label,  # Ajout du label de l'axe y
                                "title": current_ax_title,   # Ajout du titre (corrig√©)
                                "csv_data": csv_data
                            }}
                        except Exception as e:
                            vis_logger.error(f"Erreur lors de la conversion des donn√©es en CSV am√©lior√©: {{e}}")
                            ax_data[line_id] = {{
                                "type": "line",
                                "x": x_data.tolist() if hasattr(x_data, 'tolist') else list(x_data),
                                "y": y_data.tolist() if hasattr(y_data, 'tolist') else list(y_data),
                                "label": line.get_label() if line.get_label() != '_nolegend_' else f"Line {{line_idx+1}}",
                                "x_label": x_label,
                                "y_label": y_label
                            }}

                # Extraire les lignes, barres, etc.
                for line_idx, line in enumerate(ax.get_lines()):
                    line_id = f"ax{{ax_idx}}_line{{line_idx}}"
                    if line_id not in ax_data:  # Si n'est pas d√©j√† ajout√© avec CSV
                        ax_data[line_id] = {{
                            "type": "line",
                            "x": line.get_xdata().tolist() if hasattr(line, 'get_xdata') else [],
                            "y": line.get_ydata().tolist() if hasattr(line, 'get_ydata') else [],
                            "label": line.get_label() if line.get_label() != '_nolegend_' else f"Line {{line_idx+1}}",
                            "x_label": x_label,
                            "y_label": y_label
                        }}

                # Barres
                for container_idx, container in enumerate(ax.containers):
                    if isinstance(container, matplotlib.container.BarContainer):
                        container_id = f"ax{{ax_idx}}_bars{{container_idx}}"
                        rectangles = container.get_children()
                        bar_data = {{
                            "type": "bar",
                            "heights": [],
                            "positions": [],
                            "widths": [],
                            "x_label": x_label,
                            "y_label": y_label
                        }}
                        for rect in rectangles:
                            if hasattr(rect, 'get_height') and hasattr(rect, 'get_x') and hasattr(rect, 'get_width'):
                                bar_data["heights"].append(rect.get_height())
                                bar_data["positions"].append(rect.get_x())
                                bar_data["widths"].append(rect.get_width())
                        ax_data[container_id] = bar_data
                        
                        # Essayer de convertir les donn√©es de barres en CSV avec contexte
                        try:
                            import pandas as pd
                            
                            # Cr√©er un DataFrame pour les barres avec noms personnalis√©s
                            bar_df = pd.DataFrame({{
                                x_label: bar_data["positions"],
                                f"{{y_label}} (hauteur)": bar_data["heights"],
                                "Largeur": bar_data["widths"]
                            }})
                            
                            # CORRECTION: R√©cup√©rer le titre de l'axe depuis ax_data ou utiliser une valeur par d√©faut
                            current_ax_title = ax_data.get("title", f"Figure {{_fig_counter}}")
                            
                            # Ajouter des m√©tadonn√©es en commentaire CSV
                            csv_header = f"# {{current_ax_title}} - Graphique √† barres\\n"
                            csv_header += f"# Source: Figure {{_fig_counter}}, Container {{container_idx+1}}\\n"
                            csv_header += f"# X: {{x_label}}, Y: {{y_label}}\\n"
                            
                            # Ajouter des statistiques
                            if len(bar_data["heights"]) > 0:
                                heights = np.array(bar_data["heights"])
                                stats = f"# Statistiques hauteurs: min={{heights.min():.2f}}, max={{heights.max():.2f}}, moyenne={{heights.mean():.2f}}\\n"
                                csv_header += stats
                            
                            ax_data[container_id]["csv_data"] = csv_header + bar_df.to_csv(index=False)
                        except Exception as e:
                            vis_logger.error(f"Erreur lors de la conversion des donn√©es de barres en CSV am√©lior√©: {{e}}")

                # Collections (scatter plots, heatmaps)
                for collection_idx, collection in enumerate(ax.collections):
                    collection_id = f"ax{{ax_idx}}_collection{{collection_idx}}"
                    collection_data = {{"type": "collection", "x_label": x_label, "y_label": y_label}}

                    # Points (scatter)
                    if hasattr(collection, 'get_offsets'):
                        offsets = collection.get_offsets()
                        if len(offsets) > 0:
                            collection_data["points"] = offsets.tolist() if hasattr(offsets, 'tolist') else []
                            
                            # Convertir en CSV am√©lior√© avec contexte
                            try:
                                import pandas as pd
                                offsets_array = np.array(offsets)
                                if offsets_array.shape[1] == 2:  # Si ce sont des points 2D
                                    # Utiliser les labels des axes comme noms de colonnes
                                    points_df = pd.DataFrame({{
                                        x_label: offsets_array[:, 0],
                                        y_label: offsets_array[:, 1]
                                    }})
                                    
                                    # CORRECTION: R√©cup√©rer le titre de l'axe depuis ax_data ou utiliser une valeur par d√©faut
                                    current_ax_title = ax_data.get("title", f"Figure {{_fig_counter}}")
                                    
                                    # Ajouter des m√©tadonn√©es en commentaire CSV
                                    csv_header = f"# {{current_ax_title}} - Nuage de points\\n"
                                    csv_header += f"# Source: Figure {{_fig_counter}}, Collection {{collection_idx+1}}\\n"
                                    
                                    # Ajouter des statistiques
                                    x_stats = f"# Statistiques {{x_label}}: min={{offsets_array[:, 0].min():.2f}}, max={{offsets_array[:, 0].max():.2f}}, moyenne={{offsets_array[:, 0].mean():.2f}}\\n"
                                    y_stats = f"# Statistiques {{y_label}}: min={{offsets_array[:, 1].min():.2f}}, max={{offsets_array[:, 1].max():.2f}}, moyenne={{offsets_array[:, 1].mean():.2f}}\\n"
                                    csv_header += x_stats + y_stats
                                    
                                    collection_data["csv_data"] = csv_header + points_df.to_csv(index=False)
                            except Exception as e:
                                vis_logger.error(f"Erreur lors de la conversion des points en CSV am√©lior√©: {{e}}")

                    # Couleurs
                    if hasattr(collection, 'get_array') and collection.get_array() is not None:
                        collection_data["values"] = collection.get_array().tolist() if hasattr(collection.get_array(), 'tolist') else []

                    ax_data[collection_id] = collection_data

                # Tentative d'extraire les √©tiquettes des axes
                if ax.get_title():
                    ax_data["title"] = ax.get_title()
                if ax.get_xlabel():
                    ax_data["xlabel"] = ax.get_xlabel()
                if ax.get_ylabel():
                    ax_data["ylabel"] = ax.get_ylabel()

                # Tentative d'extraire les limites des axes
                if ax.get_xlim():
                    ax_data["xlim"] = ax.get_xlim()
                if ax.get_ylim():
                    ax_data["ylim"] = ax.get_ylim()

                fig_data[f"axes{{ax_idx}}"] = ax_data

            # Information globale sur la figure
            if fig.get_label():
                fig_data["title"] = fig.get_label()
            else:
                fig_data["title"] = f"Figure {{_fig_counter}}"
                
            # Essayer d'extraire directement un dataframe de la figure si possible
            fig_csv_data = ""
            try:
                # Parcourir les objets pour trouver un DataFrame
                for ax in fig.get_axes():
                    for line in ax.get_lines():
                        if hasattr(line, '_data_source') and hasattr(line._data_source, 'to_csv'):
                            title = fig_data.get("title", "Figure")
                            x_label = ax.get_xlabel() or "X"
                            y_label = ax.get_ylabel() or "Y"
                            
                            # Cr√©er un en-t√™te CSV avec m√©tadonn√©es
                            csv_header = f"# {{title}} - Donn√©es source\\n"
                            csv_header += f"# Axes: {{x_label}} vs {{y_label}}\\n"
                            # Ajouter des statistiques si possible
                            if hasattr(line._data_source, 'describe'):
                                try:
                                    describe = line._data_source.describe().to_dict()
                                    for col, stats in describe.items():
                                        csv_header += f"# Statistiques {{col}}: min={{stats.get('min', 'N/A')}}, max={{stats.get('max', 'N/A')}}, moyenne={{stats.get('mean', 'N/A')}}\\n"
                                except:
                                    pass
                            
                            fig_csv_data = csv_header + line._data_source.to_csv(index=False)
                            break
            except Exception as e:
                vis_logger.error(f"Erreur lors de l'extraction directe du DataFrame: {{e}}")
            
            if fig_csv_data:
                fig_data["csv_data"] = fig_csv_data

            try:
                plt.savefig(filepath, bbox_inches='tight', dpi=100)
                vis_logger.info(f"Visualisation sauvegard√©e : {{filepath}}")

                # Sauvegarde des donn√©es dans un fichier JSON
                data_filepath = os.path.join(VIS_DIR, f"figure_{{_fig_counter}}_data.json")
                with open(data_filepath, 'w', encoding='utf-8') as f:
                    json.dump(fig_data, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
                vis_logger.info(f"Donn√©es de visualisation sauvegard√©es : {{data_filepath}}")
                
                # Sauvegarde des donn√©es CSV si disponibles
                if any(["csv_data" in data for ax_data in fig_data.values() if isinstance(ax_data, dict) 
                       for data in ax_data.values() if isinstance(data, dict)]) or "csv_data" in fig_data:
                    csv_filepath = os.path.join(VIS_DIR, f"figure_{{_fig_counter}}_data.csv")
                    
                    # Choisir la premi√®re source de donn√©es CSV disponible
                    csv_content = fig_data.get("csv_data", "")
                    if not csv_content:
                        for ax_data in fig_data.values():
                            if isinstance(ax_data, dict):
                                for data in ax_data.values():
                                    if isinstance(data, dict) and "csv_data" in data:
                                        csv_content = data["csv_data"]
                                        break
                                if csv_content:
                                    break
                    
                    if csv_content:
                        with open(csv_filepath, 'w', encoding='utf-8') as f:
                            f.write(csv_content)
                        vis_logger.info(f"Donn√©es CSV am√©lior√©es sauvegard√©es : {{csv_filepath}}")

            except Exception as e:
                vis_logger.error(f"Impossible de sauvegarder la figure {{_fig_counter}} √† {{filepath}}: {{e}}")

            # On permet l'affichage pour d√©bogage mais √ßa sera ignor√© dans l'environnement non-interactif
            _original_show(*args, **kwargs)
    except Exception as e:
        vis_logger.error(f"Erreur dans _custom_show: {{e}}")

plt.show = _custom_show

# Fonction pour capturer les sorties de r√©gression OLS de statsmodels
_original_print = print
def _custom_print(*args, **kwargs):
    output = " ".join(str(arg) for arg in args)
    _original_print(*args, **kwargs)  # Affiche normalement

    # D√©tecter si c'est un r√©sultat de r√©gression OLS
    if "OLS Regression Results" in output and "=" * 10 in output:
        try:
            # Extraire la table de r√©gression compl√®te
            global _tables_counter
            if '_tables_counter' not in globals():
                _tables_counter = 0
            _tables_counter += 1

            # Sauvegarder le texte
            filepath = os.path.join(TABLES_DIR, f"regression_{{_tables_counter}}.txt")
            with open(filepath, "w") as f:
                f.write(output)

            # Cr√©er une visualisation de la table
            img_path = os.path.join(TABLES_DIR, f"regression_{{_tables_counter}}.png")
            plt.figure(figsize=(12, 10))
            plt.text(0.01, 0.99, output, family='monospace', fontsize=10, verticalalignment='top')
            plt.axis('off')
            plt.tight_layout()
            plt.savefig(img_path, dpi=100, bbox_inches='tight')
            plt.close()

            # Extraire et sauvegarder les donn√©es de r√©gression
            # Extraire R-squared
            r_squared_match = re.search(r"R-squared:\\s*([\d\.]+)", output)
            r_squared = r_squared_match.group(1) if r_squared_match else "N/A"

            # Extraire les variables et coefficients
            coef_section = re.search(r"==+\\s*\\n\\s*(coef.*?)(?:\\n\\s*==+|\\Z)", output, re.DOTALL)
            regression_data = {{
                "r_squared": r_squared,
                "coefficients": []
            }}

            if coef_section:
                lines = coef_section.group(1).split('\\n') # Utilisation de \\n car c'est dans une string Python
                headers = None

                for line in lines:
                    if "coef" in line.lower():
                        # C'est la ligne d'en-t√™te
                        headers = re.split(r'\\s{{2,}}', line.strip()) # Utilisation de \\s{{2,}}
                    elif line.strip() and headers:
                        # C'est une ligne de donn√©es
                        values = re.split(r'\\s{{2,}}', line.strip()) # Utilisation de \\s{{2,}}
                        if len(values) >= len(headers):
                            variable = values[0]
                            coef_data = {{
                                "variable": variable,
                                "coef": values[1] if len(values) > 1 else "N/A",
                                "std_err": values[2] if len(values) > 2 else "N/A",
                                "p_value": values[4] if len(values) > 4 else "N/A"
                            }}
                            regression_data["coefficients"].append(coef_data)
                            
            # NOUVELLE SECTION: Convertir les coefficients en CSV am√©lior√©
            try:
                import pandas as pd
                if regression_data["coefficients"]:
                    coef_df = pd.DataFrame(regression_data["coefficients"])
                    
                    # Ajouter une colonne de significativit√© pour faciliter l'interpr√©tation
                    try:
                        coef_df['significatif'] = coef_df['p_value'].astype(float) < 0.05
                    except:
                        # Si conversion en float √©choue, on continue sans cette colonne
                        pass
                    
                    # Ajouter des m√©tadonn√©es contextuelles en commentaire
                    csv_header = f"# R√©sultats de r√©gression OLS - Table {{_tables_counter}}\\n"
                    csv_header += f"# R¬≤: {{r_squared}}\\n"
                    csv_header += f"# Interpr√©tation: un coefficient positif indique une relation positive avec la variable d√©pendante\\n"
                    csv_header += f"# Une p-value < 0.05 indique que le coefficient est statistiquement significatif\\n"
                    
                    regression_data["csv_data"] = csv_header + coef_df.to_csv(index=False)
                    
                    # Sauvegarder aussi en fichier CSV am√©lior√©
                    csv_path = os.path.join(TABLES_DIR, f"regression_{{_tables_counter}}_coefficients.csv")
                    with open(csv_path, 'w', encoding='utf-8') as f:
                        f.write(regression_data["csv_data"])
                    vis_logger.info(f"Donn√©es CSV am√©lior√©es de coefficients sauvegard√©es: {{csv_path}}")
            except Exception as e:
                vis_logger.error(f"Erreur lors de la conversion des coefficients en CSV am√©lior√©: {{e}}")

            # Sauvegarder les donn√©es
            data_path = os.path.join(TABLES_DIR, f"regression_{{_tables_counter}}_data.json")
            with open(data_path, 'w', encoding='utf-8') as f:
                json.dump(regression_data, f, ensure_ascii=False, indent=2)

            vis_logger.info(f"Table de r√©gression sauvegard√©e : {{filepath}}")
            vis_logger.info(f"Donn√©es de r√©gression sauvegard√©es : {{data_path}}")
        except Exception as e:
            vis_logger.error(f"Erreur lors de la capture d'une table de r√©gression: {{e}}")

# Remplacer la fonction print
print = _custom_print

# Fonction manuelle pour sauvegarder des figures
def save_figure(fig, name):
    filepath = os.path.join(VIS_DIR, f"{{name}}.png")
    fig.savefig(filepath, bbox_inches='tight', dpi=100)
    vis_logger.info(f"Figure sauvegard√©e manuellement: {{filepath}}")
    return filepath
""") # Fin de la string vis_code


    # Initialiser les variables pour la boucle d'ex√©cution
    temp_filename = os.path.join(temp_dir, "analysis_script.py")
    attempt = 0
    llm_correction_attempt = 0  # Compteur sp√©cifique pour les tentatives de correction par LLM
    execution_results = {"success": False}
    execution_output = ""
    all_code_versions = []
    recent_errors = deque(maxlen=3)
    max_attempts = 5
    current_stderr = ""

    # Flag pour le mod√®le puissant
    tried_powerful_model = False
    
    # Boucle principale d'ex√©cution
    while attempt < max_attempts:
        attempt += 1
        logger.info(f"--- Tentative d'ex√©cution {attempt}/{max_attempts} ---")

        # Sanitize le code avant l'ex√©cution
        try:
            valid_columns = agent1_data["metadata"].get("noms_colonnes", [])
            sanitized_code = sanitize_code(code, csv_file, valid_columns)
            if sanitized_code != code:
                logger.info("Code assaini (chemin CSV, colonnes, df_numeric).")
                code = sanitized_code
        except Exception as e:
            logger.error(f"Erreur pendant l'assainissement du code: {e}. Tentative avec le code actuel.")

        # Sauvegarder la version actuelle du code
        current_code_path = os.path.join(code_versions_dir, f"attempt_{attempt}.py")
        try:
            with open(current_code_path, "w", encoding="utf-8") as f:
                f.write(code)
            all_code_versions.append(current_code_path)
            logger.info(f"Version {attempt} du code sauvegard√©e: {current_code_path}")
        except Exception as e:
             logger.error(f"Impossible de sauvegarder la version {attempt} du code : {e}")

        # Par celles-ci:
        import textwrap
        vis_code_dedented = textwrap.dedent(vis_code)  # Conserve l'indentation relative
        modified_code = vis_code_dedented + "\n\n" + code.strip()

        # √âcrire le code modifi√© dans un fichier temporaire
        try:
            with open(temp_filename, "w", encoding="utf-8") as f:
                f.write(modified_code)
        except Exception as e:
            logger.error(f"Impossible d'√©crire le script temporaire {temp_filename}: {e}")
            execution_results = {"success": False, "error": "Erreur √©criture fichier temporaire", "stderr": str(e)}
            break

                            # <<< AJOUTER CE BLOC DE D√âBOGAGE >>>
        logger.info(f"--- DEBUG: V√©rification du contenu √©crit dans {temp_filename} ---")
        try:
            with open(temp_filename, "r", encoding="utf-8") as f_read:
                lines_to_check = 5
                lines = []
                for i in range(lines_to_check):
                    try:
                        lines.append(next(f_read).rstrip('\n'))
                    except StopIteration:
                        break # Fin du fichier
                logger.info(f"Premi√®res {len(lines)} lignes (espaces repr√©sent√©s par '¬∑', tabulations par '\\t'):")
                for i, line in enumerate(lines):
                    # Repr√©sentation visuelle des espaces/tabs
                    line_repr = line.replace(' ', '¬∑').replace('\t', '\\t') 
                    logger.info(f"  Ligne {i+1}: [{line_repr}]")
        except Exception as read_err:
            logger.error(f"  Erreur de lecture du fichier temporaire pour d√©bogage: {read_err}")
        logger.info("--- FIN DEBUG ---")
        # <<< FIN DU BLOC DE D√âBOGAGE >>>


        # Ex√©cuter le code
        try:
            logger.info(f"Ex√©cution de: python3 {temp_filename}")
            result = subprocess.run(
                ["python3", temp_filename],
                capture_output=True, text=True, check=True, timeout=300
            )
            logger.info("Ex√©cution r√©ussie")
            logger.debug(f"Sortie standard (stdout):\n{result.stdout}")
            execution_output = result.stdout
            execution_results = {
                "success": True,
                "output": result.stdout,
                "temp_dir": temp_dir,
                "vis_dir": vis_dir,
                "tables_dir": tables_dir,
                "script_path": temp_filename,
                "all_code_versions": all_code_versions,
                "final_code_used_path": current_code_path
            }

            # Traiter les visualisations
            vis_files = []
            if os.path.exists(vis_dir):
                logger.info(f"Recherche de visualisations dans {vis_dir}")
                for file in sorted(os.listdir(vis_dir)):
                    if file.lower().endswith(('.png', '.jpg', '.jpeg', '.svg')):
                        file_path = os.path.join(vis_dir, file)
                        try:
                            with open(file_path, 'rb') as img_file:
                                img_data = base64.b64encode(img_file.read()).decode('utf-8')
                            # V√©rifier la taille de l'image pour le d√©bogage
                            file_size = os.path.getsize(file_path)
                            logger.info(f"  -> Visualisation trouv√©e: {file}, taille: {file_size} octets")
                            
                            # Chercher les donn√©es associ√©es
                            fig_name = os.path.splitext(file)[0]
                            data_path = os.path.join(vis_dir, f"{fig_name}_data.json")
                            chart_data = None
                            if os.path.exists(data_path):
                                try:
                                    with open(data_path, 'r', encoding='utf-8') as f:
                                        chart_data = json.load(f)
                                    logger.info(f"  -> Donn√©es associ√©es trouv√©es pour: {file}")
                                except Exception as e:
                                    logger.error(f"Impossible de lire les donn√©es associ√©es √† {file}: {e}")
                            
                            # Chercher les donn√©es CSV associ√©es
                            csv_path = os.path.join(vis_dir, f"{fig_name}_data.csv")
                            csv_data = ""
                            if os.path.exists(csv_path):
                                try:
                                    with open(csv_path, 'r', encoding='utf-8') as f:
                                        csv_data = f.read()
                                    logger.info(f"  -> Donn√©es CSV trouv√©es pour: {file}")
                                except Exception as e:
                                    logger.error(f"Impossible de lire les donn√©es CSV associ√©es √† {file}: {e}")
                            
                            # Si pas de fichier CSV, essayer d'extraire des donn√©es CSV imbriqu√©es
                            if not csv_data and chart_data:
                                # Parcourir chart_data pour trouver csv_data
                                for axes_key, axes_value in chart_data.items():
                                    if isinstance(axes_value, dict):
                                        for line_key, line_value in axes_value.items():
                                            if isinstance(line_value, dict) and 'csv_data' in line_value:
                                                csv_data = line_value['csv_data']
                                                logger.info(f"  -> Donn√©es CSV int√©gr√©es trouv√©es pour: {file}")
                                                break
                                    if csv_data:
                                        break
                            
                            vis_files.append({
                                'filename': file,
                                'path': file_path,
                                'base64': img_data,
                                'title': chart_data.get('title', ' '.join(os.path.splitext(file)[0].split('_')).capitalize()) if chart_data else ' '.join(os.path.splitext(file)[0].split('_')).capitalize(),
                                'size': file_size,
                                'data': chart_data,
                                'csv_data': csv_data  # Conserver les donn√©es CSV pour compatibilit√©
                            })
                        except Exception as e:
                            logger.error(f"Impossible de lire ou encoder l'image {file_path}: {e}")
            else:
                logger.warning(f"R√©pertoire de visualisations non trouv√©: {vis_dir}")

            # Capturer les tables de r√©gression
            regression_outputs = capture_regression_outputs(result.stdout, tables_dir)
            
            # V√©rifier √©galement si des tables ont √©t√© sauvegard√©es directement par le script
            if os.path.exists(tables_dir):
                logger.info(f"Recherche de tables de r√©gression dans {tables_dir}")
                for file in sorted(os.listdir(tables_dir)):
                    # Ne traiter que les fichiers texte qui ne sont pas d√©j√† trait√©s
                    if file.lower().endswith('.txt') and not any(r['text_path'].endswith(file) for r in regression_outputs):
                        text_path = os.path.join(tables_dir, file)
                        img_path = os.path.join(tables_dir, os.path.splitext(file)[0] + '.png')
                        
                        # Lire le contenu du fichier texte
                        try:
                            with open(text_path, 'r', encoding='utf-8') as f:
                                table_content = f.read()
                            
                            # V√©rifier si c'est une table de r√©gression OLS
                            if "OLS Regression Results" in table_content:
                                table_id = os.path.splitext(file)[0]
                                
                                # Cr√©er l'image si elle n'existe pas
                                if not os.path.exists(img_path):
                                    plt.figure(figsize=(12, 10))
                                    plt.text(0.01, 0.99, table_content, family='monospace', fontsize=10, verticalalignment='top')
                                    plt.axis('off')
                                    plt.tight_layout()
                                    plt.savefig(img_path, dpi=100, bbox_inches='tight')
                                    plt.close()
                                
                                # Encoder l'image en base64
                                with open(img_path, 'rb') as img_file:
                                    img_data = base64.b64encode(img_file.read()).decode('utf-8')
                                
                                # Extraire des m√©tadonn√©es basiques
                                r_squared_match = re.search(r"R-squared:\s*([\d\.]+)", table_content)
                                r_squared = r_squared_match.group(1) if r_squared_match else "N/A"
                                
                                # Chercher le fichier de donn√©es JSON associ√©
                                data_path = os.path.join(tables_dir, f"{table_id}_data.json")
                                regression_data = None
                                if os.path.exists(data_path):
                                    try:
                                        with open(data_path, 'r', encoding='utf-8') as f:
                                            regression_data = json.load(f)
                                        logger.info(f"  -> Donn√©es de r√©gression trouv√©es pour: {file}")
                                    except Exception as e:
                                        logger.error(f"Impossible de lire les donn√©es de r√©gression pour {file}: {e}")
                                
                                # Chercher les donn√©es CSV des coefficients
                                csv_path = os.path.join(tables_dir, f"{table_id}_coefficients.csv")
                                csv_data = ""
                                if os.path.exists(csv_path):
                                    try:
                                        with open(csv_path, 'r', encoding='utf-8') as f:
                                            csv_data = f.read()
                                        logger.info(f"  -> Donn√©es CSV des coefficients trouv√©es pour: {file}")
                                    except Exception as e:
                                        logger.error(f"Impossible de lire les donn√©es CSV pour {file}: {e}")
                                
                                # Si pas de fichier CSV mais donn√©es JSON disponibles
                                if not csv_data and regression_data and "csv_data" in regression_data:
                                    csv_data = regression_data["csv_data"]
                                    logger.info(f"  -> Donn√©es CSV int√©gr√©es trouv√©es pour: {file}")
                                
                                # V√©rifier la taille de l'image pour le d√©bogage
                                file_size = os.path.getsize(img_path)
                                logger.info(f"  -> Table de r√©gression trouv√©e: {file}, taille: {file_size} octets")
                                
                                regression_outputs.append({
                                    'type': 'regression_table',
                                    'id': table_id,
                                    'text_path': text_path,
                                    'image_path': img_path,
                                    'base64': img_data,
                                    'title': f"R√©sultats de R√©gression: {table_id.replace('_', ' ').capitalize()}",
                                    'metadata': {
                                        'r_squared': r_squared
                                    },
                                    'size': file_size,
                                    'data': regression_data,
                                    'csv_data': csv_data  # Conserver les donn√©es CSV pour compatibilit√©
                                })
                        except Exception as e:
                            logger.error(f"Erreur lors du traitement de la table {text_path}: {e}")

            # Mettre √† jour les r√©sultats
            execution_results["visualisations"] = vis_files
            execution_results["regressions"] = regression_outputs
            logger.info(f"{len(vis_files)} visualisation(s) et {len(regression_outputs)} table(s) de r√©gression trait√©e(s).")
            
            # Journaliser plus de d√©tails sur les visualisations pour le d√©bogage
            for i, vis in enumerate(vis_files):
                logger.info(f"  Visualisation #{i+1}: {vis.get('filename')}, taille: {vis.get('size', 'inconnue')} octets")
                if 'base64' in vis:
                    logger.info(f"    Longueur base64: {len(vis['base64'])} caract√®res")
                if 'data' in vis and vis['data']:
                    logger.info(f"    Donn√©es pr√©sentes: Oui")
                if 'csv_data' in vis and vis['csv_data']:
                    logger.info(f"    Donn√©es CSV pr√©sentes: {len(vis['csv_data'])} caract√®res")
                
            for i, reg in enumerate(regression_outputs):
                logger.info(f"  R√©gression #{i+1}: {reg.get('id')}, taille: {reg.get('size', 'inconnue')} octets")
                if 'base64' in reg:
                    logger.info(f"    Longueur base64: {len(reg['base64'])} caract√®res")
                if 'data' in reg and reg['data']:
                    logger.info(f"    Donn√©es structur√©es: Oui")
                if 'csv_data' in reg and reg['csv_data']:
                    logger.info(f"    Donn√©es CSV pr√©sentes: {len(reg['csv_data'])} caract√®res")
            
            break

        # Gestion des erreurs d'ex√©cution
        except subprocess.TimeoutExpired:
             logger.error("L'ex√©cution a d√©pass√© le d√©lai de 300 secondes.")
             error_message = "TimeoutExpired: Le script a mis trop de temps √† s'ex√©cuter."
             recent_errors.append(error_message.strip())
             current_stderr = error_message

        except subprocess.CalledProcessError as e:
            stderr_output = e.stderr.strip()
            logger.error(f"Erreur lors de l'ex√©cution (stderr):\n{stderr_output}")
            recent_errors.append(stderr_output)
            current_stderr = e.stderr

            # Sauvegarder l'erreur dans un fichier
            error_file = os.path.join(code_versions_dir, f"attempt_{attempt}_error.txt")
            try:
                with open(error_file, "w", encoding="utf-8") as f:
                    f.write(f"ERREUR RENCONTR√âE LORS DE LA TENTATIVE {attempt}:\n\n")
                    f.write(current_stderr)
                logger.info(f"Erreur sauvegard√©e dans: {error_file}")
            except Exception as write_err:
                 logger.error(f"Impossible de sauvegarder le fichier d'erreur {error_file}: {write_err}")

            # MODIFICATION: Utiliser le mod√®le puissant apr√®s 5 tentatives √©chou√©es
            if attempt >= 5 and not tried_powerful_model and backend == "gemini":
                logger.warning(f"Apr√®s {attempt} tentatives √©chou√©es, basculement vers le mod√®le Gemini Pro puissant")
                tried_powerful_model = True
                powerful_model = "gemini-2.5-pro-exp-03-25"
                current_model = powerful_model  # Mise √† jour du mod√®le courant
                
                # Cr√©er un prompt sp√©cifique pour r√©soudre les probl√®mes de formule OLS
                powerful_prompt = f"""Voici mon script Python qui g√©n√®re une erreur. Corrige-le avec soin:

```python
{code}
```

Erreur:
```
{current_stderr}
```

Je veux uniquement le code Python corrig√©, sans explications. Le script est utilis√© pour analyser des donn√©es avec pandas, matplotlib et statsmodels.

PROBL√àME √Ä R√âSOUDRE:
- Il y a une erreur de "unterminated string literal" dans une formule OLS
- Le probl√®me est li√© aux noms de pays avec espaces et caract√®res sp√©ciaux dans la formule
- La solution est d'utiliser des backticks (`) pour encadrer chaque nom de variable avec espaces/caract√®res sp√©ciaux
- Exemple: `Pays_Afrique du Sud` au lieu de Pays_Afrique du Sud
- OU simplifier la formule en utilisant une technique d'encodage diff√©rente pour les pays
"""
                
                # Sauvegarder le prompt pour le mod√®le puissant
                save_prompt_to_file(powerful_prompt, prompts_log_path, "Powerful Model after 5 attempts")
                logger.info(f"Envoi du prompt de correction au mod√®le puissant {powerful_model}")
                
                try:
                    powerful_correction = call_llm(prompt=powerful_prompt, model_name=powerful_model, backend="gemini")
                    corrected_code = extract_code(powerful_correction)
                    if corrected_code and len(corrected_code.strip()) > 0:
                        code = corrected_code
                        logger.info("Code corrig√© par le mod√®le puissant re√ßu")
                        recent_errors.clear()
                        continue
                    else:
                        logger.warning("Le mod√®le puissant n'a pas renvoy√© de code utilisable")
                except Exception as powerful_err:
                    logger.error(f"Erreur avec le mod√®le puissant: {powerful_err}")

            # V√©rifier si les erreurs se r√©p√®tent
            if len(recent_errors) == 3 and len(set(recent_errors)) == 1:
                logger.warning("Trois erreurs identiques cons√©cutives d√©tect√©es.")
                
                # Si on n'a pas encore essay√© le mod√®le puissant apr√®s 3 erreurs identiques
                if not tried_powerful_model and backend == "gemini":
                    logger.warning("Basculement vers le mod√®le Gemini Pro puissant pour correction")
                    tried_powerful_model = True
                    powerful_model = "gemini-2.5-pro-exp-03-25"
                    current_model = powerful_model  # Mise √† jour du mod√®le courant
                    
                    # Cr√©er un prompt simplifi√©
                    powerful_prompt = f"""Voici mon script Python qui g√©n√®re une erreur. Corrige-le avec soin:

```python
{code}
```

Erreur:
```
{current_stderr}
```

Je veux uniquement le code Python corrig√©, sans explications. Le script est utilis√© pour analyser des donn√©es avec pandas et matplotlib.
"""
                    
                    # Sauvegarder le prompt pour le mod√®le puissant
                    save_prompt_to_file(powerful_prompt, prompts_log_path, "Powerful Model Correction")
                    logger.info(f"Envoi du prompt de correction au mod√®le puissant {powerful_model}")
                    
                    try:
                        powerful_correction = call_llm(prompt=powerful_prompt, model_name=powerful_model, backend="gemini")
                        corrected_code = extract_code(powerful_correction)
                        if corrected_code and len(corrected_code.strip()) > 0:
                            code = corrected_code
                            logger.info("Code corrig√© par le mod√®le puissant re√ßu")
                            recent_errors.clear()
                            continue
                        else:
                            logger.warning("Le mod√®le puissant n'a pas renvoy√© de code utilisable")
                    except Exception as powerful_err:
                        logger.error(f"Erreur avec le mod√®le puissant: {powerful_err}")
                
                # Si le mod√®le puissant a √©chou√© ou a d√©j√† √©t√© essay√©, passer √† la correction manuelle
                logger.warning("Passage √† la correction manuelle.")
                manual_edit_path = os.path.join(code_versions_dir, f"manual_fix_for_attempt_{attempt+1}.py")

                if not os.path.exists(manual_edit_path):
                    try:
                        with open(manual_edit_path, "w", encoding="utf-8") as f:
                            f.write(code)
                        logger.info(f"Fichier de correction manuelle cr√©√© : {manual_edit_path}")
                    except Exception as create_err:
                        logger.error(f"Erreur lors de la cr√©ation du fichier de correction manuelle {manual_edit_path}: {create_err}")
                else:
                    try:
                        import shutil
                        shutil.copyfile(manual_edit_path, manual_edit_path + ".bak")
                        logger.info(f"Backup du fichier de correction manuelle cr√©√© : {manual_edit_path}.bak")
                    except Exception as copy_err:
                        logger.error(f"Erreur lors de la sauvegarde du fichier {manual_edit_path}: {copy_err}")

                logger.info(f"Modifiez le fichier {manual_edit_path} si n√©cessaire.")

                import sys

                # Afficher le message sur stderr (pas stdout)
                sys.stderr.write("‚è∏Ô∏è Appuyez sur Entr√©e pour continuer apr√®s modification...\n")
                sys.stderr.flush()  # Assurez-vous que le message s'affiche imm√©diatement
                try:
                    input()  # Pas de message dans input() pour √©viter qu'il aille dans stdout
                except EOFError:
                    logger.warning("Pas d'entr√©e utilisateur d√©tect√©e, poursuite automatique.")

                try:
                    with open(manual_edit_path, "r", encoding="utf-8") as f:
                        edited_code = f.read()
                except Exception as read_err:
                    logger.error(f"Erreur lors de la lecture du fichier {manual_edit_path}: {read_err}")
                    edited_code = code

                if edited_code != code:
                    logger.info(f"Code modifi√© d√©tect√© dans {manual_edit_path}. Mise √† jour pour la prochaine tentative.")
                    code = edited_code
                    recent_errors.clear()
                    continue
                else:
                    logger.info(f"Aucune modification d√©tect√©e dans {manual_edit_path}. Passage √† la correction automatique par LLM.")

            # Tentative de correction automatique par LLM standard
            if attempt < max_attempts:
                llm_correction_attempt += 1
                logger.info(f"Tentative de correction LLM #{llm_correction_attempt}")
                
                # Cr√©er le prompt pour la correction
                metadata_str = json.dumps(agent1_data["metadata"], indent=2, ensure_ascii=False)
                recall_prompt = f"""
Le contexte suivant concerne l'analyse d'un fichier CSV :
{metadata_str}

Le chemin absolu du fichier CSV est : {csv_file}
Assure-toi d'utiliser ce chemin exact dans pd.read_csv('{csv_file}').

Le code Python ci-dessous, complet avec toutes ses fonctionnalit√©s (gestion des visualisations, sauvegarde des versions, correction automatique et manuelle, etc.), a g√©n√©r√© l'erreur suivante lors de son ex√©cution :
------------------------------------------------------------
Code Fautif :
```python
{code}
```
Erreur Rencontr√©e : {current_stderr}
TA MISSION : Corrige uniquement l'erreur indiqu√©e sans modifier la logique globale du code. Garde int√©gralement la structure et l'ensemble des fonctionnalit√©s du code initial. Ne simplifie pas le script : toutes les parties (gestion des visualisations, sauvegarde des versions, correction manuelle, etc.) doivent √™tre conserv√©es. GARDE les noms de colonnes exacts. Colonnes valides : {agent1_data["metadata"].get("noms_colonnes", [])}

RENVOIE UNIQUEMENT le code Python corrig√©, encapsul√© dans un bloc de code d√©limit√© par trois backticks (python ... ), sans explications suppl√©mentaires. 

Fais bien attention a la nature des variables, num√©riques, cat√©gorielles, etc.

IMPORTANT: Pour les styles dans matplotlib, utilise 'seaborn-v0_8-whitegrid' au lieu de 'seaborn-whitegrid' qui est obsol√®te.
"""

                # Sauvegarder le prompt
                save_prompt_to_file(recall_prompt, prompts_log_path, f"Code Correction Attempt {attempt} (LLM #{llm_correction_attempt})")
                logger.info(f"Envoi du prompt de correction au LLM via backend '{backend}' avec mod√®le '{current_model}'...")
                
                try:
                    # Obtenir et traiter la correction du LLM
                    new_generated_script = call_llm(prompt=recall_prompt, model_name=current_model, backend=backend)
                    
                    extracted_code = extract_code(new_generated_script)
                    clean_code = remove_shell_commands(extracted_code)

                    if not clean_code.strip():
                        logger.warning("Le LLM a renvoy√© un code vide apr√®s nettoyage. R√©utilisation du code pr√©c√©dent.")
                    else:
                        code = clean_code # Mettre √† jour le code pour la prochaine tentative
                        logger.info("Code corrig√© par le LLM re√ßu et nettoy√©.")

                    time.sleep(1)

                except Exception as llm_call_err:
                    logger.error(f"Erreur lors de l'appel au LLM pour correction: {llm_call_err}")
                    
                    # Si l'appel au LLM standard a √©chou√© et qu'on n'a pas encore essay√© le mod√®le puissant
                    if not tried_powerful_model and backend == "gemini":
                        logger.warning("Tentative avec le mod√®le Gemini Pro puissant apr√®s √©chec du LLM standard")
                        tried_powerful_model = True
                        powerful_model = "gemini-2.5-pro-exp-03-25"
                        current_model = powerful_model  # Mise √† jour du mod√®le courant
                        
                        # Cr√©er un prompt simplifi√©
                        powerful_prompt = f"""Voici mon script Python qui g√©n√®re une erreur. Corrige-le avec soin:

```python
{code}
```

Erreur:
```
{current_stderr}
```

Je veux uniquement le code Python corrig√©, sans explications. Le script est utilis√© pour analyser des donn√©es avec pandas et matplotlib. 
Assure-toi d'utiliser 'seaborn-v0_8-whitegrid' au lieu de 'seaborn-whitegrid' qui est obsol√®te.
"""
                        
                        # Sauvegarder le prompt pour le mod√®le puissant
                        save_prompt_to_file(powerful_prompt, prompts_log_path, "Powerful Model Correction After LLM Failure")
                        logger.info(f"Envoi du prompt de correction au mod√®le puissant {powerful_model}")
                        
                        try:
                            powerful_correction = call_llm(prompt=powerful_prompt, model_name=powerful_model, backend="gemini")
                            corrected_code = extract_code(powerful_correction)
                            if corrected_code and len(corrected_code.strip()) > 0:
                                code = corrected_code
                                logger.info("Code corrig√© par le mod√®le puissant re√ßu")
                                recent_errors.clear()
                                continue
                            else:
                                logger.warning("Le mod√®le puissant n'a pas renvoy√© de code utilisable")
                        except Exception as powerful_err:
                            logger.error(f"Erreur avec le mod√®le puissant: {powerful_err}")
                    
                    # Si aucun LLM ne fonctionne, passer √† la correction manuelle en dernier recours
                    logger.warning("Toutes les tentatives de correction automatique ont √©chou√©. Passage √† la correction manuelle.")
                    manual_edit_path = os.path.join(code_versions_dir, f"manual_fix_final_attempt.py")
                    try:
                        with open(manual_edit_path, "w", encoding="utf-8") as f:
                            f.write(code)
                        logger.info(f"Fichier de correction manuelle finale cr√©√© : {manual_edit_path}")
                        
                        # Attendre la correction manuelle
                        sys.stderr.write("‚è∏Ô∏è CORRECTION FINALE MANUELLE REQUISE. Modifiez le fichier puis appuyez sur Entr√©e...\n")
                        sys.stderr.flush()
                        try:
                            input()
                        except EOFError:
                            logger.warning("Pas d'entr√©e utilisateur d√©tect√©e, poursuite automatique.")
                            
                        with open(manual_edit_path, "r", encoding="utf-8") as f:
                            edited_code = f.read()
                        code = edited_code
                        recent_errors.clear()
                        
                    except Exception as manual_err:
                        logger.error(f"Erreur lors de la correction manuelle finale: {manual_err}")
                        execution_results = {
                            "success": False, 
                            "error": f"√âchec complet - correction automatique et manuelle: {manual_err}",
                            "stderr": current_stderr,
                            "all_code_versions": all_code_versions
                        }
                        break

        except Exception as general_err:
            logger.error(f"Erreur g√©n√©rale inattendue lors de la tentative {attempt}: {general_err}", exc_info=True)
            error_message = f"Erreur g√©n√©rale inattendue: {general_err}"
            recent_errors.append(error_message.strip())
            execution_results = {
                "success": False,
                "error": error_message,
                "stderr": current_stderr if current_stderr else str(general_err),
                "all_code_versions": all_code_versions
            }
            break

    # Ajouter les interpr√©tations pour les visualisations et tables de r√©gression
    if execution_results.get("success"):
        all_visuals = []
        
        # Traiter les visualisations
        if "visualisations" in execution_results:
            all_visuals.extend(execution_results["visualisations"])
            
        # Traiter les tables de r√©gression
        if "regressions" in execution_results:
            all_visuals.extend(execution_results["regressions"])
        
        if all_visuals:
            logger.info("G√©n√©ration d'interpr√©tations pour les visualisations et tables avec Gemini")
            all_visuals_with_interpretations = generate_visualization_interpretations(
                execution_results.get("visualisations", []),
                execution_results.get("regressions", []),
                agent1_data,
                current_model,  # Utilisation du mod√®le courant (qui peut √™tre le mod√®le puissant)
                backend,
                prompts_log_path
            )
            
            # Mettre √† jour les r√©sultats avec les interpr√©tations
            execution_results["all_visuals"] = all_visuals_with_interpretations

    # Finaliser et journaliser les r√©sultats
    final_status = "success" if execution_results.get("success") else "failed"
    logger.info(f"Fin de la boucle d'ex√©cution. Statut final: {final_status}. Total tentatives: {attempt}")

    # Sauvegarder une copie du code final
    final_script_path = None
    last_code_version_path = all_code_versions[-1] if all_code_versions else None

    if last_code_version_path and os.path.exists(last_code_version_path):
        final_script_name = f"analysis_script_{timestamp}_{final_status}.py"
        outputs_dir = "outputs"
        os.makedirs(outputs_dir, exist_ok=True)
        final_script_path = os.path.join(outputs_dir, final_script_name)

        try:
            shutil.copyfile(last_code_version_path, final_script_path)
            logger.info(f"Version finale du code ({final_status}) copi√©e vers: {final_script_path}")
        except Exception as copy_err:
            logger.error(f"Impossible de copier le script final {last_code_version_path} vers {final_script_path}: {copy_err}")
            final_script_path = None
    elif not all_code_versions:
        logger.warning("Aucune version de code n'a √©t√© g√©n√©r√©e ou sauvegard√©e.")
    else:
        logger.warning(f"Le dernier fichier de code {last_code_version_path} n'existe pas. Impossible de sauvegarder le script final.")

    # Cr√©er un fichier d'index pour documenter la session
    index_file_path = os.path.join(code_versions_dir, "index.txt")
    try:
        with open(index_file_path, "w", encoding="utf-8") as f:
            f.write(f"SESSION D'ANALYSE DU {timestamp}\n")
            f.write(f"Fichier CSV: {csv_file}\n")
            f.write(f"Mod√®le LLM: {model}\n")
            f.write(f"Mod√®le LLM courant √† la fin: {current_model}\n")
            f.write(f"Nombre de versions de code g√©n√©r√©es/sauvegard√©es: {len(all_code_versions)}\n")
            f.write(f"Nombre total de tentatives d'ex√©cution effectu√©es: {attempt}\n")
            f.write(f"Nombre de tentatives de correction LLM: {llm_correction_attempt}\n")
            f.write(f"Mod√®le puissant utilis√©: {'Oui' if tried_powerful_model else 'Non'}\n")
            f.write(f"R√©sultat final: {'Succ√®s' if execution_results.get('success') else '√âchec'}\n")

            if not execution_results.get("success"):
                f.write(f"Derni√®re erreur enregistr√©e: {execution_results.get('error', 'N/A')}\n")
                last_stderr = execution_results.get('stderr')
                if last_stderr:
                    f.write(f"Dernier stderr:\n```\n{last_stderr}\n```\n")

            f.write("\nLISTE DES VERSIONS DE CODE G√âN√âR√âES/UTILIS√âES:\n")
            for i, path in enumerate(all_code_versions, 1):
                f.write(f"- Version {i} (Tentative {i}): {os.path.basename(path)}\n")
                error_path = path.replace(".py", "_error.txt")
                if os.path.exists(error_path):
                    f.write(f"  - Erreur associ√©e: {os.path.basename(error_path)}\n")
                manual_edit_trigger_path = os.path.join(os.path.dirname(path), f"manual_fix_for_attempt_{i+1}.py")
                if os.path.exists(manual_edit_trigger_path):
                    f.write(f"  - Fichier pour √©dition manuelle (cr√©√© apr√®s √©chec tentative {i}): {os.path.basename(manual_edit_trigger_path)}\n")

            if final_script_path and os.path.exists(final_script_path):
                f.write(f"\nScript final utilis√© ({final_status}): {os.path.basename(final_script_path)}\n")
                f.write(f"Chemin complet: {final_script_path}\n")
            else:
                if last_code_version_path and not os.path.exists(last_code_version_path):
                    f.write(f"\nScript final non sauvegard√© (fichier source {os.path.basename(last_code_version_path)} introuvable).\n")
                elif final_script_path is None and last_code_version_path:
                    f.write(f"\nScript final non sauvegard√© (√©chec de la copie depuis {os.path.basename(last_code_version_path)}).\n")
                else:
                    f.write("\nScript final non sauvegard√© (aucune version de code disponible).\n")

            if os.path.exists(prompts_log_path):
                f.write(f"\nJournal des prompts envoy√©s au LLM: {os.path.basename(prompts_log_path)}\n")
                f.write(f"Chemin complet: {prompts_log_path}\n")

            f.write(f"\nR√©pertoire des versions de code: {code_versions_dir}\n")
            if execution_results.get("success"):
                vis_dir_final = execution_results.get('vis_dir', os.path.join(temp_dir, "visualisations"))
                tables_dir_final = execution_results.get('tables_dir', os.path.join(temp_dir, "tables"))
                f.write(f"R√©pertoire des visualisations: {vis_dir_final}\n")
                f.write(f"R√©pertoire des tables de r√©gression: {tables_dir_final}\n")
            temp_dir_final = execution_results.get('temp_dir', temp_dir)
            f.write(f"R√©pertoire temporaire d'ex√©cution: {temp_dir_final}\n")
        logger.info(f"Fichier d'index cr√©√©: {index_file_path}")
    except Exception as index_err:
        logger.error(f"Impossible de cr√©er le fichier d'index {index_file_path}: {index_err}")
        index_file_path = None

# Ajouter les chemins finaux aux r√©sultats retourn√©s
    execution_results["final_script_path"] = final_script_path
    execution_results["code_versions_dir"] = code_versions_dir
    execution_results["index_file"] = index_file_path
    execution_results["current_model"] = current_model  # Ajouter le mod√®le utilis√© pour les derni√®res interpr√©tations

    # Traiter les tables de r√©gression avec une interpr√©tation d√©di√©e
    regression_outputs = execution_results.get("regressions", [])
    if regression_outputs:
        logger.info(f"G√©n√©ration d'interpr√©tations d√©taill√©es pour {len(regression_outputs)} tables de r√©gression")
        
        for reg in regression_outputs:
            if 'data' in reg:
                # R√©cup√©rer le code de r√©gression
                regression_code = reg.get('regression_code', '')
                
                # Appel √† notre nouvelle fonction pour interpr√©ter la r√©gression
                detailed_interpretation = interpret_regression_with_llm(
                    reg['data'],
                    regression_code,
                    agent1_data,
                    current_model,  # Utiliser le mod√®le courant qui peut √™tre le mod√®le puissant
                    backend,
                    prompts_log_path,
                    timeout=180  # Plus de temps pour l'analyse d√©taill√©e
                )
                
                # Ajouter l'interpr√©tation d√©taill√©e au r√©sultat
                reg['detailed_interpretation'] = detailed_interpretation
                logger.info(f"Interpr√©tation d√©taill√©e g√©n√©r√©e pour la r√©gression {reg.get('id', 'inconnue')}")
            else:
                logger.warning(f"Pas de donn√©es structur√©es pour la r√©gression {reg.get('id', 'inconnue')}. Interpr√©tation d√©taill√©e impossible.")
    
    # Mettre √† jour les r√©sultats avec les r√©gressions interpr√©t√©es
    execution_results["regressions"] = regression_outputs

    return execution_results

# Nouvelle fonction √† ajouter dans agent2.py

def interpret_regression_with_llm(regression_data, code_executed, agent1_data, model, backend, prompts_log_path, timeout=120):
    """
    Interpr√®te de mani√®re d√©taill√©e les r√©sultats d'une r√©gression √©conom√©trique en utilisant un LLM.
    
    Args:
        regression_data: Donn√©es structur√©es de la r√©gression (coefficients, r-squared, etc.)
        code_executed: Code Python qui a g√©n√©r√© cette r√©gression
        agent1_data: Donn√©es de l'agent1 pour le contexte
        model: Mod√®le LLM √† utiliser
        backend: Backend pour les appels LLM
        prompts_log_path: Chemin pour sauvegarder les prompts
        timeout: Timeout en secondes. D√©faut: 120.
        
    Returns:
        str: Interpr√©tation d√©taill√©e des r√©sultats de r√©gression
    """
    from llm_utils import call_llm
    from datetime import datetime
    
    # Extraire les informations cl√©s de la r√©gression
    r_squared = regression_data.get('r_squared', 'N/A')
    coefficients = regression_data.get('coefficients', [])
    
    # Pr√©parer un r√©sum√© des coefficients pour le prompt
    coef_summary = ""
    for coef in coefficients:
        var_name = coef.get('variable', 'Inconnu')
        coef_value = coef.get('coef', 'N/A')
        p_value = coef.get('p_value', 'N/A')
        std_err = coef.get('std_err', 'N/A')
        coef_summary += f"{var_name}: coefficient={coef_value}, p-value={p_value}, std_err={std_err}\n"
    
    # Extraire le contexte de recherche de l'agent1
    user_prompt = agent1_data.get('user_prompt', 'Non disponible')
    introduction = agent1_data.get('llm_output', {}).get('introduction', 'Non disponible')
    hypotheses = agent1_data.get('llm_output', {}).get('hypotheses', 'Non disponible')
    
    # Extraire les noms des colonnes pour avoir une id√©e des variables disponibles
    column_names = agent1_data.get('metadata', {}).get('noms_colonnes', [])
    
    # Cr√©er le prompt pour l'interpr√©tation de la r√©gression
    prompt = f"""## INTERPR√âTATION √âCONOM√âTRIQUE D√âTAILL√âE

### R√©sultats de r√©gression
R-squared: {r_squared}

### Coefficients
{coef_summary}

### Code Python ayant g√©n√©r√© cette r√©gression
```python
{code_executed}
```

### Question de recherche
{user_prompt}

### Contexte acad√©mique
{introduction[:500]}...

### Hypoth√®ses de recherche
{hypotheses}

### Variables disponibles dans le dataset
{', '.join(column_names)}

---

En tant qu'√©conom√®tre expert, ton objectif est de fournir une interpr√©tation pr√©cise, rigoureuse et acad√©mique de ces r√©sultats de r√©gression.

Ton interpr√©tation doit inclure:

1. **Analyse du mod√®le global**
   - Qualit√© globale de l'ajustement (R¬≤)
   - Validit√© statistique du mod√®le
   - Ad√©quation du mod√®le √† la question de recherche

2. **Interpr√©tation des coefficients significatifs**
   - Analyse d√©taill√©e de chaque coefficient statistiquement significatif (p < 0.05)
   - Interpr√©tation pr√©cise de l'effet marginal (magnitude et direction)
   - Unit√©s de mesure et contexte √©conomique de chaque effet

3. **Implications √©conomiques**
   - M√©canismes √©conomiques sous-jacents expliquant les relations observ√©es
   - Liens avec les th√©ories √©conomiques pertinentes
   - Implications pour la question de recherche initiale

4. **Limites de l'estimation**
   - Probl√®mes potentiels d'endog√©n√©it√©, de variables omises ou de causalit√©
   - Robustesse des r√©sultats
   - Pistes d'am√©lioration du mod√®le

IMPORTANT:
- Base ton analyse uniquement sur les r√©sultats statistiques fournis, tout en les contextualisant avec la question de recherche
- Utilise un langage √©conom√©trique pr√©cis (√©lasticit√©s, effets marginaux, significativit√©, etc.)
- Proc√®de coefficient par coefficient pour les variables significatives
- Fais le lien entre les r√©sultats statistiques et les m√©canismes √©conomiques
- Ton analyse doit √™tre concise mais compl√®te, en 3-4 paragraphes

Il est essentiel que cette interpr√©tation soit suffisamment d√©taill√©e pour former le c≈ìur d'une analyse √©conom√©trique acad√©mique rigoureuse.
"""

    # Sauvegarder le prompt dans le fichier journal
    try:
        with open(prompts_log_path, "a", encoding="utf-8") as f:
            f.write("\n\n" + "="*80 + "\n")
            f.write(f"PROMPT POUR INTERPR√âTATION DE R√âGRESSION - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n")
            f.write(prompt)
            f.write("\n" + "="*80 + "\n")
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde du prompt: {e}")
    
    try:
        logger.info(f"Appel au LLM pour l'interpr√©tation d√©taill√©e de la r√©gression (R¬≤={r_squared})")
        interpretation = call_llm(
            prompt=prompt, 
            model_name=model, 
            backend=backend,
            timeout=timeout
        )
        
        logger.info("Interpr√©tation d√©taill√©e de la r√©gression g√©n√©r√©e avec succ√®s")
        return interpretation
    except Exception as e:
        logger.error(f"Erreur lors de l'interpr√©tation d√©taill√©e de la r√©gression: {e}")
        return f"Erreur lors de l'interpr√©tation d√©taill√©e de la r√©gression: {e}"

def extract_regression_code(full_code: str, table_content: str) -> str:
    """
    Tente d'identifier le bloc de code Python qui a g√©n√©r√© une table de r√©gression sp√©cifique.
    Recherche l'appel .summary() puis remonte vers le .fit() correspondant.

    Args:
        full_code: Le script Python complet qui a √©t√© ex√©cut√©.
        table_content: Le contenu textuel de la table de r√©gression OLS (la partie interne).

    Returns:
        str: Le bloc de code extrait pertinent, ou un message indiquant l'√©chec.
    """
    if not full_code or not table_content:
        logger.warning("Code complet ou contenu de table manquant pour l'extraction.")
        return "Code non fourni ou table vide."

    logger.debug("D√©but de l'extraction du code de r√©gression.")

    # 1. Identifier les variables cl√©s (D√©p., et quelques Ind√©p. non-FE)
    key_vars = set()
    dep_var_match = re.search(r"Dep\. Variable:\s*(\w+)", table_content, re.IGNORECASE)
    if dep_var_match:
        dep_var = dep_var_match.group(1)
        key_vars.add(dep_var)
        logger.debug(f"Variable d√©pendante identifi√©e: {dep_var}")

    # Extraire les noms des variables ind√©pendantes de la section coef
    coef_section_match = re.search(r"={2,}\s*\n\s*(coef.*?)(?:\n\s*={2,}|\Z)", table_content, re.DOTALL | re.IGNORECASE)
    if coef_section_match:
        coef_lines = coef_section_match.group(1).strip().split('\n')
        # Ignorer l'en-t√™te potentiel
        if coef_lines and ("coef" in coef_lines[0].lower()):
            coef_lines = coef_lines[1:]

        non_fe_vars_count = 0
        for line in coef_lines:
            stripped_line = line.strip()
            if not stripped_line or stripped_line.startswith('---'): continue
            # Extraire le premier "mot" comme nom de variable potentiel
            match_var = re.match(r'^(\S+)', stripped_line)
            if match_var:
                var_name = match_var.group(1)
                # Ignorer l'intercept et les effets fixes √©vidents
                if var_name.lower() != 'intercept' and not any(p.match(var_name) for p in FIXED_EFFECT_PATTERNS):
                    key_vars.add(var_name)
                    non_fe_vars_count += 1
                    # Limiter le nombre de variables cl√©s √† rechercher pour l'efficacit√©
                    if non_fe_vars_count >= 5:
                        break
        logger.debug(f"Variables cl√©s (non-FE) identifi√©es pour la recherche: {key_vars}")

    if not key_vars:
        logger.warning("Aucune variable cl√© (d√©pendante ou ind√©pendante non-FE) n'a pu √™tre extraite de la table.")
        # On peut quand m√™me continuer sans validation par variable cl√© si on le souhaite

    # 2. Localiser les appels .summary() dans le code
    # Chercher `print(<quelque chose>.summary())` ou juste `<quelque chose>.summary()`
    summary_pattern = re.compile(r"(?:print\s*\(.*?\b(\w+)\.summary\s*\(\s*\)\s*\)|(\w+)\.summary\s*\(\s*\))")
    summary_matches = list(summary_pattern.finditer(full_code))
    logger.debug(f"Nombre d'appels .summary() potentiels trouv√©s: {len(summary_matches)}")

    if not summary_matches:
        logger.warning("Aucun appel .summary() trouv√© dans le code.")
        # Passer aux fallbacks si aucun summary n'est trouv√©
    else:
        # It√©rer sur les summary trouv√©s (du dernier au premier)
        for summary_match in reversed(summary_matches):
            summary_line_end = summary_match.end()
            # Trouver le nom de la variable contenant les r√©sultats (ex: 'results' dans results.summary())
            results_var_name = summary_match.group(1) or summary_match.group(2)
            if not results_var_name: continue # Ne devrait pas arriver avec le pattern mais s√©curit√©

            logger.debug(f"Recherche en amont de .summary() √† la position {summary_line_end} pour la variable '{results_var_name}'")

            # 3. Remonter depuis .summary() pour trouver l'assignation et le .fit()
            # Chercher une ligne comme `results_var_name = ... .fit()` avant le summary
            fit_pattern_str = rf"^\s*({re.escape(results_var_name)}\s*=\s*.*?\.fit\s*\(.*?\))\s*$"
            fit_pattern = re.compile(fit_pattern_str, re.MULTILINE | re.DOTALL)

            # Chercher dans la partie du code *avant* le summary
            code_before_summary = full_code[:summary_line_end]
            fit_matches = list(fit_pattern.finditer(code_before_summary))

            if not fit_matches:
                 logger.debug(f"Aucun appel .fit() assign√© √† '{results_var_name}' trouv√© avant le summary.")
                 continue # Essayer le summary pr√©c√©dent

            # Prendre le dernier appel .fit() trouv√© avant ce summary
            fit_match = fit_matches[-1]
            fit_line_start = fit_match.start()
            fit_line_end = fit_match.end()
            logger.debug(f"Appel .fit() correspondant trouv√© entre {fit_line_start} et {fit_line_end}")

            # 4. D√©terminer le bloc de code pertinent
            # Remonter de N lignes avant le d√©but de la ligne .fit()
            lines_before_fit = 15 # Nombre de lignes √† inclure avant le fit
            code_lines = full_code.splitlines()
            # Trouver l'indice de la ligne o√π commence le .fit()
            current_pos = 0
            fit_start_line_index = -1
            for i, line in enumerate(code_lines):
                if current_pos >= fit_line_start:
                    fit_start_line_index = i
                    break
                current_pos += len(line) + 1 # +1 pour le \n

            if fit_start_line_index == -1:
                logger.warning("Impossible de d√©terminer l'indice de ligne pour le d√©but du .fit()")
                continue

            # Trouver l'indice de la ligne o√π finit le .summary()
            current_pos = 0
            summary_end_line_index = -1
            for i, line in enumerate(code_lines):
                 current_pos += len(line) + 1
                 if current_pos >= summary_line_end:
                     summary_end_line_index = i
                     break

            if summary_end_line_index == -1: summary_end_line_index = len(code_lines) -1 # Prende la fin si non trouv√©

            # D√©finir les bornes du bloc
            block_start_line = max(0, fit_start_line_index - lines_before_fit)
            block_end_line = summary_end_line_index + 1 # Inclure la ligne du summary

            extracted_block = "\n".join(code_lines[block_start_line:block_end_line])

            # 5. Valider le bloc (optionnel mais recommand√©)
            # V√©rifier si le bloc contient les variables cl√©s extraites de la table
            block_contains_key_vars = True # Commencer optimiste
            if key_vars: # Ne valider que si on a des variables cl√©s
                found_count = 0
                for var in key_vars:
                    # Utiliser word boundary \b pour √©viter les correspondances partielles
                    if re.search(r'\b' + re.escape(var) + r'\b', extracted_block):
                        found_count +=1
                # Exiger au moins la variable d√©pendante ou une des ind√©pendantes cl√©s
                if found_count == 0:
                     block_contains_key_vars = False
                     logger.debug(f"Bloc extrait ne semble pas contenir les variables cl√©s {key_vars}. Essai du summary pr√©c√©dent.")

            if block_contains_key_vars:
                logger.info(f"Bloc de code pertinent trouv√© et valid√© (lignes {block_start_line+1} √† {block_end_line}).")
                return extracted_block.strip()
            else:
                 continue # Passer au .summary() pr√©c√©dent

    # --- Fallbacks si la m√©thode principale √©choue ---
    logger.warning("M√©thode principale (summary -> fit) a √©chou√©. Tentative des fallbacks.")

    # Fallback 1: Chercher n'importe quel pattern de r√©gression (moins pr√©cis)
    regression_patterns_fb = [
        re.compile(r"(\S+\s*=\s*.*?sm\.OLS\(.*?\)\.fit\(.*?\))", re.DOTALL),
        re.compile(r"(\S+\s*=\s*.*?statsmodels\.api\.OLS\(.*?\)\.fit\(.*?\))", re.DOTALL),
        re.compile(r"(\S+\s*=\s*.*?LinearRegression\(.*?\)\.fit\(.*?\))", re.DOTALL)
    ]
    for pattern in regression_patterns_fb:
        matches = list(pattern.finditer(full_code))
        if matches:
            last_match_code = matches[-1].group(1) # Prendre le dernier trouv√©
            logger.info("Fallback 1: Trouv√© un pattern de r√©gression g√©n√©rique.")
            # Essayer de prendre quelques lignes avant aussi
            match_start_pos = matches[-1].start(1)
            lines = full_code.splitlines()
            start_line_idx = -1
            current_pos = 0
            for i, line in enumerate(lines):
                 if current_pos >= match_start_pos:
                     start_line_idx = i
                     break
                 current_pos += len(line) + 1

            if start_line_idx != -1:
                 block_start = max(0, start_line_idx - 5) # 5 lignes avant
                 # Trouver la ligne de fin du match
                 match_end_pos = matches[-1].end(1)
                 end_line_idx = start_line_idx
                 current_pos = match_start_pos
                 for i in range(start_line_idx, len(lines)):
                      current_pos += len(lines[i]) + 1
                      if current_pos >= match_end_pos:
                           end_line_idx = i
                           break
                 block_end = end_line_idx + 1
                 return "\n".join(lines[block_start:block_end]).strip()
            else:
                 return last_match_code.strip() # Retourner juste la ligne si indices non trouv√©s

    # Fallback 2: Chercher des mots-cl√©s
    logger.warning("Fallback 1 a √©chou√©. Recherche de mots-cl√©s.")
    regression_keywords = ["OLS", "LinearRegression", ".fit("] # Mots cl√©s plus sp√©cifiques
    lines = full_code.splitlines()
    keyword_matches = []
    for i, line in enumerate(lines):
        for keyword in regression_keywords:
            if keyword in line:
                keyword_matches.append(i)
                break # Une seule correspondance par ligne suffit

    if keyword_matches:
        last_match_line_index = keyword_matches[-1] # Prendre la derni√®re ligne contenant un mot cl√©
        start = max(0, last_match_line_index - 7) # 7 lignes avant
        end = min(len(lines), last_match_line_index + 4) # 3 lignes apr√®s
        logger.info(f"Fallback 2: Trouv√© mot-cl√© '{keyword}' √† la ligne {last_match_line_index+1}. Retourne lignes {start+1}-{end}.")
        return "\n".join(lines[start:end]).strip()

    logger.error("√âchec de l'extraction du code de r√©gression par toutes les m√©thodes.")
    return "Code snippet not reliably extracted."

# --- Fin de la fonction extract_regression_code ---

def generate_analysis_code(csv_file, user_prompt, agent1_data, model, prompts_log_path, backend: str):
    """
    G√©n√®re le code d'analyse √©conom√©trique de niveau accessible
    
    Args:
        csv_file: Chemin absolu du fichier CSV
        user_prompt: Prompt initial de l'utilisateur
        agent1_data: Donn√©es de l'agent1
        model: Mod√®le LLM √† utiliser
        prompts_log_path: Chemin pour sauvegarder les prompts
        backend: Backend pour les appels LLM
        
    Returns:
        str: Code d'analyse g√©n√©r√©
    """
    # Pr√©paration du prompt pour le LLM avec le chemin absolu du fichier et les noms de colonnes exacts
    metadata_str = json.dumps(agent1_data["metadata"], indent=2, ensure_ascii=False)
    colonnes = agent1_data["metadata"].get("noms_colonnes", [])
    col_dict = ",\n".join([f'    "{col}": "{col}"' for col in colonnes])
    
    # Extraire les sections si disponibles
    introduction = agent1_data['llm_output'].get('introduction', 'Non disponible')
    literature_review = agent1_data['llm_output'].get('literature_review', 'Non disponible')
    hypotheses = agent1_data['llm_output'].get('hypotheses', 'Non disponible')
    methodology = agent1_data['llm_output'].get('methodology', 'Non disponible')
    limitations = agent1_data['llm_output'].get('limitations', 'Non disponible')
    variables_info = agent1_data['llm_output'].get('variables', 'Non disponible')
    
# Utiliser les anciennes sections si les nouvelles ne sont pas disponibles
    if introduction == 'Non disponible':
        introduction = agent1_data['llm_output'].get('problematisation', 'Non disponible')
    
    if methodology == 'Non disponible':
        methodology = agent1_data['llm_output'].get('approches_suggerees', 'Non disponible')
    
    if limitations == 'Non disponible':
        limitations = agent1_data['llm_output'].get('points_vigilance', 'Non disponible')
    
    # Cr√©er le prompt pour la g√©n√©ration de code
    prompt = f"""## G√âN√âRATION DE CODE D'ANALYSE DE DONN√âES

### Fichier CSV et M√©tadonn√©es
```json
{metadata_str}
```

### Chemin absolu du fichier CSV
{csv_file}

### Noms exacts des colonnes √† utiliser
{colonnes}

### Introduction et probl√©matique de recherche
{introduction}

### Hypoth√®ses de recherche
{hypotheses}

### M√©thodologie propos√©e
{methodology}

### Limites identifi√©es
{limitations}

### Informations sur les variables
{variables_info}

### Demande initiale de l'utilisateur
{user_prompt}

---

Tu es un analyste de donn√©es exp√©riment√©. Ta mission est de g√©n√©rer un script Python d'analyse de donn√©es clair et accessible. Le code doit √™tre robuste et produire des visualisations attrayantes.

DIRECTIVES:

1. CHARGEMENT ET PR√âTRAITEMENT DES DONN√âES
   - Utilise strictement le chemin absolu '{csv_file}'
   - Nettoie les donn√©es (valeurs manquantes, outliers)
   - Cr√©e des statistiques descriptives claires

2. VISUALISATIONS ATTRAYANTES ET INFORMATIVES
   - Cr√©e au moins 4-5 visualisations avec matplotlib/seaborn:
     * Matrice de corr√©lation color√©e et lisible
     * Distributions des variables principales
     * Relations entre variables importantes
     * Graphiques adapt√©s au type de donn√©es
     *Si DiD, graphique de tendance temporelle avec deux groupe avant et apr√®s traitement
   - Utilise des couleurs attrayantes et des styles modernes
   - Ajoute des titres clairs, des l√©gendes informatives et des √âTIQUETTES D'AXES EXPLICITES
   - IMPORTANT: Assure-toi d'utiliser ax.set_xlabel() et ax.set_ylabel() avec des descriptions claires
   - IMPORTANT: Assure-toi que les graphiques soient sauvegard√©s ET affich√©s
   - Utilise plt.savefig() AVANT plt.show() pour chaque graphique

3. MOD√âLISATION SIMPLE ET CLAIRE
   - Impl√©mente les mod√®les de r√©gression appropri√©s
   - Utilise statsmodels avec des r√©sultats complets
   - Pr√©sente les r√©sultats de mani√®re lisible
   - Documente clairement chaque √©tape

4. TESTS DE BASE
   - V√©rifie la qualit√© du mod√®le avec des tests simples
   - Analyse les r√©sidus
   - V√©rifie la multicolin√©arit√© si pertinent

5. CAPTURE ET STOCKAGE DES DONN√âES POUR INTERPR√âTATION
   - IMPORTANT: Pour chaque visualisation, stocke le DataFrame utilis√© dans une variable
   - IMPORTANT: Apr√®s chaque cr√©ation de figure, stocke les donn√©es utilis√©es pour permettre une interpr√©tation pr√©cise
   - Assure-toi que chaque figure peut √™tre associ√©e aux donn√©es qui ont servi √† la g√©n√©rer

EXIGENCES TECHNIQUES:
- Utilise pandas, numpy, matplotlib, seaborn, et statsmodels
- Organise ton code en sections clairement comment√©es
- Utilise ce dictionnaire pour acc√©der aux colonnes:
```python
col = {{
{col_dict}
}}
```
- Document chaque √©tape de fa√ßon simple et accessible
- Pour chaque visualisation:
  * UTILISE des titres clairs pour les graphiques et les axes
  * SAUVEGARDE avec plt.savefig() PUIS
  * AFFICHE avec plt.show()
- Pour les tableaux de r√©gression, utilise print(results.summary())

IMPORTANT:
- Adapte l'analyse aux donn√©es disponibles
- Mets l'accent sur les visualisations attrayantes et bien √©tiquet√©es
- Assure-toi que chaque graphique a des √©tiquettes d'axe claires via ax.set_xlabel() et ax.set_ylabel()
- Assure-toi que chaque graphique est √† la fois SAUVEGARD√â et AFFICH√â
- Utilise plt.savefig() AVANT plt.show() pour chaque graphique
- IMPORTANT: Pour les styles Seaborn, utilise 'whitegrid' au lieu de 'seaborn-whitegrid' ou 'seaborn-v0_8-whitegrid' qui sont obsol√®tes 
"""

    # Sauvegarder le prompt dans le fichier journal
    save_prompt_to_file(prompt, prompts_log_path, "Initial Code Generation")
    logger.info("Appel LLM pour g√©n√©ration du code d'analyse")
    try:
        # Utilisation de llm_utils.call_llm
        generated_output = call_llm(prompt=prompt, model_name=model, backend=backend)
        
        # Extraire et nettoyer le code g√©n√©r√©
        generated_code = extract_code(generated_output)
        clean_code = remove_shell_commands(generated_code)
        
        # S'assurer que print(results.summary()) est pr√©sent
        if "results.summary()" in clean_code and "print(results.summary())" not in clean_code:
            clean_code = clean_code.replace("results.summary()", "print(results.summary())")
        
        # Assurer que les styles Seaborn sont compatibles
        clean_code = clean_code.replace("seaborn-v0_8-whitegrid", "whitegrid")
        clean_code = clean_code.replace("seaborn-whitegrid", "whitegrid")
        clean_code = clean_code.replace("seaborn-v0_8-white", "white")
        clean_code = clean_code.replace("seaborn-white", "white")
        clean_code = clean_code.replace("seaborn-v0_8-darkgrid", "darkgrid")
        clean_code = clean_code.replace("seaborn-darkgrid", "darkgrid")
        
        return clean_code
        
    except Exception as e:
        logger.error(f"Erreur lors de l'appel LLM pour g√©n√©ration: {e}")
        sys.exit(1)

def generate_analysis_narrative(execution_results, agent1_data, model):
    """
    Fonction de placeholder pour la g√©n√©ration de narration explicative.
    Cette fonction peut √™tre impl√©ment√©e pour g√©n√©rer une narration bas√©e sur les r√©sultats.
    
    Args:
        execution_results: R√©sultats de l'ex√©cution du code
        agent1_data: Donn√©es de l'agent1
        model: Mod√®le LLM √† utiliser
        
    Returns:
        Dictionnaire avec une narration par d√©faut
    """
    # Pour l'instant, retourne juste un dictionnaire avec une narration par d√©faut
    return {
        "narrative": "Analyse ex√©cut√©e avec succ√®s. Veuillez consulter les visualisations et les r√©sultats."
    }

def main():
    """
    Fonction principale qui ex√©cute le pipeline d'analyse √©conom√©trique.
    """
    parser = argparse.ArgumentParser(
        description="Agent 2: Analyse √©conom√©trique"
    )
    parser.add_argument("csv_file", help="Chemin vers le fichier CSV")
    parser.add_argument("user_prompt", help="Prompt initial de l'utilisateur")
    parser.add_argument("agent1_output", help="Chemin vers le fichier de sortie de l'agent 1")
    parser.add_argument("--model", default="gemini-2.0-flash", help="Mod√®le LLM √† utiliser")
    parser.add_argument("--backend", default="gemini", choices=["ollama", "gemini"], help="Backend LLM √† utiliser: 'ollama' ou 'gemini'")
    parser.add_argument("--auto-confirm", action="store_true", help="Ignore la pause manuelle pour correction")
    parser.add_argument("--log-file", help="Fichier de log sp√©cifique")  # Nouvel argument
    args = parser.parse_args()

    # Reconfigurer le logging si un fichier de log sp√©cifique est fourni
    if args.log_file:
        # Supprimer les handlers existants
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        # Ajouter les nouveaux handlers
        file_handler = logging.FileHandler(args.log_file, mode='a')  # mode 'a' pour append
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        logger.addHandler(file_handler)
        
        stream_handler = logging.StreamHandler(sys.stderr)
        stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        logger.addHandler(stream_handler)
        
        logger.info(f"Logging redirig√© vers {args.log_file}")

    # Cr√©er les r√©pertoires de sortie et les noms de fichiers
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    os.makedirs("outputs", exist_ok=True)
    code_versions_dir = os.path.join("outputs", f"code_versions_{timestamp}")
    prompts_log_path = os.path.join(code_versions_dir, "prompts.txt")

    # Charger les donn√©es de l'agent 1
    try:
        with open(args.agent1_output, "r", encoding="utf-8") as f:
            agent1_data = json.load(f)
    except FileNotFoundError:
        logger.error(f"Erreur: Le fichier de l'agent 1 '{args.agent1_output}' n'a pas √©t√© trouv√©.")
        sys.exit(1)
    except json.JSONDecodeError:
        logger.error(f"Erreur: Impossible de d√©coder le JSON du fichier de l'agent 1 '{args.agent1_output}'.")
        sys.exit(1)

    # G√©n√©rer le code d'analyse √©conom√©trique
    logger.info("G√©n√©ration du code d'analyse √©conom√©trique")
    analysis_code = generate_analysis_code(
        args.csv_file,
        args.user_prompt,
        agent1_data,
        args.model,
        prompts_log_path,
        args.backend
    )
    if not analysis_code:
         logger.error("La g√©n√©ration du code initial a √©chou√©. Arr√™t.")
         sys.exit(1)

    # Ex√©cuter le code d'analyse
    logger.info("Ex√©cution du code d'analyse")
    execution_results = attempt_execution_loop(
        analysis_code,
        args.csv_file,
        agent1_data,
        args.model,
        prompts_log_path,
        args.backend
    )

    # G√©n√©rer la narration explicative des r√©sultats
    narrative = {"narrative": "L'analyse n'a pas √©t√© ex√©cut√©e avec succ√®s ou a √©chou√©."}
    if execution_results.get("success"):
        logger.info("G√©n√©ration de la narration explicative des r√©sultats")
        narrative = generate_analysis_narrative(
            execution_results,
            agent1_data,
            execution_results.get("current_model", args.model)  # Utiliser le mod√®le courant (qui peut √™tre le puissant)
        )
    else:
        error_msg = execution_results.get('error', 'Erreur inconnue lors de l\'ex√©cution')
        stderr_msg = execution_results.get('stderr')
        full_error = f"L'analyse a √©chou√©: {error_msg}"
        if stderr_msg:
            full_error += f"\nDernier Stderr:\n{stderr_msg[:500]}..."
        narrative = {
            "narrative": full_error
        }
        logger.warning(f"L'ex√©cution du code a √©chou√©. Raison: {error_msg}")

    # Combiner toutes les visualisations et tables de r√©gression en une seule liste
    all_visuals = execution_results.get("all_visuals", [])
    if not all_visuals:
        all_visuals = execution_results.get("visualisations", []) + execution_results.get("regressions", [])

    # Pr√©parer la sortie finale
    output = {
        "initial_generated_code": analysis_code,
        "execution_results": execution_results,
        "narrative": narrative.get("narrative"),
        "final_script_path": execution_results.get("final_script_path", "Non disponible"),
        "visualisations": all_visuals,  # Liste combin√©e
        "prompts_log_file": prompts_log_path if os.path.exists(prompts_log_path) else "Non g√©n√©r√© ou erreur",
        "output_directory": code_versions_dir,
        "index_file": execution_results.get("index_file", "Non g√©n√©r√© ou erreur"),
        "current_model": execution_results.get("current_model", args.model)  # Ajouter le mod√®le utilis√© pour les derni√®res actions
    }

    # Afficher la sortie au format JSON
    try:
        print(json.dumps(output, ensure_ascii=False, indent=2))
    except TypeError as e:
         logger.error(f"Erreur lors de la s√©rialisation en JSON: {e}")
         simplified_output = {k: str(v) for k, v in output.items()}
         print(json.dumps(simplified_output, ensure_ascii=False, indent=2))

    return 0 if execution_results.get("success") else 1

if __name__ == "__main__":
    sys.exit(main())