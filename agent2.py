#!/usr/bin/env python3
"""
Agent 2: Analyse √âconom√©trique

Ce script g√©n√®re et ex√©cute du code d'analyse √©conom√©trique √† partir d'un fichier CSV
et des suggestions fournies par l'agent 1. Il g√®re la correction automatique des erreurs,
capture les visualisations et interpr√®te les r√©sultats.

Usage:
    python agent2.py chemin_csv prompt_utilisateur chemin_sortie_agent1 [--model modele] [--backend backend] [--auto-confirm]

Arguments:
    chemin_csv: Chemin vers le fichier CSV √† analyser
    prompt_utilisateur: Prompt initial de l'utilisateur
    chemin_sortie_agent1: Chemin vers le fichier de sortie de l'agent 1
    --model: Mod√®le LLM √† utiliser (d√©faut: gemma3:27b)
    --backend: Backend LLM ('ollama' ou 'gemini', d√©faut: 'ollama')
    --auto-confirm: Ignorer la pause manuelle pour correction
"""

import argparse
import json
import logging
import os
import re
import sys
import subprocess
import tempfile
from datetime import datetime
import base64
import matplotlib.pyplot as plt
from io import StringIO
import pandas as pd
import threading # <--- AJOUTER CECI
import time      # <--- AJOUTER CECI

# Importation du module llm_utils
from llm_utils import call_llm

# ======================================================
# Configuration du logging
# ======================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("agent2.log"),
        logging.StreamHandler(sys.stderr)
    ]
)
logger = logging.getLogger("agent2")

def save_prompt_to_file(prompt_content: str, log_file_path: str, prompt_type: str):
    """
    Ajoute un prompt format√© au fichier journal sp√©cifi√©.
    Cr√©e le r√©pertoire si n√©cessaire.
    
    Args:
        prompt_content: Contenu du prompt √† sauvegarder
        log_file_path: Chemin du fichier journal
        prompt_type: Type de prompt (pour identification)
    """
    try:
        log_dir = os.path.dirname(log_file_path)
        os.makedirs(log_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        separator = "=" * 80

        with open(log_file_path, "a", encoding="utf-8") as f:
            f.write(f"{separator}\n")
            f.write(f"Timestamp: {timestamp}\n")
            f.write(f"Prompt Type: {prompt_type}\n")
            f.write(f"{separator}\n\n")
            f.write(prompt_content)
            f.write(f"\n\n{separator}\n\n")

        logger.info(f"Prompt '{prompt_type}' ajout√© √† {log_file_path}")

    except Exception as e:
        logger.error(f"Impossible d'√©crire le prompt '{prompt_type}' dans {log_file_path}: {e}")

def interpret_single_visualization_thread(vis, agent1_data, model, backend, prompts_log_path, results_list):
    """
    Fonction ex√©cut√©e dans un thread pour interpr√©ter une seule visualisation.
    Ajoute le r√©sultat (ou une erreur) √† la liste partag√©e 'results_list'.
    """
    # Utiliser le filename ou l'id comme identifiant unique
    if 'filename' in vis:
        vis_id = os.path.splitext(vis.get("filename", "unknown.png"))[0]
    else:
        vis_id = vis.get("id", f"unknown_{time.time()}") # Fallback ID unique

    try:
        logger.info(f"Thread d√©marr√© pour l'interpr√©tation de: {vis_id}")
        interpretation = interpret_visualization_with_gemini(
            vis,
            agent1_data,
            model,
            backend,
            prompts_log_path
        )
        # Ajouter le r√©sultat avec l'identifiant
        results_list.append({'id': vis_id, 'interpretation': interpretation})
        logger.info(f"Interpr√©tation re√ßue pour {vis_id} dans le thread.")
    except Exception as e:
        logger.error(f"Erreur dans le thread pour {vis_id}: {e}")
        # Ajouter l'erreur avec l'identifiant
        results_list.append({'id': vis_id, 'interpretation': f"Erreur d'interpr√©tation (thread): {e}"})
# --- FIN NOUVELLE FONCTION ---

def extract_code(llm_output: str) -> str:
    """
    Extrait uniquement le code Python des blocs d√©limit√©s par triple backticks.
    
    Args:
        llm_output: Texte complet de la sortie du LLM
        
    Returns:
        str: Code Python extrait ou texte complet si aucun bloc n'est trouv√©
    """
    code_blocks = re.findall(r"```(?:python)?\s*(.*?)```", llm_output, re.DOTALL)
    if code_blocks:
        return "\n\n".join(code_blocks)
    else:
        return llm_output

def remove_shell_commands(code: str) -> str:
    """
    Filtre le code en supprimant les lignes ressemblant √† des commandes shell.
    
    Args:
        code: Code Python potentiellement contenant des commandes shell
        
    Returns:
        str: Code nettoy√©
    """
    filtered_lines = []
    for line in code.splitlines():
        stripped = line.strip()
        if (stripped.startswith("pip install") or 
            stripped.startswith("bash") or 
            stripped.startswith("$") or 
            (stripped.startswith("python") and not stripped.startswith("python3"))):
            continue
        filtered_lines.append(line)
    return "\n".join(filtered_lines)

def sanitize_code(code: str, csv_path: str, valid_columns: list[str]) -> str:
    """
    Force le bon chemin CSV, corrige les noms de colonnes, corrige les probl√®mes d'indentation,
    et ins√®re un bloc pour √©viter les erreurs de corr√©lation.
    
    Args:
        code: Code Python √† nettoyer
        csv_path: Chemin absolu du fichier CSV
        valid_columns: Liste des noms de colonnes valides
        
    Returns:
        str: Code Python nettoy√©
    """
    import ast

    # Forcer le bon chemin CSV
    code = re.sub(
        r"pd\.read_csv\((.*?)\)",
        f"pd.read_csv('{csv_path}')",
        code
    )

    # Identifier les colonnes utilis√©es
    used_columns = set(re.findall(r"df\[['\"](.*?)['\"]\]", code)) | \
                   set(re.findall(r"df\.([a-zA-Z_][a-zA-Z0-9_]*)", code))

    # Mapper les noms de colonnes utilis√©s aux noms r√©els
    col_map = {}
    for used in used_columns:
        for real in valid_columns:
            if used.lower() == real.lower():
                col_map[used] = real
                break

    # Corriger les noms de colonnes
    for used, correct in col_map.items():
        if used != correct:
            code = re.sub(rf"df\[['\"]{used}['\"]\]", f"df['{correct}']", code)
            code = re.sub(rf"df\.{used}\b", f"df['{correct}']", code)

    # Ajouter un bloc pour √©viter les erreurs de corr√©lation
    if "df.corr()" in code or re.search(r"df\.corr\s*\(", code):
        init_numeric_block = "\n# üîç S√©lection des colonnes num√©riques pour √©viter les erreurs sur df.corr()\ndf_numeric = df.select_dtypes(include='number')\n"
        match = re.search(r"(df\s*=\s*pd\.read_csv\(.*?\))", code)
        if match:
            insertion_point = match.end()
            code = code[:insertion_point] + init_numeric_block + code[insertion_point:]
        else:
            code = init_numeric_block + code

        code = re.sub(r"df\.corr(\s*\(.*?\))", r"df_numeric.corr\1", code)
    
    # NOUVEAU: Corriger les probl√®mes d'indentation avec plt.show() dans les blocs try/except/else
    # Cette solution utilise une approche ligne par ligne pour une correction pr√©cise
    lines = code.split('\n')
    i = 0
    while i < len(lines) - 1:
        current_line = lines[i].rstrip()
        next_line = lines[i + 1].rstrip()
        
        # Chercher plt.show() suivi par except, else ou elif √† une indentation diff√©rente
        if current_line.strip() == 'plt.show()' and next_line.lstrip().startswith(('except', 'else', 'elif')):
            # Calculer l'indentation du bloc suivant
            next_indent = len(next_line) - len(next_line.lstrip())
            if next_indent > 0:
                # R√©indenter plt.show() pour correspondre au bloc
                lines[i] = ' ' * next_indent + 'plt.show()'
        i += 1
    
    # Reconstruire le code avec les lignes corrig√©es
    code = '\n'.join(lines)
    
    # MODIFICATION: Ne pas d√©sactiver plt.show() compl√®tement, mais le remplacer par une sauvegarde suivie d'un show()
    code = re.sub(r"plt\.show\(\)", "plt.savefig('temp_figure.png', dpi=100, bbox_inches='tight')\nplt.show()", code)
    
    # Remplacer les styles Seaborn obsol√®tes
    code = code.replace("seaborn-v0_8-whitegrid", "whitegrid")
    code = code.replace("seaborn-whitegrid", "whitegrid")
    code = code.replace("seaborn-v0_8-white", "white")
    code = code.replace("seaborn-white", "white")
    code = code.replace("seaborn-v0_8-darkgrid", "darkgrid")
    code = code.replace("seaborn-darkgrid", "darkgrid")
    code = code.replace("seaborn-v0_8-dark", "dark")
    code = code.replace("seaborn-dark", "dark")
    code = code.replace("seaborn-v0_8-paper", "ticks")
    code = code.replace("seaborn-paper", "ticks")
    code = code.replace("seaborn-v0_8-talk", "ticks")
    code = code.replace("seaborn-talk", "ticks")
    code = code.replace("seaborn-v0_8-poster", "ticks")
    code = code.replace("seaborn-poster", "ticks")
    
    return code

def capture_regression_outputs(output_text, output_dir, executed_code=""):
    """
    Capture les tables de r√©gression OLS du texte de sortie et les enregistre
    √† la fois comme texte et comme images. Tente √©galement d'identifier le code
    qui a g√©n√©r√© chaque r√©gression.
    
    Args:
        output_text: Texte de sortie contenant potentiellement des r√©sultats de r√©gression
        output_dir: R√©pertoire o√π sauvegarder les r√©sultats
        executed_code: Code Python complet qui a √©t√© ex√©cut√©
        
    Returns:
        Liste des chemins des fichiers g√©n√©r√©s et des m√©tadonn√©es associ√©es
    """
    regression_outputs = []
    regression_pattern = r"={10,}\s*\n\s*OLS Regression Results\s*\n={10,}(.*?)(?:\n={10,}|\Z)"
    
    # Cr√©er le r√©pertoire de sortie s'il n'existe pas
    os.makedirs(output_dir, exist_ok=True)
    
    # Chercher toutes les tables de r√©gression dans la sortie
    regression_tables = re.findall(regression_pattern, output_text, re.DOTALL)
    
    for i, table_content in enumerate(regression_tables):
        table_id = f"regression_{i+1}"
        
        # Sauvegarder le contenu textuel
        text_file_path = os.path.join(output_dir, f"{table_id}.txt")
        with open(text_file_path, "w", encoding="utf-8") as f:
            f.write("="*80 + "\n")
            f.write("OLS Regression Results\n")
            f.write("="*80 + "\n")
            f.write(table_content)
            f.write("\n" + "="*80 + "\n")
        
        # Cr√©er une visualisation de la table et la sauvegarder comme image
        img_file_path = os.path.join(output_dir, f"{table_id}.png")
        
        # Cr√©er une visualisation de la table avec matplotlib
        plt.figure(figsize=(12, 10))
        plt.text(0.01, 0.99, "OLS Regression Results\n" + table_content, 
                 family='monospace', fontsize=10, 
                 verticalalignment='top')
        plt.axis('off')
        plt.tight_layout()
        plt.savefig(img_file_path, dpi=100, bbox_inches='tight')
        plt.close()
        
        # Encoder l'image en base64 pour l'inclure dans la sortie JSON
        with open(img_file_path, 'rb') as img_file:
            img_data = base64.b64encode(img_file.read()).decode('utf-8')
        
        # Extraire quelques m√©tadonn√©es de base (R-squared, variables cl√©s)
        r_squared_match = re.search(r"R-squared:\s*([\d\.]+)", table_content)
        r_squared = r_squared_match.group(1) if r_squared_match else "N/A"
        
        key_variables = re.findall(r"([A-Za-z0-9_]+)\s+[\d\.-]+\s+[\d\.-]+\s+[\d\.-]+\s+[\d\.-]+", table_content)
        
        # Extraire quelques statistiques cl√©s pour les donn√©es de r√©gression
        regression_data = {
            "r_squared": r_squared,
            "variables": key_variables[:5],  # Limiter aux 5 premi√®res variables
            "statistics": {}
        }

        coef_section = re.search(r"==+\s*\n\s*(coef.*?)(?:\n\s*==+|\Z)", output_text, re.DOTALL)
        if coef_section:
            lines = coef_section.group(1).split('\n')
            headers = None
            coef_data = []
            
            for line in lines:
                if "coef" in line.lower():
                    # C'est la ligne d'en-t√™te
                    headers = re.split(r'\s{2,}', line.strip())
                elif line.strip() and headers:
                    # C'est une ligne de donn√©es
                    values = re.split(r'\s{2,}', line.strip())
                    if len(values) >= len(headers):
                        variable = values[0]
                        coef_data.append({
                            "variable": variable,
                            "coef": values[1] if len(values) > 1 else "N/A",
                            "std_err": values[2] if len(values) > 2 else "N/A",
                            "p_value": values[4] if len(values) > 4 else "N/A"
                        })
            
            regression_data["coefficients"] = coef_data
            
            # NOUVELLE SECTION: Convertir les donn√©es de r√©gression en CSV am√©lior√© pour faciliter l'interpr√©tation
            try:
                import pandas as pd
                if coef_data:
                    coef_df = pd.DataFrame(coef_data)
                    
                    # Ajouter une colonne de significativit√© pour faciliter l'interpr√©tation
                    try:
                        coef_df['significatif'] = coef_df['p_value'].astype(float) < 0.05
                    except:
                        # Si conversion en float √©choue, on continue sans cette colonne
                        pass
                    
                    # Ajouter des m√©tadonn√©es contextuelles en commentaire
                    csv_header = f"# R√©sultats de r√©gression - R¬≤: {r_squared}\n"
                    csv_header += f"# Interpr√©tation: un coefficient positif indique une relation positive, une p-value < 0.05 indique une significativit√© statistique\n"
                    csv_header += f"# Variables explicatives: {', '.join(key_variables[:5]) if key_variables else 'non disponibles'}\n"
                    
                    regression_data["csv_data"] = csv_header + coef_df.to_csv(index=False)
                    
                    # Sauvegarder en fichier CSV
                    csv_path = os.path.join(output_dir, f"{table_id}_coefficients.csv")
                    with open(csv_path, 'w', encoding='utf-8') as f:
                        f.write(regression_data["csv_data"])
                    logger.info(f"Donn√©es CSV am√©lior√©es de coefficients sauvegard√©es: {csv_path}")
            except Exception as e:
                logger.error(f"Erreur lors de la conversion des coefficients en CSV am√©lior√©: {e}")
        
        # Extraire le code de r√©gression si le code complet est fourni
        regression_code = ""
        if executed_code:
            regression_code = extract_regression_code(executed_code, table_content)
            regression_data["regression_code"] = regression_code
        
        regression_outputs.append({
            'type': 'regression_table',
            'id': table_id,
            'text_path': text_file_path,
            'image_path': img_file_path,
            'base64': img_data,
            'metadata': {
                'r_squared': r_squared,
                'variables': key_variables[:5]  # Limiter aux 5 premi√®res variables
            },
            'data': regression_data,  # Ajout des donn√©es structur√©es
            'csv_data': regression_data.get("csv_data", ""),  # Ajout des donn√©es CSV am√©lior√©es
            'regression_code': regression_code  # Ajout du code de r√©gression
        })
    
    return regression_outputs

def interpret_visualization_with_gemini(vis, agent1_data, model, backend, prompts_log_path, timeout):
    """
    Interpr√®te une visualisation en envoyant directement l'image √† Gemini via la fonction call_llm.
    
    Args:
        vis: M√©tadonn√©es de la visualisation avec base64 de l'image
        agent1_data: Donn√©es de l'agent1
        model: Mod√®le LLM √† utiliser
        backend: Backend pour les appels LLM
        prompts_log_path: Chemin pour sauvegarder les prompts
        
    Returns:
        str: Interpr√©tation de la visualisation
    """
    # Importer call_llm depuis llm_utils
    from llm_utils import call_llm
    
    # Valider que l'image est disponible
    if 'base64' not in vis or not vis['base64']:
        logger.error(f"Pas de donn√©es base64 disponibles pour la visualisation {vis.get('id', 'non identifi√©e')}")
        return "Erreur: Impossible d'analyser cette visualisation (image non disponible)"
    
    # Extraire le titre et le type
    if 'filename' in vis:
        filename = vis.get("filename", "figure.png")
        vis_id = os.path.splitext(filename)[0]
    else:
        vis_id = vis.get("id", "visualisation")
        filename = vis_id + '.png'
    
    # D√©terminer le type de visualisation
    vis_type = "Unknown visualization"
    if 'regression' in vis_id.lower():
        vis_type = "Table de R√©gression OLS"
    elif 'correlation' in vis_id.lower() or 'corr' in vis_id.lower():
        vis_type = "Matrice de Corr√©lation"
    elif 'distribution' in vis_id.lower() or 'hist' in vis_id.lower():
        vis_type = "Graphique de Distribution"
    elif 'scatter' in vis_id.lower() or 'relation' in vis_id.lower():
        vis_type = "Nuage de Points"
    elif 'box' in vis_id.lower():
        vis_type = "Bo√Æte √† Moustaches"
    
    # R√©cup√©rer le titre
    title = vis.get("title", vis_type)
    
    # Extraire des informations suppl√©mentaires pour le contexte
    metadata_str = json.dumps(agent1_data["metadata"], indent=2, ensure_ascii=False)
    extra_context = ""
    if 'metadata' in vis:
        if 'r_squared' in vis['metadata']:
            extra_context += f"\nR-squared: {vis['metadata']['r_squared']}"
        if 'variables' in vis['metadata']:
            extra_context += f"\nVariables principales: {', '.join(vis['metadata']['variables'])}"
    
    # Cr√©er le prompt pour Gemini
    prompt = f"""## INTERPR√âTATION DE VISUALISATION √âCONOMIQUE

### Type de visualisation
{vis_type}

### Titre
{title}

### Identifiant
{vis_id}

### M√©tadonn√©es sp√©cifiques
{extra_context}

### Contexte des donn√©es
Le dataset contient les variables suivantes: {', '.join(agent1_data["metadata"].get("noms_colonnes", []))}

### M√©tadonn√©es de l'ensemble de donn√©es
```json
{metadata_str[:500]}...
```

### Question de recherche initiale
{agent1_data.get("user_prompt", "Non disponible")}

---

Analyse cette visualisation √©conomique. Tu re√ßois directement l'image, donc base ton analyse sur ce que tu observes visuellement. Ton interpr√©tation doit:

1. D√©crire pr√©cis√©ment ce que montre la visualisation (tendances, relations, valeurs aberrantes)
2. Expliquer les relations entre les variables visibles
3. Mentionner les valeurs num√©riques sp√©cifiques (minimums, maximums, moyennes) que tu peux d√©duire visuellement
4. Relier cette visualisation √† la question de recherche

Ton interpr√©tation doit √™tre factuelle, pr√©cise et bas√©e uniquement sur ce que tu peux observer dans l'image. Reste √©conomique dans ton analyse en te concentrant sur les informations les plus importantes.
"""

    # Sauvegarder le prompt dans le fichier journal
    save_prompt_to_file(prompt, prompts_log_path, f"Gemini Image Interpretation - {vis_id}")
    
    try:
        # Utiliser call_llm de llm_utils qui g√®re maintenant les images
        logger.info(f"Appel √† Gemini avec image pour visualisation {vis_id}")
        interpretation = call_llm(
            prompt=prompt, 
            model_name=model, 
            backend=backend, 
            image_base64=vis['base64']
        )
        
        logger.info(f"Interpr√©tation g√©n√©r√©e par Gemini pour: {vis_id}")
        return interpretation
    except Exception as e:
        logger.error(f"Erreur lors de l'interpr√©tation de l'image pour {vis_id}: {e}")
        return f"Erreur d'interpr√©tation: {e}"

# --- REMPLACEMENT DE LA FONCTION ---
def generate_visualization_interpretations(visualizations, regression_outputs, agent1_data, model, backend, prompts_log_path, timeout=120):
    """
    G√©n√®re des interpr√©tations pour les visualisations S√âQUENTIELLEMENT (SANS THREADING).
    Utilise le LLM Gemini directement √† partir des images.

    Args:
        visualizations (list): Liste des m√©tadonn√©es des visualisations.
        regression_outputs (list): Liste des sorties des tables de r√©gression.
        agent1_data (dict): Donn√©es de l'agent1.
        model (str): Mod√®le LLM √† utiliser.
        backend (str): Backend pour les appels LLM.
        prompts_log_path (str): Chemin pour sauvegarder les prompts.
        timeout (int): Timeout en secondes pour chaque appel LLM d'interpr√©tation. D√©faut: 120.

    Returns:
        Liste mise √† jour des visualisations avec interpr√©tations.
    """
    # Combiner visualisations et r√©gressions
    all_visuals = visualizations + regression_outputs
    if not all_visuals:
        logger.info("Aucune visualisation ou table de r√©gression √† interpr√©ter.")
        return []

    logger.info(f"Lancement des interpr√©tations S√âQUENTIELLES pour {len(all_visuals)} √©l√©ments (timeout/item: {timeout}s)")

    updated_visuals = [] # Nouvelle liste pour stocker les r√©sultats mis √† jour

    for i, vis in enumerate(all_visuals):
        # G√©n√©rer un ID unique si n√©cessaire pour le mapping des r√©sultats
        if 'id' not in vis: # Assigner un ID si manquant
             vis_prefix = os.path.splitext(vis.get("filename", "unknown"))[0] if 'filename' in vis else vis.get("type", "item")
             vis['id'] = f"{vis_prefix}_{i}_{int(time.time()*1000)}"

        vis_id = vis['id'] # Utiliser l'ID assign√©

        # V√©rifier que l'image est disponible en base64
        if 'base64' not in vis or not vis['base64']:
            logger.warning(f"({i+1}/{len(all_visuals)}) Image manquante pour {vis_id}. Interpr√©tation impossible.")
            vis['interpretation'] = "Interpr√©tation impossible: image non disponible"
            updated_visuals.append(vis) # Ajouter l'√©l√©ment avec l'erreur
            continue # Passe √† la visualisation suivante

        logger.info(f"({i+1}/{len(all_visuals)}) Interpr√©tation de : {vis_id}")
        try:
            # Appel DIRECT √† la fonction d'interpr√©tation
            interpretation = interpret_visualization_with_gemini(
                vis,
                agent1_data,
                model,
                backend,
                prompts_log_path,
                timeout # Passer le timeout
            )
            vis['interpretation'] = interpretation
            logger.info(f"({i+1}/{len(all_visuals)}) Interpr√©tation re√ßue pour {vis_id}.")

        except Exception as e:
            logger.error(f"({i+1}/{len(all_visuals)}) Erreur lors de l'interpr√©tation directe pour {vis_id}: {e}", exc_info=True)
            vis['interpretation'] = f"Erreur d'interpr√©tation (directe): {e}"

        updated_visuals.append(vis) # Ajouter l'√©l√©ment trait√© (avec succ√®s ou erreur)

    logger.info(f"Toutes les {len(all_visuals)} interpr√©tations s√©quentielles sont termin√©es.")

    return updated_visuals

# --- FIN REMPLACEMENT FONCTION ---
def attempt_execution_loop(code: str, csv_file: str, agent1_data: dict, model: str, prompts_log_path: str, backend: str) -> dict:
    """
    Tente d'ex√©cuter le code et, en cas d'erreur, demande une correction au LLM en utilisant le backend choisi.
    
    Args:
        code: Code Python √† ex√©cuter
        csv_file: Chemin absolu du fichier CSV
        agent1_data: Donn√©es de l'agent1
        model: Mod√®le LLM √† utiliser
        prompts_log_path: Chemin pour sauvegarder les prompts
        backend: Backend pour les appels LLM ('ollama' ou 'gemini')
        
    Returns:
        Dictionnaire contenant les r√©sultats de l'ex√©cution
    """
    import base64
    from collections import deque
    import time
    import shutil
    import os
    import tempfile
    import json
    import subprocess
    import logging
    from datetime import datetime

    logger = logging.getLogger("agent2")

    # Initialiser le mod√®le courant (sera mis √† jour si on bascule vers le mod√®le puissant)
    current_model = model

    # V√©rifier si le code utilise le bon chemin CSV
    if csv_file not in code:
        logger.warning(f"Le code initial ne semble pas utiliser le chemin absolu '{csv_file}'.")

    # Cr√©er les r√©pertoires temporaires pour l'ex√©cution
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    temp_dir = tempfile.mkdtemp(prefix=f"analysis_{timestamp}_")
    
    # Ajouter un r√©pertoire pour les tables de r√©gression
    tables_dir = os.path.join(temp_dir, "tables")
    os.makedirs(tables_dir, exist_ok=True)
    
    # Ajouter un r√©pertoire pour les visualisations
    vis_dir = os.path.join(temp_dir, "visualisations")
    os.makedirs(vis_dir, exist_ok=True)

    # Cr√©er un r√©pertoire pour sauvegarder les versions du code
    code_versions_dir = os.path.join("outputs", f"code_versions_{timestamp}")
    os.makedirs(code_versions_dir, exist_ok=True)
    logger.info(f"Les versions de code seront sauvegard√©es dans {code_versions_dir}")

    import textwrap

# Code pour capturer les visualisations - MODIFI√â AVEC SUPPORT CSV AM√âLIOR√â
    vis_code = textwrap.dedent(f"""
# Ajout pour capturer les visualisations
import os
import logging
import io
import re
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
import json
import numpy as np

# Configuration pour un meilleur rendu - Utilisation d'un style compatible avec les versions r√©centes de matplotlib
try:
    plt.style.use('whitegrid')  # Pour seaborn r√©cent
except:
    try:
        plt.style.use('seaborn')  # Fallback vers le style seaborn de base
    except:
        pass  # Utiliser le style par d√©faut si seaborn n'est pas disponible

# D√©finition des r√©pertoires de sortie
VIS_DIR = r"{vis_dir}"
TABLES_DIR = r"{tables_dir}"
os.makedirs(VIS_DIR, exist_ok=True)
os.makedirs(TABLES_DIR, exist_ok=True)

# Configuration du logger
vis_logger = logging.getLogger("visualisation_capture")

# Redirection de l'affichage pour capturer les sorties
from io import StringIO
import sys

# Classe pour capturer stdout
class CaptureOutput:
    def __init__(self):
        self.old_stdout = sys.stdout
        self.buffer = StringIO()

    def __enter__(self):
        sys.stdout = self.buffer
        return self.buffer

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout = self.old_stdout

# Classe pour convertir les donn√©es numpy en JSON
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        return super(NumpyEncoder, self).default(obj)

# Remplacer la fonction plt.show() pour enregistrer les figures
_original_show = plt.show
_fig_counter = 0

def _custom_show(*args, **kwargs):
    global _fig_counter
    try:
        fig = plt.gcf()
        if fig.get_axes():
            _fig_counter += 1
            filepath = os.path.join(VIS_DIR, f"figure_{{_fig_counter}}.png")

            # Capturer les donn√©es du graphique actuel
            fig_data = {{}}

            # MODIFICATION: Capturer les informations sur les axes et le titre
            fig_title = fig.get_label() or f"Figure {{_fig_counter}}"
            fig_data["title"] = fig_title

            # MODIFICATION: Essayer de capturer les donn√©es utilis√©es pour la visualisation
            for ax_idx, ax in enumerate(fig.get_axes()):
                ax_data = {{}}
                
                # Capturer les titres et labels d'axes
                x_label = ax.get_xlabel() or "Axe X"
                y_label = ax.get_ylabel() or "Axe Y"
                ax_title = ax.get_title() or fig_title
                
                ax_data["x_label"] = x_label
                ax_data["y_label"] = y_label
                ax_data["title"] = ax_title
                
                # Capturer les donn√©es des lignes
                for line_idx, line in enumerate(ax.get_lines()):
                    line_id = f"ax{{ax_idx}}_line{{line_idx}}"
                    x_data = line.get_xdata()
                    y_data = line.get_ydata()
                    
                    # Cr√©er un petit dataframe et le convertir en CSV avec contexte
                    if len(x_data) > 0 and len(y_data) > 0:
                        try:
                            import pandas as pd
                            # Utiliser les labels des axes comme noms de colonnes
                            line_df = pd.DataFrame({{
                                x_label: x_data, 
                                y_label: y_data
                            }})
                            
                            # CORRECTION: R√©cup√©rer le titre de l'axe depuis ax_data ou utiliser une valeur par d√©faut
                            current_ax_title = ax_data.get("title", f"Figure {{_fig_counter}}")
                            
                            # Ajouter des m√©tadonn√©es en commentaire CSV
                            csv_header = f"# {{current_ax_title}} - Relation entre {{x_label}} et {{y_label}}\\n"  # Ajout du caract√®re \\n √† la fin
                            csv_header += f"# Source: Figure {{_fig_counter}}, Ligne {{line_idx+1}}\\n"
                            
                            # Ajouter des statistiques en commentaire CSV
                            if hasattr(x_data, 'min') and hasattr(y_data, 'min'):
                                stats = f"# Statistiques {{x_label}}: min={{x_data.min():.2f}}, max={{x_data.max():.2f}}, moyenne={{x_data.mean():.2f}}\\n"
                                stats += f"# Statistiques {{y_label}}: min={{y_data.min():.2f}}, max={{y_data.max():.2f}}, moyenne={{y_data.mean():.2f}}\\n"
                                csv_header += stats
                            
                            csv_data = csv_header + line_df.to_csv(index=False)
                            
                            ax_data[line_id] = {{
                                "type": "line",
                                "x": x_data.tolist() if hasattr(x_data, 'tolist') else list(x_data),
                                "y": y_data.tolist() if hasattr(y_data, 'tolist') else list(y_data),
                                "label": line.get_label() if line.get_label() != '_nolegend_' else f"Line {{line_idx+1}}",
                                "x_label": x_label,  # Ajout du label de l'axe x
                                "y_label": y_label,  # Ajout du label de l'axe y
                                "title": current_ax_title,   # Ajout du titre (corrig√©)
                                "csv_data": csv_data
                            }}
                        except Exception as e:
                            vis_logger.error(f"Erreur lors de la conversion des donn√©es en CSV am√©lior√©: {{e}}")
                            ax_data[line_id] = {{
                                "type": "line",
                                "x": x_data.tolist() if hasattr(x_data, 'tolist') else list(x_data),
                                "y": y_data.tolist() if hasattr(y_data, 'tolist') else list(y_data),
                                "label": line.get_label() if line.get_label() != '_nolegend_' else f"Line {{line_idx+1}}",
                                "x_label": x_label,
                                "y_label": y_label
                            }}

                # Extraire les lignes, barres, etc.
                for line_idx, line in enumerate(ax.get_lines()):
                    line_id = f"ax{{ax_idx}}_line{{line_idx}}"
                    if line_id not in ax_data:  # Si n'est pas d√©j√† ajout√© avec CSV
                        ax_data[line_id] = {{
                            "type": "line",
                            "x": line.get_xdata().tolist() if hasattr(line, 'get_xdata') else [],
                            "y": line.get_ydata().tolist() if hasattr(line, 'get_ydata') else [],
                            "label": line.get_label() if line.get_label() != '_nolegend_' else f"Line {{line_idx+1}}",
                            "x_label": x_label,
                            "y_label": y_label
                        }}

                # Barres
                for container_idx, container in enumerate(ax.containers):
                    if isinstance(container, matplotlib.container.BarContainer):
                        container_id = f"ax{{ax_idx}}_bars{{container_idx}}"
                        rectangles = container.get_children()
                        bar_data = {{
                            "type": "bar",
                            "heights": [],
                            "positions": [],
                            "widths": [],
                            "x_label": x_label,
                            "y_label": y_label
                        }}
                        for rect in rectangles:
                            if hasattr(rect, 'get_height') and hasattr(rect, 'get_x') and hasattr(rect, 'get_width'):
                                bar_data["heights"].append(rect.get_height())
                                bar_data["positions"].append(rect.get_x())
                                bar_data["widths"].append(rect.get_width())
                        ax_data[container_id] = bar_data
                        
                        # Essayer de convertir les donn√©es de barres en CSV avec contexte
                        try:
                            import pandas as pd
                            
                            # Cr√©er un DataFrame pour les barres avec noms personnalis√©s
                            bar_df = pd.DataFrame({{
                                x_label: bar_data["positions"],
                                f"{{y_label}} (hauteur)": bar_data["heights"],
                                "Largeur": bar_data["widths"]
                            }})
                            
                            # CORRECTION: R√©cup√©rer le titre de l'axe depuis ax_data ou utiliser une valeur par d√©faut
                            current_ax_title = ax_data.get("title", f"Figure {{_fig_counter}}")
                            
                            # Ajouter des m√©tadonn√©es en commentaire CSV
                            csv_header = f"# {{current_ax_title}} - Graphique √† barres\\n"
                            csv_header += f"# Source: Figure {{_fig_counter}}, Container {{container_idx+1}}\\n"
                            csv_header += f"# X: {{x_label}}, Y: {{y_label}}\\n"
                            
                            # Ajouter des statistiques
                            if len(bar_data["heights"]) > 0:
                                heights = np.array(bar_data["heights"])
                                stats = f"# Statistiques hauteurs: min={{heights.min():.2f}}, max={{heights.max():.2f}}, moyenne={{heights.mean():.2f}}\\n"
                                csv_header += stats
                            
                            ax_data[container_id]["csv_data"] = csv_header + bar_df.to_csv(index=False)
                        except Exception as e:
                            vis_logger.error(f"Erreur lors de la conversion des donn√©es de barres en CSV am√©lior√©: {{e}}")

                # Collections (scatter plots, heatmaps)
                for collection_idx, collection in enumerate(ax.collections):
                    collection_id = f"ax{{ax_idx}}_collection{{collection_idx}}"
                    collection_data = {{"type": "collection", "x_label": x_label, "y_label": y_label}}

                    # Points (scatter)
                    if hasattr(collection, 'get_offsets'):
                        offsets = collection.get_offsets()
                        if len(offsets) > 0:
                            collection_data["points"] = offsets.tolist() if hasattr(offsets, 'tolist') else []
                            
                            # Convertir en CSV am√©lior√© avec contexte
                            try:
                                import pandas as pd
                                offsets_array = np.array(offsets)
                                if offsets_array.shape[1] == 2:  # Si ce sont des points 2D
                                    # Utiliser les labels des axes comme noms de colonnes
                                    points_df = pd.DataFrame({{
                                        x_label: offsets_array[:, 0],
                                        y_label: offsets_array[:, 1]
                                    }})
                                    
                                    # CORRECTION: R√©cup√©rer le titre de l'axe depuis ax_data ou utiliser une valeur par d√©faut
                                    current_ax_title = ax_data.get("title", f"Figure {{_fig_counter}}")
                                    
                                    # Ajouter des m√©tadonn√©es en commentaire CSV
                                    csv_header = f"# {{current_ax_title}} - Nuage de points\\n"
                                    csv_header += f"# Source: Figure {{_fig_counter}}, Collection {{collection_idx+1}}\\n"
                                    
                                    # Ajouter des statistiques
                                    x_stats = f"# Statistiques {{x_label}}: min={{offsets_array[:, 0].min():.2f}}, max={{offsets_array[:, 0].max():.2f}}, moyenne={{offsets_array[:, 0].mean():.2f}}\\n"
                                    y_stats = f"# Statistiques {{y_label}}: min={{offsets_array[:, 1].min():.2f}}, max={{offsets_array[:, 1].max():.2f}}, moyenne={{offsets_array[:, 1].mean():.2f}}\\n"
                                    csv_header += x_stats + y_stats
                                    
                                    collection_data["csv_data"] = csv_header + points_df.to_csv(index=False)
                            except Exception as e:
                                vis_logger.error(f"Erreur lors de la conversion des points en CSV am√©lior√©: {{e}}")

                    # Couleurs
                    if hasattr(collection, 'get_array') and collection.get_array() is not None:
                        collection_data["values"] = collection.get_array().tolist() if hasattr(collection.get_array(), 'tolist') else []

                    ax_data[collection_id] = collection_data

                # Tentative d'extraire les √©tiquettes des axes
                if ax.get_title():
                    ax_data["title"] = ax.get_title()
                if ax.get_xlabel():
                    ax_data["xlabel"] = ax.get_xlabel()
                if ax.get_ylabel():
                    ax_data["ylabel"] = ax.get_ylabel()

                # Tentative d'extraire les limites des axes
                if ax.get_xlim():
                    ax_data["xlim"] = ax.get_xlim()
                if ax.get_ylim():
                    ax_data["ylim"] = ax.get_ylim()

                fig_data[f"axes{{ax_idx}}"] = ax_data

            # Information globale sur la figure
            if fig.get_label():
                fig_data["title"] = fig.get_label()
            else:
                fig_data["title"] = f"Figure {{_fig_counter}}"
                
            # Essayer d'extraire directement un dataframe de la figure si possible
            fig_csv_data = ""
            try:
                # Parcourir les objets pour trouver un DataFrame
                for ax in fig.get_axes():
                    for line in ax.get_lines():
                        if hasattr(line, '_data_source') and hasattr(line._data_source, 'to_csv'):
                            title = fig_data.get("title", "Figure")
                            x_label = ax.get_xlabel() or "X"
                            y_label = ax.get_ylabel() or "Y"
                            
                            # Cr√©er un en-t√™te CSV avec m√©tadonn√©es
                            csv_header = f"# {{title}} - Donn√©es source\\n"
                            csv_header += f"# Axes: {{x_label}} vs {{y_label}}\\n"
                            # Ajouter des statistiques si possible
                            if hasattr(line._data_source, 'describe'):
                                try:
                                    describe = line._data_source.describe().to_dict()
                                    for col, stats in describe.items():
                                        csv_header += f"# Statistiques {{col}}: min={{stats.get('min', 'N/A')}}, max={{stats.get('max', 'N/A')}}, moyenne={{stats.get('mean', 'N/A')}}\\n"
                                except:
                                    pass
                            
                            fig_csv_data = csv_header + line._data_source.to_csv(index=False)
                            break
            except Exception as e:
                vis_logger.error(f"Erreur lors de l'extraction directe du DataFrame: {{e}}")
            
            if fig_csv_data:
                fig_data["csv_data"] = fig_csv_data

            try:
                plt.savefig(filepath, bbox_inches='tight', dpi=100)
                vis_logger.info(f"Visualisation sauvegard√©e : {{filepath}}")

                # Sauvegarde des donn√©es dans un fichier JSON
                data_filepath = os.path.join(VIS_DIR, f"figure_{{_fig_counter}}_data.json")
                with open(data_filepath, 'w', encoding='utf-8') as f:
                    json.dump(fig_data, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
                vis_logger.info(f"Donn√©es de visualisation sauvegard√©es : {{data_filepath}}")
                
                # Sauvegarde des donn√©es CSV si disponibles
                if any(["csv_data" in data for ax_data in fig_data.values() if isinstance(ax_data, dict) 
                       for data in ax_data.values() if isinstance(data, dict)]) or "csv_data" in fig_data:
                    csv_filepath = os.path.join(VIS_DIR, f"figure_{{_fig_counter}}_data.csv")
                    
                    # Choisir la premi√®re source de donn√©es CSV disponible
                    csv_content = fig_data.get("csv_data", "")
                    if not csv_content:
                        for ax_data in fig_data.values():
                            if isinstance(ax_data, dict):
                                for data in ax_data.values():
                                    if isinstance(data, dict) and "csv_data" in data:
                                        csv_content = data["csv_data"]
                                        break
                                if csv_content:
                                    break
                    
                    if csv_content:
                        with open(csv_filepath, 'w', encoding='utf-8') as f:
                            f.write(csv_content)
                        vis_logger.info(f"Donn√©es CSV am√©lior√©es sauvegard√©es : {{csv_filepath}}")

            except Exception as e:
                vis_logger.error(f"Impossible de sauvegarder la figure {{_fig_counter}} √† {{filepath}}: {{e}}")

            # On permet l'affichage pour d√©bogage mais √ßa sera ignor√© dans l'environnement non-interactif
            _original_show(*args, **kwargs)
    except Exception as e:
        vis_logger.error(f"Erreur dans _custom_show: {{e}}")

plt.show = _custom_show

# Fonction pour capturer les sorties de r√©gression OLS de statsmodels
_original_print = print
def _custom_print(*args, **kwargs):
    output = " ".join(str(arg) for arg in args)
    _original_print(*args, **kwargs)  # Affiche normalement

    # D√©tecter si c'est un r√©sultat de r√©gression OLS
    if "OLS Regression Results" in output and "=" * 10 in output:
        try:
            # Extraire la table de r√©gression compl√®te
            global _tables_counter
            if '_tables_counter' not in globals():
                _tables_counter = 0
            _tables_counter += 1

            # Sauvegarder le texte
            filepath = os.path.join(TABLES_DIR, f"regression_{{_tables_counter}}.txt")
            with open(filepath, "w") as f:
                f.write(output)

            # Cr√©er une visualisation de la table
            img_path = os.path.join(TABLES_DIR, f"regression_{{_tables_counter}}.png")
            plt.figure(figsize=(12, 10))
            plt.text(0.01, 0.99, output, family='monospace', fontsize=10, verticalalignment='top')
            plt.axis('off')
            plt.tight_layout()
            plt.savefig(img_path, dpi=100, bbox_inches='tight')
            plt.close()

            # Extraire et sauvegarder les donn√©es de r√©gression
            # Extraire R-squared
            r_squared_match = re.search(r"R-squared:\\s*([\d\.]+)", output)
            r_squared = r_squared_match.group(1) if r_squared_match else "N/A"

            # Extraire les variables et coefficients
            coef_section = re.search(r"==+\\s*\\n\\s*(coef.*?)(?:\\n\\s*==+|\\Z)", output, re.DOTALL)
            regression_data = {{
                "r_squared": r_squared,
                "coefficients": []
            }}

            if coef_section:
                lines = coef_section.group(1).split('\\n') # Utilisation de \\n car c'est dans une string Python
                headers = None

                for line in lines:
                    if "coef" in line.lower():
                        # C'est la ligne d'en-t√™te
                        headers = re.split(r'\\s{{2,}}', line.strip()) # Utilisation de \\s{{2,}}
                    elif line.strip() and headers:
                        # C'est une ligne de donn√©es
                        values = re.split(r'\\s{{2,}}', line.strip()) # Utilisation de \\s{{2,}}
                        if len(values) >= len(headers):
                            variable = values[0]
                            coef_data = {{
                                "variable": variable,
                                "coef": values[1] if len(values) > 1 else "N/A",
                                "std_err": values[2] if len(values) > 2 else "N/A",
                                "p_value": values[4] if len(values) > 4 else "N/A"
                            }}
                            regression_data["coefficients"].append(coef_data)
                            
            # NOUVELLE SECTION: Convertir les coefficients en CSV am√©lior√©
            try:
                import pandas as pd
                if regression_data["coefficients"]:
                    coef_df = pd.DataFrame(regression_data["coefficients"])
                    
                    # Ajouter une colonne de significativit√© pour faciliter l'interpr√©tation
                    try:
                        coef_df['significatif'] = coef_df['p_value'].astype(float) < 0.05
                    except:
                        # Si conversion en float √©choue, on continue sans cette colonne
                        pass
                    
                    # Ajouter des m√©tadonn√©es contextuelles en commentaire
                    csv_header = f"# R√©sultats de r√©gression OLS - Table {{_tables_counter}}\\n"
                    csv_header += f"# R¬≤: {{r_squared}}\\n"
                    csv_header += f"# Interpr√©tation: un coefficient positif indique une relation positive avec la variable d√©pendante\\n"
                    csv_header += f"# Une p-value < 0.05 indique que le coefficient est statistiquement significatif\\n"
                    
                    regression_data["csv_data"] = csv_header + coef_df.to_csv(index=False)
                    
                    # Sauvegarder aussi en fichier CSV am√©lior√©
                    csv_path = os.path.join(TABLES_DIR, f"regression_{{_tables_counter}}_coefficients.csv")
                    with open(csv_path, 'w', encoding='utf-8') as f:
                        f.write(regression_data["csv_data"])
                    vis_logger.info(f"Donn√©es CSV am√©lior√©es de coefficients sauvegard√©es: {{csv_path}}")
            except Exception as e:
                vis_logger.error(f"Erreur lors de la conversion des coefficients en CSV am√©lior√©: {{e}}")

            # Sauvegarder les donn√©es
            data_path = os.path.join(TABLES_DIR, f"regression_{{_tables_counter}}_data.json")
            with open(data_path, 'w', encoding='utf-8') as f:
                json.dump(regression_data, f, ensure_ascii=False, indent=2)

            vis_logger.info(f"Table de r√©gression sauvegard√©e : {{filepath}}")
            vis_logger.info(f"Donn√©es de r√©gression sauvegard√©es : {{data_path}}")
        except Exception as e:
            vis_logger.error(f"Erreur lors de la capture d'une table de r√©gression: {{e}}")

# Remplacer la fonction print
print = _custom_print

# Fonction manuelle pour sauvegarder des figures
def save_figure(fig, name):
    filepath = os.path.join(VIS_DIR, f"{{name}}.png")
    fig.savefig(filepath, bbox_inches='tight', dpi=100)
    vis_logger.info(f"Figure sauvegard√©e manuellement: {{filepath}}")
    return filepath
""") # Fin de la string vis_code


    # Initialiser les variables pour la boucle d'ex√©cution
    temp_filename = os.path.join(temp_dir, "analysis_script.py")
    attempt = 0
    llm_correction_attempt = 0  # Compteur sp√©cifique pour les tentatives de correction par LLM
    execution_results = {"success": False}
    execution_output = ""
    all_code_versions = []
    recent_errors = deque(maxlen=3)
    max_attempts = 10
    current_stderr = ""

    # Flag pour le mod√®le puissant
    tried_powerful_model = False
    
    # Boucle principale d'ex√©cution
    while attempt < max_attempts:
        attempt += 1
        logger.info(f"--- Tentative d'ex√©cution {attempt}/{max_attempts} ---")

        # Sanitize le code avant l'ex√©cution
        try:
            valid_columns = agent1_data["metadata"].get("noms_colonnes", [])
            sanitized_code = sanitize_code(code, csv_file, valid_columns)
            if sanitized_code != code:
                logger.info("Code assaini (chemin CSV, colonnes, df_numeric).")
                code = sanitized_code
        except Exception as e:
            logger.error(f"Erreur pendant l'assainissement du code: {e}. Tentative avec le code actuel.")

        # Sauvegarder la version actuelle du code
        current_code_path = os.path.join(code_versions_dir, f"attempt_{attempt}.py")
        try:
            with open(current_code_path, "w", encoding="utf-8") as f:
                f.write(code)
            all_code_versions.append(current_code_path)
            logger.info(f"Version {attempt} du code sauvegard√©e: {current_code_path}")
        except Exception as e:
             logger.error(f"Impossible de sauvegarder la version {attempt} du code : {e}")

        # Par celles-ci:
        import textwrap
        vis_code_dedented = textwrap.dedent(vis_code)  # Conserve l'indentation relative
        modified_code = vis_code_dedented + "\n\n" + code.strip()

        # √âcrire le code modifi√© dans un fichier temporaire
        try:
            with open(temp_filename, "w", encoding="utf-8") as f:
                f.write(modified_code)
        except Exception as e:
            logger.error(f"Impossible d'√©crire le script temporaire {temp_filename}: {e}")
            execution_results = {"success": False, "error": "Erreur √©criture fichier temporaire", "stderr": str(e)}
            break

                            # <<< AJOUTER CE BLOC DE D√âBOGAGE >>>
        logger.info(f"--- DEBUG: V√©rification du contenu √©crit dans {temp_filename} ---")
        try:
            with open(temp_filename, "r", encoding="utf-8") as f_read:
                lines_to_check = 5
                lines = []
                for i in range(lines_to_check):
                    try:
                        lines.append(next(f_read).rstrip('\n'))
                    except StopIteration:
                        break # Fin du fichier
                logger.info(f"Premi√®res {len(lines)} lignes (espaces repr√©sent√©s par '¬∑', tabulations par '\\t'):")
                for i, line in enumerate(lines):
                    # Repr√©sentation visuelle des espaces/tabs
                    line_repr = line.replace(' ', '¬∑').replace('\t', '\\t') 
                    logger.info(f"  Ligne {i+1}: [{line_repr}]")
        except Exception as read_err:
            logger.error(f"  Erreur de lecture du fichier temporaire pour d√©bogage: {read_err}")
        logger.info("--- FIN DEBUG ---")
        # <<< FIN DU BLOC DE D√âBOGAGE >>>


        # Ex√©cuter le code
        try:
            logger.info(f"Ex√©cution de: python3 {temp_filename}")
            result = subprocess.run(
                ["python3", temp_filename],
                capture_output=True, text=True, check=True, timeout=300
            )
            logger.info("Ex√©cution r√©ussie")
            logger.debug(f"Sortie standard (stdout):\n{result.stdout}")
            execution_output = result.stdout
            execution_results = {
                "success": True,
                "output": result.stdout,
                "temp_dir": temp_dir,
                "vis_dir": vis_dir,
                "tables_dir": tables_dir,
                "script_path": temp_filename,
                "all_code_versions": all_code_versions,
                "final_code_used_path": current_code_path
            }

            # Traiter les visualisations
            vis_files = []
            if os.path.exists(vis_dir):
                logger.info(f"Recherche de visualisations dans {vis_dir}")
                for file in sorted(os.listdir(vis_dir)):
                    if file.lower().endswith(('.png', '.jpg', '.jpeg', '.svg')):
                        file_path = os.path.join(vis_dir, file)
                        try:
                            with open(file_path, 'rb') as img_file:
                                img_data = base64.b64encode(img_file.read()).decode('utf-8')
                            # V√©rifier la taille de l'image pour le d√©bogage
                            file_size = os.path.getsize(file_path)
                            logger.info(f"  -> Visualisation trouv√©e: {file}, taille: {file_size} octets")
                            
                            # Chercher les donn√©es associ√©es
                            fig_name = os.path.splitext(file)[0]
                            data_path = os.path.join(vis_dir, f"{fig_name}_data.json")
                            chart_data = None
                            if os.path.exists(data_path):
                                try:
                                    with open(data_path, 'r', encoding='utf-8') as f:
                                        chart_data = json.load(f)
                                    logger.info(f"  -> Donn√©es associ√©es trouv√©es pour: {file}")
                                except Exception as e:
                                    logger.error(f"Impossible de lire les donn√©es associ√©es √† {file}: {e}")
                            
                            # Chercher les donn√©es CSV associ√©es
                            csv_path = os.path.join(vis_dir, f"{fig_name}_data.csv")
                            csv_data = ""
                            if os.path.exists(csv_path):
                                try:
                                    with open(csv_path, 'r', encoding='utf-8') as f:
                                        csv_data = f.read()
                                    logger.info(f"  -> Donn√©es CSV trouv√©es pour: {file}")
                                except Exception as e:
                                    logger.error(f"Impossible de lire les donn√©es CSV associ√©es √† {file}: {e}")
                            
                            # Si pas de fichier CSV, essayer d'extraire des donn√©es CSV imbriqu√©es
                            if not csv_data and chart_data:
                                # Parcourir chart_data pour trouver csv_data
                                for axes_key, axes_value in chart_data.items():
                                    if isinstance(axes_value, dict):
                                        for line_key, line_value in axes_value.items():
                                            if isinstance(line_value, dict) and 'csv_data' in line_value:
                                                csv_data = line_value['csv_data']
                                                logger.info(f"  -> Donn√©es CSV int√©gr√©es trouv√©es pour: {file}")
                                                break
                                    if csv_data:
                                        break
                            
                            vis_files.append({
                                'filename': file,
                                'path': file_path,
                                'base64': img_data,
                                'title': chart_data.get('title', ' '.join(os.path.splitext(file)[0].split('_')).capitalize()) if chart_data else ' '.join(os.path.splitext(file)[0].split('_')).capitalize(),
                                'size': file_size,
                                'data': chart_data,
                                'csv_data': csv_data  # Conserver les donn√©es CSV pour compatibilit√©
                            })
                        except Exception as e:
                            logger.error(f"Impossible de lire ou encoder l'image {file_path}: {e}")
            else:
                logger.warning(f"R√©pertoire de visualisations non trouv√©: {vis_dir}")

            # Capturer les tables de r√©gression
            regression_outputs = capture_regression_outputs(result.stdout, tables_dir)
            
            # V√©rifier √©galement si des tables ont √©t√© sauvegard√©es directement par le script
            if os.path.exists(tables_dir):
                logger.info(f"Recherche de tables de r√©gression dans {tables_dir}")
                for file in sorted(os.listdir(tables_dir)):
                    # Ne traiter que les fichiers texte qui ne sont pas d√©j√† trait√©s
                    if file.lower().endswith('.txt') and not any(r['text_path'].endswith(file) for r in regression_outputs):
                        text_path = os.path.join(tables_dir, file)
                        img_path = os.path.join(tables_dir, os.path.splitext(file)[0] + '.png')
                        
                        # Lire le contenu du fichier texte
                        try:
                            with open(text_path, 'r', encoding='utf-8') as f:
                                table_content = f.read()
                            
                            # V√©rifier si c'est une table de r√©gression OLS
                            if "OLS Regression Results" in table_content:
                                table_id = os.path.splitext(file)[0]
                                
                                # Cr√©er l'image si elle n'existe pas
                                if not os.path.exists(img_path):
                                    plt.figure(figsize=(12, 10))
                                    plt.text(0.01, 0.99, table_content, family='monospace', fontsize=10, verticalalignment='top')
                                    plt.axis('off')
                                    plt.tight_layout()
                                    plt.savefig(img_path, dpi=100, bbox_inches='tight')
                                    plt.close()
                                
                                # Encoder l'image en base64
                                with open(img_path, 'rb') as img_file:
                                    img_data = base64.b64encode(img_file.read()).decode('utf-8')
                                
                                # Extraire des m√©tadonn√©es basiques
                                r_squared_match = re.search(r"R-squared:\s*([\d\.]+)", table_content)
                                r_squared = r_squared_match.group(1) if r_squared_match else "N/A"
                                
                                # Chercher le fichier de donn√©es JSON associ√©
                                data_path = os.path.join(tables_dir, f"{table_id}_data.json")
                                regression_data = None
                                if os.path.exists(data_path):
                                    try:
                                        with open(data_path, 'r', encoding='utf-8') as f:
                                            regression_data = json.load(f)
                                        logger.info(f"  -> Donn√©es de r√©gression trouv√©es pour: {file}")
                                    except Exception as e:
                                        logger.error(f"Impossible de lire les donn√©es de r√©gression pour {file}: {e}")
                                
                                # Chercher les donn√©es CSV des coefficients
                                csv_path = os.path.join(tables_dir, f"{table_id}_coefficients.csv")
                                csv_data = ""
                                if os.path.exists(csv_path):
                                    try:
                                        with open(csv_path, 'r', encoding='utf-8') as f:
                                            csv_data = f.read()
                                        logger.info(f"  -> Donn√©es CSV des coefficients trouv√©es pour: {file}")
                                    except Exception as e:
                                        logger.error(f"Impossible de lire les donn√©es CSV pour {file}: {e}")
                                
                                # Si pas de fichier CSV mais donn√©es JSON disponibles
                                if not csv_data and regression_data and "csv_data" in regression_data:
                                    csv_data = regression_data["csv_data"]
                                    logger.info(f"  -> Donn√©es CSV int√©gr√©es trouv√©es pour: {file}")
                                
                                # V√©rifier la taille de l'image pour le d√©bogage
                                file_size = os.path.getsize(img_path)
                                logger.info(f"  -> Table de r√©gression trouv√©e: {file}, taille: {file_size} octets")
                                
                                regression_outputs.append({
                                    'type': 'regression_table',
                                    'id': table_id,
                                    'text_path': text_path,
                                    'image_path': img_path,
                                    'base64': img_data,
                                    'title': f"R√©sultats de R√©gression: {table_id.replace('_', ' ').capitalize()}",
                                    'metadata': {
                                        'r_squared': r_squared
                                    },
                                    'size': file_size,
                                    'data': regression_data,
                                    'csv_data': csv_data  # Conserver les donn√©es CSV pour compatibilit√©
                                })
                        except Exception as e:
                            logger.error(f"Erreur lors du traitement de la table {text_path}: {e}")

            # Mettre √† jour les r√©sultats
            execution_results["visualisations"] = vis_files
            execution_results["regressions"] = regression_outputs
            logger.info(f"{len(vis_files)} visualisation(s) et {len(regression_outputs)} table(s) de r√©gression trait√©e(s).")
            
            # Journaliser plus de d√©tails sur les visualisations pour le d√©bogage
            for i, vis in enumerate(vis_files):
                logger.info(f"  Visualisation #{i+1}: {vis.get('filename')}, taille: {vis.get('size', 'inconnue')} octets")
                if 'base64' in vis:
                    logger.info(f"    Longueur base64: {len(vis['base64'])} caract√®res")
                if 'data' in vis and vis['data']:
                    logger.info(f"    Donn√©es pr√©sentes: Oui")
                if 'csv_data' in vis and vis['csv_data']:
                    logger.info(f"    Donn√©es CSV pr√©sentes: {len(vis['csv_data'])} caract√®res")
                
            for i, reg in enumerate(regression_outputs):
                logger.info(f"  R√©gression #{i+1}: {reg.get('id')}, taille: {reg.get('size', 'inconnue')} octets")
                if 'base64' in reg:
                    logger.info(f"    Longueur base64: {len(reg['base64'])} caract√®res")
                if 'data' in reg and reg['data']:
                    logger.info(f"    Donn√©es structur√©es: Oui")
                if 'csv_data' in reg and reg['csv_data']:
                    logger.info(f"    Donn√©es CSV pr√©sentes: {len(reg['csv_data'])} caract√®res")
            
            break

        # Gestion des erreurs d'ex√©cution
        except subprocess.TimeoutExpired:
             logger.error("L'ex√©cution a d√©pass√© le d√©lai de 300 secondes.")
             error_message = "TimeoutExpired: Le script a mis trop de temps √† s'ex√©cuter."
             recent_errors.append(error_message.strip())
             current_stderr = error_message

        except subprocess.CalledProcessError as e:
            stderr_output = e.stderr.strip()
            logger.error(f"Erreur lors de l'ex√©cution (stderr):\n{stderr_output}")
            recent_errors.append(stderr_output)
            current_stderr = e.stderr

            # Sauvegarder l'erreur dans un fichier
            error_file = os.path.join(code_versions_dir, f"attempt_{attempt}_error.txt")
            try:
                with open(error_file, "w", encoding="utf-8") as f:
                    f.write(f"ERREUR RENCONTR√âE LORS DE LA TENTATIVE {attempt}:\n\n")
                    f.write(current_stderr)
                logger.info(f"Erreur sauvegard√©e dans: {error_file}")
            except Exception as write_err:
                 logger.error(f"Impossible de sauvegarder le fichier d'erreur {error_file}: {write_err}")

            # MODIFICATION: Utiliser le mod√®le puissant apr√®s 5 tentatives √©chou√©es
            if attempt >= 10 and not tried_powerful_model and backend == "gemini":
                logger.warning(f"Apr√®s {attempt} tentatives √©chou√©es, basculement vers le mod√®le Gemini Pro puissant")
                tried_powerful_model = True
                powerful_model = "gemini-2.5-pro-exp-03-25"
                current_model = powerful_model  # Mise √† jour du mod√®le courant
                
                # Cr√©er un prompt sp√©cifique pour r√©soudre les probl√®mes de formule OLS
                powerful_prompt = f"""Voici mon script Python qui g√©n√®re une erreur. Corrige-le avec soin:

```python
{code}
```

Erreur:
```
{current_stderr}
```

Je veux uniquement le code Python corrig√©, sans explications. Le script est utilis√© pour analyser des donn√©es avec pandas, matplotlib et statsmodels.

PROBL√àME √Ä R√âSOUDRE:
- Il y a une erreur de "unterminated string literal" dans une formule OLS
- Le probl√®me est li√© aux noms de pays avec espaces et caract√®res sp√©ciaux dans la formule
- La solution est d'utiliser des backticks (`) pour encadrer chaque nom de variable avec espaces/caract√®res sp√©ciaux
- Exemple: `Pays_Afrique du Sud` au lieu de Pays_Afrique du Sud
- OU simplifier la formule en utilisant une technique d'encodage diff√©rente pour les pays
"""
                
                # Sauvegarder le prompt pour le mod√®le puissant
                save_prompt_to_file(powerful_prompt, prompts_log_path, "Powerful Model after 5 attempts")
                logger.info(f"Envoi du prompt de correction au mod√®le puissant {powerful_model}")
                
                try:
                    powerful_correction = call_llm(prompt=powerful_prompt, model_name=powerful_model, backend="gemini")
                    corrected_code = extract_code(powerful_correction)
                    if corrected_code and len(corrected_code.strip()) > 0:
                        code = corrected_code
                        logger.info("Code corrig√© par le mod√®le puissant re√ßu")
                        recent_errors.clear()
                        continue
                    else:
                        logger.warning("Le mod√®le puissant n'a pas renvoy√© de code utilisable")
                except Exception as powerful_err:
                    logger.error(f"Erreur avec le mod√®le puissant: {powerful_err}")

            # V√©rifier si les erreurs se r√©p√®tent
            if len(recent_errors) == 3 and len(set(recent_errors)) == 1:
                logger.warning("Trois erreurs identiques cons√©cutives d√©tect√©es.")
                
                # Si on n'a pas encore essay√© le mod√®le puissant apr√®s 3 erreurs identiques
                if not tried_powerful_model and backend == "gemini":
                    logger.warning("Basculement vers le mod√®le Gemini Pro puissant pour correction")
                    tried_powerful_model = True
                    powerful_model = "gemini-2.5-pro-exp-03-25"
                    current_model = powerful_model  # Mise √† jour du mod√®le courant
                    
                    # Cr√©er un prompt simplifi√©
                    powerful_prompt = f"""Voici mon script Python qui g√©n√®re une erreur. Corrige-le avec soin:

```python
{code}
```

Erreur:
```
{current_stderr}
```

Je veux uniquement le code Python corrig√©, sans explications. Le script est utilis√© pour analyser des donn√©es avec pandas et matplotlib.
"""
                    
                    # Sauvegarder le prompt pour le mod√®le puissant
                    save_prompt_to_file(powerful_prompt, prompts_log_path, "Powerful Model Correction")
                    logger.info(f"Envoi du prompt de correction au mod√®le puissant {powerful_model}")
                    
                    try:
                        powerful_correction = call_llm(prompt=powerful_prompt, model_name=powerful_model, backend="gemini")
                        corrected_code = extract_code(powerful_correction)
                        if corrected_code and len(corrected_code.strip()) > 0:
                            code = corrected_code
                            logger.info("Code corrig√© par le mod√®le puissant re√ßu")
                            recent_errors.clear()
                            continue
                        else:
                            logger.warning("Le mod√®le puissant n'a pas renvoy√© de code utilisable")
                    except Exception as powerful_err:
                        logger.error(f"Erreur avec le mod√®le puissant: {powerful_err}")
                
                # Si le mod√®le puissant a √©chou√© ou a d√©j√† √©t√© essay√©, passer √† la correction manuelle
                logger.warning("Passage √† la correction manuelle.")
                manual_edit_path = os.path.join(code_versions_dir, f"manual_fix_for_attempt_{attempt+1}.py")

                if not os.path.exists(manual_edit_path):
                    try:
                        with open(manual_edit_path, "w", encoding="utf-8") as f:
                            f.write(code)
                        logger.info(f"Fichier de correction manuelle cr√©√© : {manual_edit_path}")
                    except Exception as create_err:
                        logger.error(f"Erreur lors de la cr√©ation du fichier de correction manuelle {manual_edit_path}: {create_err}")
                else:
                    try:
                        import shutil
                        shutil.copyfile(manual_edit_path, manual_edit_path + ".bak")
                        logger.info(f"Backup du fichier de correction manuelle cr√©√© : {manual_edit_path}.bak")
                    except Exception as copy_err:
                        logger.error(f"Erreur lors de la sauvegarde du fichier {manual_edit_path}: {copy_err}")

                logger.info(f"Modifiez le fichier {manual_edit_path} si n√©cessaire.")

                import sys

                # Afficher le message sur stderr (pas stdout)
                sys.stderr.write("‚è∏Ô∏è Appuyez sur Entr√©e pour continuer apr√®s modification...\n")
                sys.stderr.flush()  # Assurez-vous que le message s'affiche imm√©diatement
                try:
                    input()  # Pas de message dans input() pour √©viter qu'il aille dans stdout
                except EOFError:
                    logger.warning("Pas d'entr√©e utilisateur d√©tect√©e, poursuite automatique.")

                try:
                    with open(manual_edit_path, "r", encoding="utf-8") as f:
                        edited_code = f.read()
                except Exception as read_err:
                    logger.error(f"Erreur lors de la lecture du fichier {manual_edit_path}: {read_err}")
                    edited_code = code

                if edited_code != code:
                    logger.info(f"Code modifi√© d√©tect√© dans {manual_edit_path}. Mise √† jour pour la prochaine tentative.")
                    code = edited_code
                    recent_errors.clear()
                    continue
                else:
                    logger.info(f"Aucune modification d√©tect√©e dans {manual_edit_path}. Passage √† la correction automatique par LLM.")

            # Tentative de correction automatique par LLM standard
            if attempt < max_attempts:
                llm_correction_attempt += 1
                logger.info(f"Tentative de correction LLM #{llm_correction_attempt}")
                
                # Cr√©er le prompt pour la correction
                metadata_str = json.dumps(agent1_data["metadata"], indent=2, ensure_ascii=False)
                recall_prompt = f"""
Le contexte suivant concerne l'analyse d'un fichier CSV :
{metadata_str}

Le chemin absolu du fichier CSV est : {csv_file}
Assure-toi d'utiliser ce chemin exact dans pd.read_csv('{csv_file}').

Le code Python ci-dessous, complet avec toutes ses fonctionnalit√©s (gestion des visualisations, sauvegarde des versions, correction automatique et manuelle, etc.), a g√©n√©r√© l'erreur suivante lors de son ex√©cution :
------------------------------------------------------------
Code Fautif :
```python
{code}
```
Erreur Rencontr√©e : {current_stderr}
TA MISSION : Corrige uniquement l'erreur indiqu√©e sans modifier la logique globale du code. Garde int√©gralement la structure et l'ensemble des fonctionnalit√©s du code initial. Ne simplifie pas le script : toutes les parties (gestion des visualisations, sauvegarde des versions, correction manuelle, etc.) doivent √™tre conserv√©es. GARDE les noms de colonnes exacts. Colonnes valides : {agent1_data["metadata"].get("noms_colonnes", [])}

RENVOIE UNIQUEMENT le code Python corrig√©, encapsul√© dans un bloc de code d√©limit√© par trois backticks (python ... ), sans explications suppl√©mentaires. 

Fais bien attention a la nature des variables, num√©riques, cat√©gorielles, etc.

IMPORTANT: Pour les styles dans matplotlib, utilise 'seaborn-v0_8-whitegrid' au lieu de 'seaborn-whitegrid' qui est obsol√®te.
"""

                # Sauvegarder le prompt
                save_prompt_to_file(recall_prompt, prompts_log_path, f"Code Correction Attempt {attempt} (LLM #{llm_correction_attempt})")
                logger.info(f"Envoi du prompt de correction au LLM via backend '{backend}' avec mod√®le '{current_model}'...")
                
                try:
                    # Obtenir et traiter la correction du LLM
                    new_generated_script = call_llm(prompt=recall_prompt, model_name=current_model, backend=backend)
                    
                    extracted_code = extract_code(new_generated_script)
                    clean_code = remove_shell_commands(extracted_code)

                    if not clean_code.strip():
                        logger.warning("Le LLM a renvoy√© un code vide apr√®s nettoyage. R√©utilisation du code pr√©c√©dent.")
                    else:
                        code = clean_code # Mettre √† jour le code pour la prochaine tentative
                        logger.info("Code corrig√© par le LLM re√ßu et nettoy√©.")

                    time.sleep(1)

                except Exception as llm_call_err:
                    logger.error(f"Erreur lors de l'appel au LLM pour correction: {llm_call_err}")
                    
                    # Si l'appel au LLM standard a √©chou√© et qu'on n'a pas encore essay√© le mod√®le puissant
                    if not tried_powerful_model and backend == "gemini":
                        logger.warning("Tentative avec le mod√®le Gemini Pro puissant apr√®s √©chec du LLM standard")
                        tried_powerful_model = True
                        powerful_model = "gemini-2.5-pro-exp-03-25"
                        current_model = powerful_model  # Mise √† jour du mod√®le courant
                        
                        # Cr√©er un prompt simplifi√©
                        powerful_prompt = f"""Voici mon script Python qui g√©n√®re une erreur. Corrige-le avec soin:

```python
{code}
```

Erreur:
```
{current_stderr}
```

Je veux uniquement le code Python corrig√©, sans explications. Le script est utilis√© pour analyser des donn√©es avec pandas et matplotlib. 
Assure-toi d'utiliser 'seaborn-v0_8-whitegrid' au lieu de 'seaborn-whitegrid' qui est obsol√®te.
"""
                        
                        # Sauvegarder le prompt pour le mod√®le puissant
                        save_prompt_to_file(powerful_prompt, prompts_log_path, "Powerful Model Correction After LLM Failure")
                        logger.info(f"Envoi du prompt de correction au mod√®le puissant {powerful_model}")
                        
                        try:
                            powerful_correction = call_llm(prompt=powerful_prompt, model_name=powerful_model, backend="gemini")
                            corrected_code = extract_code(powerful_correction)
                            if corrected_code and len(corrected_code.strip()) > 0:
                                code = corrected_code
                                logger.info("Code corrig√© par le mod√®le puissant re√ßu")
                                recent_errors.clear()
                                continue
                            else:
                                logger.warning("Le mod√®le puissant n'a pas renvoy√© de code utilisable")
                        except Exception as powerful_err:
                            logger.error(f"Erreur avec le mod√®le puissant: {powerful_err}")
                    
                    # Si aucun LLM ne fonctionne, passer √† la correction manuelle en dernier recours
                    logger.warning("Toutes les tentatives de correction automatique ont √©chou√©. Passage √† la correction manuelle.")
                    manual_edit_path = os.path.join(code_versions_dir, f"manual_fix_final_attempt.py")
                    try:
                        with open(manual_edit_path, "w", encoding="utf-8") as f:
                            f.write(code)
                        logger.info(f"Fichier de correction manuelle finale cr√©√© : {manual_edit_path}")
                        
                        # Attendre la correction manuelle
                        sys.stderr.write("‚è∏Ô∏è CORRECTION FINALE MANUELLE REQUISE. Modifiez le fichier puis appuyez sur Entr√©e...\n")
                        sys.stderr.flush()
                        try:
                            input()
                        except EOFError:
                            logger.warning("Pas d'entr√©e utilisateur d√©tect√©e, poursuite automatique.")
                            
                        with open(manual_edit_path, "r", encoding="utf-8") as f:
                            edited_code = f.read()
                        code = edited_code
                        recent_errors.clear()
                        
                    except Exception as manual_err:
                        logger.error(f"Erreur lors de la correction manuelle finale: {manual_err}")
                        execution_results = {
                            "success": False, 
                            "error": f"√âchec complet - correction automatique et manuelle: {manual_err}",
                            "stderr": current_stderr,
                            "all_code_versions": all_code_versions
                        }
                        break

        except Exception as general_err:
            logger.error(f"Erreur g√©n√©rale inattendue lors de la tentative {attempt}: {general_err}", exc_info=True)
            error_message = f"Erreur g√©n√©rale inattendue: {general_err}"
            recent_errors.append(error_message.strip())
            execution_results = {
                "success": False,
                "error": error_message,
                "stderr": current_stderr if current_stderr else str(general_err),
                "all_code_versions": all_code_versions
            }
            break

    # Ajouter les interpr√©tations pour les visualisations et tables de r√©gression
    if execution_results.get("success"):
        all_visuals = []
        
        # Traiter les visualisations
        if "visualisations" in execution_results:
            all_visuals.extend(execution_results["visualisations"])
            
        # Traiter les tables de r√©gression
        if "regressions" in execution_results:
            all_visuals.extend(execution_results["regressions"])
        
        if all_visuals:
            logger.info("G√©n√©ration d'interpr√©tations pour les visualisations et tables avec Gemini")
            all_visuals_with_interpretations = generate_visualization_interpretations(
                execution_results.get("visualisations", []),
                execution_results.get("regressions", []),
                agent1_data,
                current_model,  # Utilisation du mod√®le courant (qui peut √™tre le mod√®le puissant)
                backend,
                prompts_log_path
            )
            
            # Mettre √† jour les r√©sultats avec les interpr√©tations
            execution_results["all_visuals"] = all_visuals_with_interpretations

    # Finaliser et journaliser les r√©sultats
    final_status = "success" if execution_results.get("success") else "failed"
    logger.info(f"Fin de la boucle d'ex√©cution. Statut final: {final_status}. Total tentatives: {attempt}")

    # Sauvegarder une copie du code final
    final_script_path = None
    last_code_version_path = all_code_versions[-1] if all_code_versions else None

    if last_code_version_path and os.path.exists(last_code_version_path):
        final_script_name = f"analysis_script_{timestamp}_{final_status}.py"
        outputs_dir = "outputs"
        os.makedirs(outputs_dir, exist_ok=True)
        final_script_path = os.path.join(outputs_dir, final_script_name)

        try:
            shutil.copyfile(last_code_version_path, final_script_path)
            logger.info(f"Version finale du code ({final_status}) copi√©e vers: {final_script_path}")
        except Exception as copy_err:
            logger.error(f"Impossible de copier le script final {last_code_version_path} vers {final_script_path}: {copy_err}")
            final_script_path = None
    elif not all_code_versions:
        logger.warning("Aucune version de code n'a √©t√© g√©n√©r√©e ou sauvegard√©e.")
    else:
        logger.warning(f"Le dernier fichier de code {last_code_version_path} n'existe pas. Impossible de sauvegarder le script final.")

    # Cr√©er un fichier d'index pour documenter la session
    index_file_path = os.path.join(code_versions_dir, "index.txt")
    try:
        with open(index_file_path, "w", encoding="utf-8") as f:
            f.write(f"SESSION D'ANALYSE DU {timestamp}\n")
            f.write(f"Fichier CSV: {csv_file}\n")
            f.write(f"Mod√®le LLM: {model}\n")
            f.write(f"Mod√®le LLM courant √† la fin: {current_model}\n")
            f.write(f"Nombre de versions de code g√©n√©r√©es/sauvegard√©es: {len(all_code_versions)}\n")
            f.write(f"Nombre total de tentatives d'ex√©cution effectu√©es: {attempt}\n")
            f.write(f"Nombre de tentatives de correction LLM: {llm_correction_attempt}\n")
            f.write(f"Mod√®le puissant utilis√©: {'Oui' if tried_powerful_model else 'Non'}\n")
            f.write(f"R√©sultat final: {'Succ√®s' if execution_results.get('success') else '√âchec'}\n")

            if not execution_results.get("success"):
                f.write(f"Derni√®re erreur enregistr√©e: {execution_results.get('error', 'N/A')}\n")
                last_stderr = execution_results.get('stderr')
                if last_stderr:
                    f.write(f"Dernier stderr:\n```\n{last_stderr}\n```\n")

            f.write("\nLISTE DES VERSIONS DE CODE G√âN√âR√âES/UTILIS√âES:\n")
            for i, path in enumerate(all_code_versions, 1):
                f.write(f"- Version {i} (Tentative {i}): {os.path.basename(path)}\n")
                error_path = path.replace(".py", "_error.txt")
                if os.path.exists(error_path):
                    f.write(f"  - Erreur associ√©e: {os.path.basename(error_path)}\n")
                manual_edit_trigger_path = os.path.join(os.path.dirname(path), f"manual_fix_for_attempt_{i+1}.py")
                if os.path.exists(manual_edit_trigger_path):
                    f.write(f"  - Fichier pour √©dition manuelle (cr√©√© apr√®s √©chec tentative {i}): {os.path.basename(manual_edit_trigger_path)}\n")

            if final_script_path and os.path.exists(final_script_path):
                f.write(f"\nScript final utilis√© ({final_status}): {os.path.basename(final_script_path)}\n")
                f.write(f"Chemin complet: {final_script_path}\n")
            else:
                if last_code_version_path and not os.path.exists(last_code_version_path):
                    f.write(f"\nScript final non sauvegard√© (fichier source {os.path.basename(last_code_version_path)} introuvable).\n")
                elif final_script_path is None and last_code_version_path:
                    f.write(f"\nScript final non sauvegard√© (√©chec de la copie depuis {os.path.basename(last_code_version_path)}).\n")
                else:
                    f.write("\nScript final non sauvegard√© (aucune version de code disponible).\n")

            if os.path.exists(prompts_log_path):
                f.write(f"\nJournal des prompts envoy√©s au LLM: {os.path.basename(prompts_log_path)}\n")
                f.write(f"Chemin complet: {prompts_log_path}\n")

            f.write(f"\nR√©pertoire des versions de code: {code_versions_dir}\n")
            if execution_results.get("success"):
                vis_dir_final = execution_results.get('vis_dir', os.path.join(temp_dir, "visualisations"))
                tables_dir_final = execution_results.get('tables_dir', os.path.join(temp_dir, "tables"))
                f.write(f"R√©pertoire des visualisations: {vis_dir_final}\n")
                f.write(f"R√©pertoire des tables de r√©gression: {tables_dir_final}\n")
            temp_dir_final = execution_results.get('temp_dir', temp_dir)
            f.write(f"R√©pertoire temporaire d'ex√©cution: {temp_dir_final}\n")
        logger.info(f"Fichier d'index cr√©√©: {index_file_path}")
    except Exception as index_err:
        logger.error(f"Impossible de cr√©er le fichier d'index {index_file_path}: {index_err}")
        index_file_path = None

# Ajouter les chemins finaux aux r√©sultats retourn√©s
    execution_results["final_script_path"] = final_script_path
    execution_results["code_versions_dir"] = code_versions_dir
    execution_results["index_file"] = index_file_path
    execution_results["current_model"] = current_model  # Ajouter le mod√®le utilis√© pour les derni√®res interpr√©tations

    # Traiter les tables de r√©gression avec une interpr√©tation d√©di√©e
    regression_outputs = execution_results.get("regressions", [])
    if regression_outputs:
        logger.info(f"G√©n√©ration d'interpr√©tations d√©taill√©es pour {len(regression_outputs)} tables de r√©gression")
        
        for reg in regression_outputs:
            if 'data' in reg:
                # R√©cup√©rer le code de r√©gression
                regression_code = reg.get('regression_code', '')
                
                # Appel √† notre nouvelle fonction pour interpr√©ter la r√©gression
                detailed_interpretation = interpret_regression_with_llm(
                    reg['data'],
                    regression_code,
                    agent1_data,
                    current_model,  # Utiliser le mod√®le courant qui peut √™tre le mod√®le puissant
                    backend,
                    prompts_log_path,
                    timeout=180  # Plus de temps pour l'analyse d√©taill√©e
                )
                
                # Ajouter l'interpr√©tation d√©taill√©e au r√©sultat
                reg['detailed_interpretation'] = detailed_interpretation
                logger.info(f"Interpr√©tation d√©taill√©e g√©n√©r√©e pour la r√©gression {reg.get('id', 'inconnue')}")
            else:
                logger.warning(f"Pas de donn√©es structur√©es pour la r√©gression {reg.get('id', 'inconnue')}. Interpr√©tation d√©taill√©e impossible.")
    
    # Mettre √† jour les r√©sultats avec les r√©gressions interpr√©t√©es
    execution_results["regressions"] = regression_outputs

    return execution_results

# Nouvelle fonction √† ajouter dans agent2.py

def interpret_regression_with_llm(regression_data, code_executed, agent1_data, model, backend, prompts_log_path, timeout=120):
    """
    Interpr√®te de mani√®re d√©taill√©e les r√©sultats d'une r√©gression √©conom√©trique en utilisant un LLM.
    
    Args:
        regression_data: Donn√©es structur√©es de la r√©gression (coefficients, r-squared, etc.)
        code_executed: Code Python qui a g√©n√©r√© cette r√©gression
        agent1_data: Donn√©es de l'agent1 pour le contexte
        model: Mod√®le LLM √† utiliser
        backend: Backend pour les appels LLM
        prompts_log_path: Chemin pour sauvegarder les prompts
        timeout: Timeout en secondes. D√©faut: 120.
        
    Returns:
        str: Interpr√©tation d√©taill√©e des r√©sultats de r√©gression
    """
    from llm_utils import call_llm
    from datetime import datetime
    
    # Extraire les informations cl√©s de la r√©gression
    r_squared = regression_data.get('r_squared', 'N/A')
    coefficients = regression_data.get('coefficients', [])
    
    # Pr√©parer un r√©sum√© des coefficients pour le prompt
    coef_summary = ""
    for coef in coefficients:
        var_name = coef.get('variable', 'Inconnu')
        coef_value = coef.get('coef', 'N/A')
        p_value = coef.get('p_value', 'N/A')
        std_err = coef.get('std_err', 'N/A')
        coef_summary += f"{var_name}: coefficient={coef_value}, p-value={p_value}, std_err={std_err}\n"
    
    # Extraire le contexte de recherche de l'agent1
    user_prompt = agent1_data.get('user_prompt', 'Non disponible')
    introduction = agent1_data.get('llm_output', {}).get('introduction', 'Non disponible')
    hypotheses = agent1_data.get('llm_output', {}).get('hypotheses', 'Non disponible')
    
    # Extraire les noms des colonnes pour avoir une id√©e des variables disponibles
    column_names = agent1_data.get('metadata', {}).get('noms_colonnes', [])
    
    # Cr√©er le prompt pour l'interpr√©tation de la r√©gression
    prompt = f"""## INTERPR√âTATION √âCONOM√âTRIQUE D√âTAILL√âE

### R√©sultats de r√©gression
R-squared: {r_squared}

### Coefficients
{coef_summary}

### Code Python ayant g√©n√©r√© cette r√©gression
```python
{code_executed}
```

### Question de recherche
{user_prompt}

### Contexte acad√©mique
{introduction[:500]}...

### Hypoth√®ses de recherche
{hypotheses}

### Variables disponibles dans le dataset
{', '.join(column_names)}

---

En tant qu'√©conom√®tre expert, ton objectif est de fournir une interpr√©tation pr√©cise, rigoureuse et acad√©mique de ces r√©sultats de r√©gression.

Ton interpr√©tation doit inclure:

1. **Analyse du mod√®le global**
   - Qualit√© globale de l'ajustement (R¬≤)
   - Validit√© statistique du mod√®le
   - Ad√©quation du mod√®le √† la question de recherche

2. **Interpr√©tation des coefficients significatifs**
   - Analyse d√©taill√©e de chaque coefficient statistiquement significatif (p < 0.05)
   - Interpr√©tation pr√©cise de l'effet marginal (magnitude et direction)
   - Unit√©s de mesure et contexte √©conomique de chaque effet

3. **Implications √©conomiques**
   - M√©canismes √©conomiques sous-jacents expliquant les relations observ√©es
   - Liens avec les th√©ories √©conomiques pertinentes
   - Implications pour la question de recherche initiale

4. **Limites de l'estimation**
   - Probl√®mes potentiels d'endog√©n√©it√©, de variables omises ou de causalit√©
   - Robustesse des r√©sultats
   - Pistes d'am√©lioration du mod√®le

IMPORTANT:
- Base ton analyse uniquement sur les r√©sultats statistiques fournis, tout en les contextualisant avec la question de recherche
- Utilise un langage √©conom√©trique pr√©cis (√©lasticit√©s, effets marginaux, significativit√©, etc.)
- Proc√®de coefficient par coefficient pour les variables significatives
- Fais le lien entre les r√©sultats statistiques et les m√©canismes √©conomiques
- Ton analyse doit √™tre concise mais compl√®te, en 3-4 paragraphes

Il est essentiel que cette interpr√©tation soit suffisamment d√©taill√©e pour former le c≈ìur d'une analyse √©conom√©trique acad√©mique rigoureuse.
"""

    # Sauvegarder le prompt dans le fichier journal
    try:
        with open(prompts_log_path, "a", encoding="utf-8") as f:
            f.write("\n\n" + "="*80 + "\n")
            f.write(f"PROMPT POUR INTERPR√âTATION DE R√âGRESSION - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n")
            f.write(prompt)
            f.write("\n" + "="*80 + "\n")
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde du prompt: {e}")
    
    try:
        logger.info(f"Appel au LLM pour l'interpr√©tation d√©taill√©e de la r√©gression (R¬≤={r_squared})")
        interpretation = call_llm(
            prompt=prompt, 
            model_name=model, 
            backend=backend,
            timeout=timeout
        )
        
        logger.info("Interpr√©tation d√©taill√©e de la r√©gression g√©n√©r√©e avec succ√®s")
        return interpretation
    except Exception as e:
        logger.error(f"Erreur lors de l'interpr√©tation d√©taill√©e de la r√©gression: {e}")
        return f"Erreur lors de l'interpr√©tation d√©taill√©e de la r√©gression: {e}"

# Fonction pour extraire le code qui a g√©n√©r√© une r√©gression
def extract_regression_code(full_code, table_content):
    """Tente d'identifier le code qui a g√©n√©r√© une r√©gression sp√©cifique."""
    if not full_code:
        return ""
        
    # Rechercher des patterns communs dans les codes de r√©gression
    import re
    
    # Extraire les noms de variables potentiels de la table de r√©gression
    var_names = re.findall(r"([A-Za-z0-9_]+)\s+[\d\.-]+\s+[\d\.-]+\s+[\d\.-]+\s+[\d\.-]+", table_content)
    
    # Chercher les mod√®les OLS qui utilisent ces variables
    regression_patterns = [
        r"(.*?sm\.OLS.*?\.fit\(\).*?)(?=\n\n|\Z)",  # Pattern pour statsmodels OLS
        r"(.*?statsmodels\.api.*?OLS.*?\.fit\(\).*?)(?=\n\n|\Z)",  # Autre pattern pour statsmodels
        r"(.*?LinearRegression\(\).*?\.fit\(.*?\).*?)(?=\n\n|\Z)"   # Pattern pour sklearn LinearRegression
    ]
    
    # Si nous avons des noms de variables, chercher des lignes qui les utilisent
    if var_names:
        var_patterns = []
        for var in var_names[:5]:  # Limiter aux 5 premi√®res variables pour √©viter de surcharger
            var_patterns.append(r".*?" + re.escape(var) + r".*?\n")
        
        # Chercher des blocs de code qui contiennent √† la fois un mod√®le de r√©gression
        # et les variables identifi√©es
        for reg_pattern in regression_patterns:
            reg_blocks = re.findall(reg_pattern, full_code, re.DOTALL)
            for block in reg_blocks:
                # V√©rifier si ce bloc contient au moins une des variables
                for var_pattern in var_patterns:
                    if re.search(var_pattern, block, re.MULTILINE):
                        # Bloc de code qui contient √† la fois un mod√®le de r√©gression et une variable pertinente
                        return block.strip()
    
    # Si nous n'avons pas trouv√© de code sp√©cifique, chercher simplement un mod√®le OLS
    for pattern in regression_patterns:
        matches = re.findall(pattern, full_code, re.DOTALL)
        if matches:
            # Prendre le premier bloc de code qui correspond √† un mod√®le de r√©gression
            return matches[0].strip()
    
    # Si nous n'avons toujours rien trouv√©, retourner une portion du code global
    # qui contient des mots-cl√©s de r√©gression
    regression_keywords = ["OLS", "regression", "statsmodels", "LinearRegression", "fit("]
    lines = full_code.split('\n')
    for i, line in enumerate(lines):
        for keyword in regression_keywords:
            if keyword in line:
                # Retourner un bloc de 10 lignes autour de cette ligne si possible
                start = max(0, i - 5)
                end = min(len(lines), i + 6)
                return '\n'.join(lines[start:end])
    
    # Si tout √©choue, retourner une cha√Æne vide
    return ""

def generate_analysis_code(csv_file, user_prompt, agent1_data, model, prompts_log_path, backend: str):
    """
    G√©n√®re le code d'analyse √©conom√©trique de niveau accessible
    
    Args:
        csv_file: Chemin absolu du fichier CSV
        user_prompt: Prompt initial de l'utilisateur
        agent1_data: Donn√©es de l'agent1
        model: Mod√®le LLM √† utiliser
        prompts_log_path: Chemin pour sauvegarder les prompts
        backend: Backend pour les appels LLM
        
    Returns:
        str: Code d'analyse g√©n√©r√©
    """
    # Pr√©paration du prompt pour le LLM avec le chemin absolu du fichier et les noms de colonnes exacts
    metadata_str = json.dumps(agent1_data["metadata"], indent=2, ensure_ascii=False)
    colonnes = agent1_data["metadata"].get("noms_colonnes", [])
    col_dict = ",\n".join([f'    "{col}": "{col}"' for col in colonnes])
    
    # Extraire les sections si disponibles
    introduction = agent1_data['llm_output'].get('introduction', 'Non disponible')
    literature_review = agent1_data['llm_output'].get('literature_review', 'Non disponible')
    hypotheses = agent1_data['llm_output'].get('hypotheses', 'Non disponible')
    methodology = agent1_data['llm_output'].get('methodology', 'Non disponible')
    limitations = agent1_data['llm_output'].get('limitations', 'Non disponible')
    variables_info = agent1_data['llm_output'].get('variables', 'Non disponible')
    
# Utiliser les anciennes sections si les nouvelles ne sont pas disponibles
    if introduction == 'Non disponible':
        introduction = agent1_data['llm_output'].get('problematisation', 'Non disponible')
    
    if methodology == 'Non disponible':
        methodology = agent1_data['llm_output'].get('approches_suggerees', 'Non disponible')
    
    if limitations == 'Non disponible':
        limitations = agent1_data['llm_output'].get('points_vigilance', 'Non disponible')
    
    # Cr√©er le prompt pour la g√©n√©ration de code
    prompt = f"""## G√âN√âRATION DE CODE D'ANALYSE DE DONN√âES

### Fichier CSV et M√©tadonn√©es
```json
{metadata_str}
```

### Chemin absolu du fichier CSV
{csv_file}

### Noms exacts des colonnes √† utiliser
{colonnes}

### Introduction et probl√©matique de recherche
{introduction}

### Hypoth√®ses de recherche
{hypotheses}

### M√©thodologie propos√©e
{methodology}

### Limites identifi√©es
{limitations}

### Informations sur les variables
{variables_info}

### Demande initiale de l'utilisateur
{user_prompt}

---

Tu es un analyste de donn√©es exp√©riment√©. Ta mission est de g√©n√©rer un script Python d'analyse de donn√©es clair et accessible. Le code doit √™tre robuste et produire des visualisations attrayantes.

DIRECTIVES:

1. CHARGEMENT ET PR√âTRAITEMENT DES DONN√âES
   - Utilise strictement le chemin absolu '{csv_file}'
   - Nettoie les donn√©es (valeurs manquantes, outliers)
   - Cr√©e des statistiques descriptives claires

2. VISUALISATIONS ATTRAYANTES ET INFORMATIVES
   - Cr√©e au moins 4-5 visualisations avec matplotlib/seaborn:
     * Matrice de corr√©lation color√©e et lisible
     * Distributions des variables principales
     * Relations entre variables importantes
     * Graphiques adapt√©s au type de donn√©es
   - Utilise des couleurs attrayantes et des styles modernes
   - Ajoute des titres clairs, des l√©gendes informatives et des √âTIQUETTES D'AXES EXPLICITES
   - IMPORTANT: Assure-toi d'utiliser ax.set_xlabel() et ax.set_ylabel() avec des descriptions claires
   - IMPORTANT: Assure-toi que les graphiques soient sauvegard√©s ET affich√©s
   - Utilise plt.savefig() AVANT plt.show() pour chaque graphique
   - IMPORTANT: Pour les styles Seaborn, utilise 'seaborn-v0_8-whitegrid' au lieu de 'seaborn-whitegrid' qui est obsol√®te

3. MOD√âLISATION SIMPLE ET CLAIRE
   - Impl√©mente les mod√®les de r√©gression appropri√©s
   - Utilise statsmodels avec des r√©sultats complets
   - Pr√©sente les r√©sultats de mani√®re lisible
   - Documente clairement chaque √©tape

4. TESTS DE BASE
   - V√©rifie la qualit√© du mod√®le avec des tests simples
   - Analyse les r√©sidus
   - V√©rifie la multicolin√©arit√© si pertinent

5. CAPTURE ET STOCKAGE DES DONN√âES POUR INTERPR√âTATION
   - IMPORTANT: Pour chaque visualisation, stocke le DataFrame utilis√© dans une variable
   - IMPORTANT: Apr√®s chaque cr√©ation de figure, stocke les donn√©es utilis√©es pour permettre une interpr√©tation pr√©cise
   - Assure-toi que chaque figure peut √™tre associ√©e aux donn√©es qui ont servi √† la g√©n√©rer

EXIGENCES TECHNIQUES:
- Utilise pandas, numpy, matplotlib, seaborn, et statsmodels
- Organise ton code en sections clairement comment√©es
- Utilise ce dictionnaire pour acc√©der aux colonnes:
```python
col = {{
{col_dict}
}}
```
- Document chaque √©tape de fa√ßon simple et accessible
- Pour chaque visualisation:
  * UTILISE des titres clairs pour les graphiques et les axes
  * SAUVEGARDE avec plt.savefig() PUIS
  * AFFICHE avec plt.show()
- Pour les tableaux de r√©gression, utilise print(results.summary())

IMPORTANT:
- Adapte l'analyse aux donn√©es disponibles
- Mets l'accent sur les visualisations attrayantes et bien √©tiquet√©es
- Assure-toi que chaque graphique a des √©tiquettes d'axe claires via ax.set_xlabel() et ax.set_ylabel()
- Assure-toi que chaque graphique est √† la fois SAUVEGARD√â et AFFICH√â
- Utilise plt.savefig() AVANT plt.show() pour chaque graphique
- IMPORTANT: Pour les styles Seaborn, utilise 'whitegrid' au lieu de 'seaborn-whitegrid' ou 'seaborn-v0_8-whitegrid' qui sont obsol√®tes
"""

    # Sauvegarder le prompt dans le fichier journal
    save_prompt_to_file(prompt, prompts_log_path, "Initial Code Generation")
    logger.info("Appel LLM pour g√©n√©ration du code d'analyse")
    try:
        # Utilisation de llm_utils.call_llm
        generated_output = call_llm(prompt=prompt, model_name=model, backend=backend)
        
        # Extraire et nettoyer le code g√©n√©r√©
        generated_code = extract_code(generated_output)
        clean_code = remove_shell_commands(generated_code)
        
        # S'assurer que print(results.summary()) est pr√©sent
        if "results.summary()" in clean_code and "print(results.summary())" not in clean_code:
            clean_code = clean_code.replace("results.summary()", "print(results.summary())")
        
        # Assurer que les styles Seaborn sont compatibles
        clean_code = clean_code.replace("seaborn-v0_8-whitegrid", "whitegrid")
        clean_code = clean_code.replace("seaborn-whitegrid", "whitegrid")
        clean_code = clean_code.replace("seaborn-v0_8-white", "white")
        clean_code = clean_code.replace("seaborn-white", "white")
        clean_code = clean_code.replace("seaborn-v0_8-darkgrid", "darkgrid")
        clean_code = clean_code.replace("seaborn-darkgrid", "darkgrid")
        
        return clean_code
        
    except Exception as e:
        logger.error(f"Erreur lors de l'appel LLM pour g√©n√©ration: {e}")
        sys.exit(1)

def generate_analysis_narrative(execution_results, agent1_data, model):
    """
    Fonction de placeholder pour la g√©n√©ration de narration explicative.
    Cette fonction peut √™tre impl√©ment√©e pour g√©n√©rer une narration bas√©e sur les r√©sultats.
    
    Args:
        execution_results: R√©sultats de l'ex√©cution du code
        agent1_data: Donn√©es de l'agent1
        model: Mod√®le LLM √† utiliser
        
    Returns:
        Dictionnaire avec une narration par d√©faut
    """
    # Pour l'instant, retourne juste un dictionnaire avec une narration par d√©faut
    return {
        "narrative": "Analyse ex√©cut√©e avec succ√®s. Veuillez consulter les visualisations et les r√©sultats."
    }

def main():
    """
    Fonction principale qui ex√©cute le pipeline d'analyse √©conom√©trique.
    """
    parser = argparse.ArgumentParser(
        description="Agent 2: Analyse √©conom√©trique"
    )
    parser.add_argument("csv_file", help="Chemin vers le fichier CSV")
    parser.add_argument("user_prompt", help="Prompt initial de l'utilisateur")
    parser.add_argument("agent1_output", help="Chemin vers le fichier de sortie de l'agent 1")
    parser.add_argument("--model", default="gemini-2.0-flash", help="Mod√®le LLM √† utiliser")
    parser.add_argument("--backend", default="gemini", choices=["ollama", "gemini"], help="Backend LLM √† utiliser: 'ollama' ou 'gemini'")
    parser.add_argument("--auto-confirm", action="store_true", help="Ignore la pause manuelle pour correction")
    parser.add_argument("--log-file", help="Fichier de log sp√©cifique")  # Nouvel argument
    args = parser.parse_args()

    # Reconfigurer le logging si un fichier de log sp√©cifique est fourni
    if args.log_file:
        # Supprimer les handlers existants
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        # Ajouter les nouveaux handlers
        file_handler = logging.FileHandler(args.log_file, mode='a')  # mode 'a' pour append
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        logger.addHandler(file_handler)
        
        stream_handler = logging.StreamHandler(sys.stderr)
        stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        logger.addHandler(stream_handler)
        
        logger.info(f"Logging redirig√© vers {args.log_file}")

    # Cr√©er les r√©pertoires de sortie et les noms de fichiers
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    os.makedirs("outputs", exist_ok=True)
    code_versions_dir = os.path.join("outputs", f"code_versions_{timestamp}")
    prompts_log_path = os.path.join(code_versions_dir, "prompts.txt")

    # Charger les donn√©es de l'agent 1
    try:
        with open(args.agent1_output, "r", encoding="utf-8") as f:
            agent1_data = json.load(f)
    except FileNotFoundError:
        logger.error(f"Erreur: Le fichier de l'agent 1 '{args.agent1_output}' n'a pas √©t√© trouv√©.")
        sys.exit(1)
    except json.JSONDecodeError:
        logger.error(f"Erreur: Impossible de d√©coder le JSON du fichier de l'agent 1 '{args.agent1_output}'.")
        sys.exit(1)

    # G√©n√©rer le code d'analyse √©conom√©trique
    logger.info("G√©n√©ration du code d'analyse √©conom√©trique")
    analysis_code = generate_analysis_code(
        args.csv_file,
        args.user_prompt,
        agent1_data,
        args.model,
        prompts_log_path,
        args.backend
    )
    if not analysis_code:
         logger.error("La g√©n√©ration du code initial a √©chou√©. Arr√™t.")
         sys.exit(1)

    # Ex√©cuter le code d'analyse
    logger.info("Ex√©cution du code d'analyse")
    execution_results = attempt_execution_loop(
        analysis_code,
        args.csv_file,
        agent1_data,
        args.model,
        prompts_log_path,
        args.backend
    )

    # G√©n√©rer la narration explicative des r√©sultats
    narrative = {"narrative": "L'analyse n'a pas √©t√© ex√©cut√©e avec succ√®s ou a √©chou√©."}
    if execution_results.get("success"):
        logger.info("G√©n√©ration de la narration explicative des r√©sultats")
        narrative = generate_analysis_narrative(
            execution_results,
            agent1_data,
            execution_results.get("current_model", args.model)  # Utiliser le mod√®le courant (qui peut √™tre le puissant)
        )
    else:
        error_msg = execution_results.get('error', 'Erreur inconnue lors de l\'ex√©cution')
        stderr_msg = execution_results.get('stderr')
        full_error = f"L'analyse a √©chou√©: {error_msg}"
        if stderr_msg:
            full_error += f"\nDernier Stderr:\n{stderr_msg[:500]}..."
        narrative = {
            "narrative": full_error
        }
        logger.warning(f"L'ex√©cution du code a √©chou√©. Raison: {error_msg}")

    # Combiner toutes les visualisations et tables de r√©gression en une seule liste
    all_visuals = execution_results.get("all_visuals", [])
    if not all_visuals:
        all_visuals = execution_results.get("visualisations", []) + execution_results.get("regressions", [])

    # Pr√©parer la sortie finale
    output = {
        "initial_generated_code": analysis_code,
        "execution_results": execution_results,
        "narrative": narrative.get("narrative"),
        "final_script_path": execution_results.get("final_script_path", "Non disponible"),
        "visualisations": all_visuals,  # Liste combin√©e
        "prompts_log_file": prompts_log_path if os.path.exists(prompts_log_path) else "Non g√©n√©r√© ou erreur",
        "output_directory": code_versions_dir,
        "index_file": execution_results.get("index_file", "Non g√©n√©r√© ou erreur"),
        "current_model": execution_results.get("current_model", args.model)  # Ajouter le mod√®le utilis√© pour les derni√®res actions
    }

    # Afficher la sortie au format JSON
    try:
        print(json.dumps(output, ensure_ascii=False, indent=2))
    except TypeError as e:
         logger.error(f"Erreur lors de la s√©rialisation en JSON: {e}")
         simplified_output = {k: str(v) for k, v in output.items()}
         print(json.dumps(simplified_output, ensure_ascii=False, indent=2))

    return 0 if execution_results.get("success") else 1

if __name__ == "__main__":
    sys.exit(main())